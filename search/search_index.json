{"config":{"lang":["zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"MIL/C02_Basics/S01_Calculating/","title":"S01 Calculating","text":"<p>import MIL.Common import Mathlib.Data.Real.Basic /- TEXT: Calculating</p> <p>We generally learn to carry out mathematical calculations without thinking of them as proofs. But when we justify each step in a calculation, as Lean requires us to do, the net result is a proof that the left-hand side of the calculation is equal to the right-hand side.</p> <p>.. index:: rewrite, rw, tactics ; rw and rewrite</p> <p>In Lean, stating a theorem is tantamount to stating a goal, namely, the goal of proving the theorem. Lean provides the rewriting tactic <code>rw</code>, to replace the left-hand side of an identity by the right-hand side in the goal. If <code>a</code>, <code>b</code>, and <code>c</code> are real numbers, <code>mul_assoc a b c</code>  is the identity <code>a * b * c = a * (b * c)</code> and <code>mul_comm a b</code> is the identity <code>a * b = b * a</code>. Lean provides automation that generally eliminates the need to refer the facts like these explicitly, but they are useful for the purposes of illustration. In Lean, multiplication associates to the left, so the left-hand side of <code>mul_assoc</code> could also be written <code>(a * b) * c</code>. However, it is generally good style to be mindful of Lean's notational conventions and leave out parentheses when Lean does as well.</p> <p>Let's try out <code>rw</code>.</p> <p>.. index:: real numbers TEXT. -/ -- An example. -- QUOTE: example (a b c : \u211d) : a * b * c = b * (a * c) := by   rw [mul_comm a b]   rw [mul_assoc b a c] -- QUOTE.</p> <p>/- TEXT: The <code>import</code> lines at the beginning of the associated examples file import the theory of the real numbers from Mathlib, as well as useful automation. For the sake of brevity, we generally suppress information like this in the textbook.</p> <p>You are welcome to make changes to see what happens. You can type the <code>\u211d</code> character as <code>\\R</code> or <code>\\real</code> in VS Code. The symbol doesn't appear until you hit space or the tab key. If you hover over a symbol when reading a Lean file, VS Code will show you the syntax that can be used to enter it. If you are curious to see all available abbreviations, you can hit Ctrl-Shift-P and then type abbreviations to get access to the <code>Lean 4: Show all abbreviations</code> command. If your keyboard does not have an easily accessible backslash, you can change the leading character by changing the <code>lean4.input.leader</code> setting.</p> <p>.. index:: proof state, local context, goal</p> <p>When a cursor is in the middle of a tactic proof, Lean reports on the current proof state in the Lean Infoview window. As you move your cursor past each step of the proof, you can see the state change. A typical proof state in Lean might look as follows:</p> <p>.. code-block::</p> <pre><code>1 goal\nx y : \u2115,\nh\u2081 : Prime x,\nh\u2082 : \u00acEven x,\nh\u2083 : y &gt; x\n\u22a2 y \u2265 4\n</code></pre> <p>The lines before the one that begins with <code>\u22a2</code> denote the context: they are the objects and assumptions currently at play. In this example, these include two objects, <code>x</code> and <code>y</code>, each a natural number. They also include three assumptions, labelled <code>h\u2081</code>, <code>h\u2082</code>, and <code>h\u2083</code>. In Lean, everything in a context is labelled with an identifier. You can type these subscripted labels as <code>h\\1</code>, <code>h\\2</code>, and <code>h\\3</code>, but any legal identifiers would do: you can use <code>h1</code>, <code>h2</code>, <code>h3</code> instead, or <code>foo</code>, <code>bar</code>, and <code>baz</code>. The last line represents the goal, that is, the fact to be proved. Sometimes people use target for the fact to be proved, and goal for the combination of the context and the target. In practice, the intended meaning is usually clear.</p> <p>Try proving these identities, in each case replacing <code>sorry</code> by a tactic proof. With the <code>rw</code> tactic, you can use a left arrow (<code>\\l</code>) to reverse an identity. For example, <code>rw [\u2190 mul_assoc a b c]</code> replaces <code>a * (b * c)</code> by <code>a * b * c</code> in the current goal. Note that the left-pointing arrow refers to going from right to left in the identity provided by <code>mul_assoc</code>, it has nothing to do with the left or right side of the goal. TEXT. -/ -- Try these. -- QUOTE: example (a b c : \u211d) : c * b * a = b * (a * c) := by   sorry</p> <p>example (a b c : \u211d) : a * (b * c) = b * (a * c) := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example (a b c : \u211d) : c * b * a = b * (a * c) := by   rw [mul_comm c b]   rw [mul_assoc b c a]   rw [mul_comm c a]</p> <p>example (a b c : \u211d) : a * (b * c) = b * (a * c) := by   rw [\u2190 mul_assoc a b c]   rw [mul_comm a b]   rw [mul_assoc b a c]</p> <p>/- TEXT: You can also use identities like <code>mul_assoc</code> and <code>mul_comm</code> without arguments. In this case, the rewrite tactic tries to match the left-hand side with an expression in the goal, using the first pattern it finds. TEXT. -/ -- An example. -- QUOTE: example (a b c : \u211d) : a * b * c = b * c * a := by   rw [mul_assoc]   rw [mul_comm] -- QUOTE.</p> <p>/- TEXT: You can also provide partial information. For example, <code>mul_comm a</code> matches any pattern of the form <code>a * ?</code> and rewrites it to <code>? * a</code>. Try doing the first of these examples without providing any arguments at all, and the second with only one argument. TEXT. -/ /- Try doing the first of these without providing any arguments at all,    and the second with only one argument. -/ -- QUOTE: example (a b c : \u211d) : a * (b * c) = b * (c * a) := by   sorry</p> <p>example (a b c : \u211d) : a * (b * c) = b * (a * c) := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example (a b c : \u211d) : a * (b * c) = b * (c * a) := by   rw [mul_comm]   rw [mul_assoc]</p> <p>example (a b c : \u211d) : a * (b * c) = b * (a * c) := by   rw [\u2190 mul_assoc]   rw [mul_comm a]   rw [mul_assoc]</p> <p>/- TEXT: You can also use <code>rw</code> with facts from the local context. TEXT. -/ -- Using facts from the local context. -- QUOTE: example (a b c d e f : \u211d) (h : a * b = c * d) (h' : e = f) : a * (b * e) = c * (d * f) := by   rw [h']   rw [\u2190 mul_assoc]   rw [h]   rw [mul_assoc] -- QUOTE.</p> <p>/- TEXT: Try these, using the theorem <code>sub_self</code> for the second one: TEXT. -/ -- QUOTE: example (a b c d e f : \u211d) (h : b * c = e * f) : a * b * c * d = a * e * f * d := by   sorry</p> <p>example (a b c d : \u211d) (hyp : c = b * a - d) (hyp' : d = a * b) : c = 0 := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example (a b c d e f : \u211d) (h : b * c = e * f) : a * b * c * d = a * e * f * d := by   rw [mul_assoc a]   rw [h]   rw [\u2190 mul_assoc]</p> <p>example (a b c d : \u211d) (hyp : c = b * a - d) (hyp' : d = a * b) : c = 0 := by   rw [hyp]   rw [hyp']   rw [mul_comm]   rw [sub_self]</p> <p>/- TEXT: Multiple rewrite commands can be carried out with a single command, by listing the relevant identities separated by commas inside the square brackets. TEXT. -/ -- QUOTE: example (a b c d e f : \u211d) (h : a * b = c * d) (h' : e = f) : a * (b * e) = c * (d * f) := by   rw [h', \u2190 mul_assoc, h, mul_assoc] -- QUOTE.</p> <p>/- TEXT: You still see the incremental progress by placing the cursor after a comma in any list of rewrites.</p> <p>Another trick is that we can declare variables once and for all outside an example or theorem. Lean then includes them automatically. TEXT. -/ section</p> <p>-- QUOTE: variable (a b c d e f : \u211d)</p> <p>example (h : a * b = c * d) (h' : e = f) : a * (b * e) = c * (d * f) := by   rw [h', \u2190 mul_assoc, h, mul_assoc] -- QUOTE.</p> <p>end</p> <p>/- TEXT: Inspection of the tactic state at the beginning of the above proof reveals that Lean indeed included all variables. We can delimit the scope of the declaration by putting it in a <code>section ... end</code> block. Finally, recall from the introduction that Lean provides us with a command to determine the type of an expression: TEXT. -/ -- QUOTE: section variable (a b c : \u211d)</p>"},{"location":"MIL/C02_Basics/S01_Calculating/#check-a","title":"check a","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-a-b","title":"check a + b","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-a-r","title":"check (a : \u211d)","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-mul_comm-a-b","title":"check mul_comm a b","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-mul_comm-a-b-a-b-b-a","title":"check (mul_comm a b : a * b = b * a)","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-mul_assoc-c-a-b","title":"check mul_assoc c a b","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-mul_comm-a","title":"check mul_comm a","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-mul_comm","title":"check mul_comm","text":"<p>end -- QUOTE.</p> <p>/- TEXT: The <code>#check</code> command works for both objects and facts. In response to the command <code>#check a</code>, Lean reports that <code>a</code> has type <code>\u211d</code>. In response to the command <code>#check mul_comm a b</code>, Lean reports that <code>mul_comm a b</code> is a proof of the fact <code>a * b = b * a</code>. The command <code>#check (a : \u211d)</code> states our expectation that the type of <code>a</code> is <code>\u211d</code>, and Lean will raise an error if that is not the case. We will explain the output of the last three <code>#check</code> commands later, but in the meanwhile, you can take a look at them, and experiment with some <code>#check</code> commands of your own.</p> <p>Let's try some more examples. The theorem <code>two_mul a</code> says that <code>2 * a = a + a</code>. The theorems <code>add_mul</code> and <code>mul_add</code> express the distributivity of multiplication over addition, and the theorem <code>add_assoc</code> expresses the associativity of addition. Use the <code>#check</code> command to see the precise statements.</p> <p>.. index:: calc, tactics ; calc TEXT. -/ section variable (a b : \u211d)</p> <p>-- QUOTE: example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b := by   rw [mul_add, add_mul, add_mul]   rw [\u2190 add_assoc, add_assoc (a * a)]   rw [mul_comm b a, \u2190 two_mul] -- QUOTE.</p> <p>/- TEXT: Whereas it is possible to figure out what it going on in this proof by stepping through it in the editor, it is hard to read on its own. Lean provides a more structured way of writing proofs like this using the <code>calc</code> keyword. TEXT. -/ -- QUOTE: example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b :=   calc     (a + b) * (a + b) = a * a + b * a + (a * b + b * b) := by       rw [mul_add, add_mul, add_mul]     _ = a * a + (b * a + a * b) + b * b := by       rw [\u2190 add_assoc, add_assoc (a * a)]     _ = a * a + 2 * (a * b) + b * b := by       rw [mul_comm b a, \u2190 two_mul] -- QUOTE.</p> <p>/- TEXT: Notice that the proof does not begin with <code>by</code>: an expression that begins with <code>calc</code> is a proof term. A <code>calc</code> expression can also be used inside a tactic proof, but Lean interprets it as the instruction to use the resulting proof term to solve the goal. The <code>calc</code> syntax is finicky: the underscores and justification have to be in the format indicated above. Lean uses indentation to determine things like where a block of tactics or a <code>calc</code> block begins and ends; try changing the indentation in the proof above to see what happens.</p> <p>One way to write a <code>calc</code> proof is to outline it first using the <code>sorry</code> tactic for justification, make sure Lean accepts the expression modulo these, and then justify the individual steps using tactics. TEXT. -/ -- QUOTE: example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b :=   calc     (a + b) * (a + b) = a * a + b * a + (a * b + b * b) := by       sorry     _ = a * a + (b * a + a * b) + b * b := by       sorry     _ = a * a + 2 * (a * b) + b * b := by       sorry -- QUOTE.</p> <p>end</p> <p>/- TEXT: Try proving the following identity using both a pure <code>rw</code> proof and a more structured <code>calc</code> proof: TEXT. -/ -- Try these. For the second, use the theorems listed underneath. section variable (a b c d : \u211d)</p> <p>-- QUOTE: example : (a + b) * (c + d) = a * c + a * d + b * c + b * d := by   sorry -- QUOTE.</p> <p>/- TEXT: The following exercise is a little more challenging. You can use the theorems listed underneath. TEXT. -/ -- QUOTE: example (a b : \u211d) : (a + b) * (a - b) = a ^ 2 - b ^ 2 := by   sorry</p>"},{"location":"MIL/C02_Basics/S01_Calculating/#check-pow_two-a","title":"check pow_two a","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-mul_sub-a-b-c","title":"check mul_sub a b c","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-add_mul-a-b-c","title":"check add_mul a b c","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-add_sub-a-b-c","title":"check add_sub a b c","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-sub_sub-a-b-c","title":"check sub_sub a b c","text":""},{"location":"MIL/C02_Basics/S01_Calculating/#check-add_zero-a","title":"check add_zero a","text":"<p>-- QUOTE.</p> <p>end</p> <p>/- TEXT: .. index:: rw, tactics ; rw and rewrite</p> <p>We can also perform rewriting in an assumption in the context. For example, <code>rw [mul_comm a b] at hyp</code> replaces <code>a * b</code> by <code>b * a</code> in the assumption <code>hyp</code>. TEXT. -/ -- Examples.</p> <p>section variable (a b c d : \u211d)</p> <p>-- QUOTE: example (a b c d : \u211d) (hyp : c = d * a + b) (hyp' : b = a * d) : c = 2 * a * d := by   rw [hyp'] at hyp   rw [mul_comm d a] at hyp   rw [\u2190 two_mul (a * d)] at hyp   rw [\u2190 mul_assoc 2 a d] at hyp   exact hyp -- QUOTE.</p> <p>/- TEXT: .. index:: exact, tactics ; exact</p> <p>In the last step, the <code>exact</code> tactic can use <code>hyp</code> to solve the goal because at that point <code>hyp</code> matches the goal exactly.</p> <p>.. index:: ring (tactic), tactics ; ring</p> <p>We close this section by noting that Mathlib provides a useful bit of automation with a <code>ring</code> tactic, which is designed to prove identities in any commutative ring as long as they follow purely from the ring axioms, without using any local assumption. TEXT. -/ -- QUOTE: example : c * b * a = b * (a * c) := by   ring</p> <p>example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b := by   ring</p> <p>example : (a + b) * (a - b) = a ^ 2 - b ^ 2 := by   ring</p> <p>example (hyp : c = d * a + b) (hyp' : b = a * d) : c = 2 * a * d := by   rw [hyp, hyp']   ring -- QUOTE.</p> <p>end</p> <p>/- TEXT: The <code>ring</code> tactic is imported indirectly when we import <code>Mathlib.Data.Real.Basic</code>, but we will see in the next section that it can be used for calculations on structures other than the real numbers. It can be imported explicitly with the command <code>import Mathlib.Tactic</code>. We will see there are similar tactics for other common kind of algebraic structures.</p> <p>There is a variation of <code>rw</code> called <code>nth_rw</code> that allows you to replace only particular instances of an expression in the goal. Possible matches are enumerated starting with 1, so in the following example, <code>nth_rw 2 [h]</code> replaces the second occurrence of <code>a + b</code> with <code>c</code>. EXAMPLES: -/ -- QUOTE: example (a b c : \u2115) (h : a + b = c) : (a + b) * (a + b) = a * c + b * c := by   nth_rw 2 [h]   rw [add_mul] -- QUOTE.</p>"},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/","title":"S02 Proving Identities in Algebraic Structures","text":"<p>-- BOTH: import Mathlib.Algebra.Ring.Defs import Mathlib.Data.Real.Basic import MIL.Common</p> <p>/- TEXT: .. _proving_identities_in_algebraic_structures:</p>"},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#proving-identities-in-algebraic-structures","title":"Proving Identities in Algebraic Structures","text":"<p>.. index:: ring (algebraic structure)</p> <p>Mathematically, a ring consists of a collection of objects, :math:<code>R</code>, operations :math:<code>+</code> :math:<code>\\times</code>, and constants :math:<code>0</code> and :math:<code>1</code>, and an operation :math:<code>x \\mapsto -x</code> such that:</p> <ul> <li>:math:<code>R</code> with :math:<code>+</code> is an abelian group, with :math:<code>0</code>   as the additive identity and negation as inverse.</li> <li>Multiplication is associative with identity :math:<code>1</code>,   and multiplication distributes over addition.</li> </ul> <p>In Lean, the collection of objects is represented as a type, <code>R</code>. The ring axioms are as follows: TEXT. -/ section -- QUOTE: variable (R : Type*) [Ring R]</p>"},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_assoc-a-b-c-r-a-b-c-a-b-c","title":"check (add_assoc : \u2200 a b c : R, a + b + c = a + (b + c))","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_comm-a-b-r-a-b-b-a","title":"check (add_comm : \u2200 a b : R, a + b = b + a)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-zero_add-a-r-0-a-a","title":"check (zero_add : \u2200 a : R, 0 + a = a)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_left_neg-a-r-a-a-0","title":"check (add_left_neg : \u2200 a : R, -a + a = 0)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-mul_assoc-a-b-c-r-a-b-c-a-b-c","title":"check (mul_assoc : \u2200 a b c : R, a * b * c = a * (b * c))","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-mul_one-a-r-a-1-a","title":"check (mul_one : \u2200 a : R, a * 1 = a)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-one_mul-a-r-1-a-a","title":"check (one_mul : \u2200 a : R, 1 * a = a)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-mul_add-a-b-c-r-a-b-c-a-b-a-c","title":"check (mul_add : \u2200 a b c : R, a * (b + c) = a * b + a * c)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_mul-a-b-c-r-a-b-c-a-c-b-c","title":"check (add_mul : \u2200 a b c : R, (a + b) * c = a * c + b * c)","text":"<p>-- QUOTE.</p> <p>end</p> <p>/- TEXT: You will learn more about the square brackets in the first line later, but for the time being, suffice it to say that the declaration gives us a type, <code>R</code>, and a ring structure on <code>R</code>. Lean then allows us to use generic ring notation with elements of <code>R</code>, and to make use of a library of theorems about rings.</p> <p>The names of some of the theorems should look familiar: they are exactly the ones we used to calculate with the real numbers in the last section. Lean is good not only for proving things about concrete mathematical structures like the natural numbers and the integers, but also for proving things about abstract structures, characterized axiomatically, like rings. Moreover, Lean supports generic reasoning about both abstract and concrete structures, and can be trained to recognize appropriate instances. So any theorem about rings can be applied to concrete rings like the integers, <code>\u2124</code>, the rational numbers,  <code>\u211a</code>, and the complex numbers <code>\u2102</code>. It can also be applied to any instance of an abstract structure that extends rings, such as any ordered ring or any field.</p> <p>.. index:: commutative ring</p> <p>Not all important properties of the real numbers hold in an arbitrary ring, however. For example, multiplication on the real numbers is commutative, but that does not hold in general. If you have taken a course in linear algebra, you will recognize that, for every :math:<code>n</code>, the :math:<code>n</code> by :math:<code>n</code> matrices of real numbers form a ring in which commutativity usually fails. If we declare <code>R</code> to be a commutative ring, in fact, all the theorems in the last section continue to hold when we replace <code>\u211d</code> by <code>R</code>. TEXT. -/ section -- QUOTE: variable (R : Type*) [CommRing R] variable (a b c d : R)</p> <p>example : c * b * a = b * (a * c) := by ring</p> <p>example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b := by ring</p> <p>example : (a + b) * (a - b) = a ^ 2 - b ^ 2 := by ring</p> <p>example (hyp : c = d * a + b) (hyp' : b = a * d) : c = 2 * a * d := by   rw [hyp, hyp']   ring -- QUOTE.</p> <p>end</p> <p>/- TEXT: We leave it to you to check that all the other proofs go through unchanged. Notice that when a proof is short, like <code>by ring</code> or <code>by linarith</code> or <code>by sorry</code>, it is common (and permissible) to put it on the same line as the <code>by</code>. Good proof-writing style should strike a balance between concision and readability.</p> <p>The goal of this section is to strengthen the skills you have developed in the last section and apply them to reasoning axiomatically about rings. We will start with the axioms listed above, and use them to derive other facts. Most of the facts we prove are already in Mathlib. We will give the versions we prove the same names to help you learn the contents of the library as well as the naming conventions.</p> <p>.. index:: namespace, open, command ; open</p> <p>Lean provides an organizational mechanism similar to those used in programming languages: when a definition or theorem <code>foo</code> is introduced in a namespace <code>bar</code>, its full name is <code>bar.foo</code>. The command <code>open bar</code> later opens the namespace, which allows us to use the shorter name <code>foo</code>. To avoid errors due to name clashes, in the next example we put our versions of the library theorems in a new namespace called <code>MyRing.</code></p> <p>The next example shows that we do not need <code>add_zero</code> or <code>add_right_neg</code> as ring axioms, because they follow from the other axioms. TEXT. -/ -- QUOTE: namespace MyRing variable {R : Type*} [Ring R]</p> <p>theorem add_zero (a : R) : a + 0 = a := by rw [add_comm, zero_add]</p> <p>theorem add_right_neg (a : R) : a + -a = 0 := by rw [add_comm, add_left_neg]</p>"},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-myringadd_zero","title":"check MyRing.add_zero","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_zero","title":"check add_zero","text":"<p>end MyRing -- QUOTE.</p> <p>/- TEXT: The net effect is that we can temporarily reprove a theorem in the library, and then go on using the library version after that. But don't cheat! In the exercises that follow, take care to use only the general facts about rings that we have proved earlier in this section.</p> <p>(If you are paying careful attention, you may have noticed that we changed the round brackets in <code>(R : Type*)</code> for curly brackets in <code>{R : Type*}</code>. This declares <code>R</code> to be an implicit argument. We will explain what this means in a moment, but don't worry about it in the meanwhile.)</p> <p>Here is a useful theorem: TEXT. -/ -- BOTH: namespace MyRing variable {R : Type*} [Ring R]</p> <p>-- EXAMPLES: -- QUOTE: theorem neg_add_cancel_left (a b : R) : -a + (a + b) = b := by   rw [\u2190 add_assoc, add_left_neg, zero_add] -- QUOTE.</p> <p>/- TEXT: Prove the companion version: TEXT. -/ -- Prove these: -- QUOTE: theorem add_neg_cancel_right (a b : R) : a + b + -b = a := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem add_neg_cancel_right\u03b1\u03b1 (a b : R) : a + b + -b = a := by   rw [add_assoc, add_right_neg, add_zero]</p> <p>/- TEXT: Use these to prove the following: TEXT. -/ -- QUOTE: theorem add_left_cancel {a b c : R} (h : a + b = a + c) : b = c := by   sorry</p> <p>theorem add_right_cancel {a b c : R} (h : a + b = c + b) : a = c := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem add_left_cancel\u03b1\u03b1 {a b c : R} (h : a + b = a + c) : b = c := by   rw [\u2190 neg_add_cancel_left a b, h, neg_add_cancel_left]</p> <p>theorem add_right_cancel\u03b1\u03b1 {a b c : R} (h : a + b = c + b) : a = c := by   rw [\u2190 add_neg_cancel_right a b, h, add_neg_cancel_right]</p> <p>/- TEXT: With enough planning, you can do each of them with three rewrites.</p> <p>.. index:: implicit argument</p> <p>We will now explain the use of the curly braces. Imagine you are in a situation where you have <code>a</code>, <code>b</code>, and <code>c</code> in your context, as well as a hypothesis <code>h : a + b = a + c</code>, and you would like to draw the conclusion <code>b = c</code>. In Lean, you can apply a theorem to hypotheses and facts just the same way that you can apply them to objects, so you might think that <code>add_left_cancel a b c h</code> is a proof of the fact <code>b = c</code>. But notice that explicitly writing <code>a</code>, <code>b</code>, and <code>c</code> is redundant, because the hypothesis <code>h</code> makes it clear that those are the objects we have in mind. In this case, typing a few extra characters is not onerous, but if we wanted to apply <code>add_left_cancel</code> to more complicated expressions, writing them would be tedious. In cases like these, Lean allows us to mark arguments as implicit, meaning that they are supposed to be left out and inferred by other means, such as later arguments and hypotheses. The curly brackets in <code>{a b c : R}</code> do exactly that. So, given the statement of the theorem above, the correct expression is simply <code>add_left_cancel h</code>.</p> <p>To illustrate, let us show that <code>a * 0 = 0</code> follows from the ring axioms. TEXT. -/ -- QUOTE: theorem mul_zero (a : R) : a * 0 = 0 := by   have h : a * 0 + a * 0 = a * 0 + 0 := by     rw [\u2190 mul_add, add_zero, add_zero]   rw [add_left_cancel h] -- QUOTE.</p> <p>/- TEXT: .. index:: have, tactics ; have</p> <p>We have used a new trick! If you step through the proof, you can see what is going on. The <code>have</code> tactic introduces a new goal, <code>a * 0 + a * 0 = a * 0 + 0</code>, with the same context as the original goal. The fact that the next line is indented indicates that Lean is expecting a block of tactics that serves to prove this new goal. The indentation therefore promotes a modular style of proof: the indented subproof establishes the goal that was introduced by the <code>have</code>. After that, we are back to proving the original goal, except a new hypothesis <code>h</code> has been added: having proved it, we are now free to use it. At this point, the goal is exactly the result of <code>add_left_cancel h</code>.</p> <p>.. index:: apply, tactics ; apply, exact, tactics ; exact</p> <p>We could equally well have closed the proof with <code>apply add_left_cancel h</code> or <code>exact add_left_cancel h</code>. The <code>exact</code> tactic takes as argument a proof term which completely proves the current goal, without creating any new goal. The <code>apply</code> tactic is a variant whose argument is not necessarily a complete proof. The missing pieces are either inferred automatically by Lean or become new goals to prove. While the <code>exact</code> tactic is technically redundant since it is strictly less powerful than <code>apply</code>, it makes proof scripts slightly clearer to human readers and easier to maintain when the library evolves.</p> <p>Remember that multiplication is not assumed to be commutative, so the following theorem also requires some work. TEXT. -/ -- QUOTE: theorem zero_mul (a : R) : 0 * a = 0 := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem zero_mul\u03b1\u03b1 (a : R) : 0 * a = 0 := by   have h : 0 * a + 0 * a = 0 * a + 0 := by rw [\u2190 add_mul, add_zero, add_zero]   rw [add_left_cancel h]</p> <p>/- TEXT: By now, you should also be able replace each <code>sorry</code> in the next exercise with a proof, still using only facts about rings that we have established in this section. TEXT. -/ -- QUOTE: theorem neg_eq_of_add_eq_zero {a b : R} (h : a + b = 0) : -a = b := by   sorry</p> <p>theorem eq_neg_of_add_eq_zero {a b : R} (h : a + b = 0) : a = -b := by   sorry</p> <p>theorem neg_zero : (-0 : R) = 0 := by   apply neg_eq_of_add_eq_zero   rw [add_zero]</p> <p>theorem neg_neg (a : R) : - -a = a := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem neg_eq_of_add_eq_zero\u03b1\u03b1 {a b : R} (h : a + b = 0) : -a = b := by   rw [\u2190 neg_add_cancel_left a b, h, add_zero]</p> <p>theorem eq_neg_of_add_eq_zero\u03b1\u03b1 {a b : R} (h : a + b = 0) : a = -b := by   symm   apply neg_eq_of_add_eq_zero   rw [add_comm, h]</p> <p>theorem neg_zero\u03b1\u03b1 : (-0 : R) = 0 := by   apply neg_eq_of_add_eq_zero   rw [add_zero]</p> <p>theorem neg_neg\u03b1\u03b1 (a : R) : - -a = a := by   apply neg_eq_of_add_eq_zero   rw [add_left_neg]</p> <p>-- BOTH: end MyRing</p> <p>/- TEXT: We had to use the annotation <code>(-0 : R)</code> instead of <code>0</code> in the third theorem because without specifying <code>R</code> it is impossible for Lean to infer which <code>0</code> we have in mind, and by default it would be interpreted as a natural number.</p> <p>In Lean, subtraction in a ring is provably equal to addition of the additive inverse. TEXT. -/ -- Examples. section variable {R : Type*} [Ring R]</p> <p>-- QUOTE: example (a b : R) : a - b = a + -b :=   sub_eq_add_neg a b -- QUOTE.</p> <p>end</p> <p>/- TEXT: On the real numbers, it is defined that way: TEXT. -/ -- QUOTE: example (a b : \u211d) : a - b = a + -b :=   rfl</p> <p>example (a b : \u211d) : a - b = a + -b := by   rfl -- QUOTE.</p> <p>/- TEXT: .. index:: rfl, reflexivity, tactics ; refl and reflexivity, definitional equality</p> <p>The proof term <code>rfl</code> is short for \"reflexivity\". Presenting it as a proof of <code>a - b = a + -b</code> forces Lean to unfold the definition and recognize both sides as being the same. The <code>rfl</code> tactic does the same. This is an instance of what is known as a definitional equality in Lean's underlying logic. This means that not only can one rewrite with <code>sub_eq_add_neg</code> to replace <code>a - b = a + -b</code>, but in some contexts, when dealing with the real numbers, you can use the two sides of the equation interchangeably. For example, you now have enough information to prove the theorem <code>self_sub</code> from the last section: TEXT. -/ -- BOTH: namespace MyRing variable {R : Type*} [Ring R]</p> <p>-- EXAMPLES: -- QUOTE: theorem self_sub (a : R) : a - a = 0 := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem self_sub\u03b1\u03b1 (a : R) : a - a = 0 := by   rw [sub_eq_add_neg, add_right_neg]</p> <p>/- TEXT: Show that you can prove this using <code>rw</code>, but if you replace the arbitrary ring <code>R</code> by the real numbers, you can also prove it using either <code>apply</code> or <code>exact</code>.</p> <p>Lean knows that <code>1 + 1 = 2</code> holds in any ring. With a bit of effort, you can use that to prove the theorem <code>two_mul</code> from the last section: TEXT. -/ -- QUOTE: -- BOTH: theorem one_add_one_eq_two : 1 + 1 = (2 : R) := by   norm_num</p> <p>-- EXAMPLES: theorem two_mul (a : R) : 2 * a = a + a := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem two_mul\u03b1\u03b1 (a : R) : 2 * a = a + a := by   rw [\u2190 one_add_one_eq_two, add_mul, one_mul]</p> <p>-- BOTH: end MyRing</p> <p>/- TEXT: .. index:: group (algebraic structure)</p> <p>We close this section by noting that some of the facts about addition and negation that we established above do not need the full strength of the ring axioms, or even commutativity of addition. The weaker notion of a group can be axiomatized as follows: TEXT. -/ section -- QUOTE: variable (A : Type*) [AddGroup A]</p>"},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_assoc-a-b-c-a-a-b-c-a-b-c","title":"check (add_assoc : \u2200 a b c : A, a + b + c = a + (b + c))","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-zero_add-a-a-0-a-a","title":"check (zero_add : \u2200 a : A, 0 + a = a)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-add_left_neg-a-a-a-a-0","title":"check (add_left_neg : \u2200 a : A, -a + a = 0)","text":"<p>-- QUOTE.</p> <p>end</p> <p>/- TEXT: It is conventional to use additive notation when the group operation is commutative, and multiplicative notation otherwise. So Lean defines a multiplicative version as well as the additive version (and also their abelian variants, <code>AddCommGroup</code> and <code>CommGroup</code>). TEXT. -/ -- BOTH: section -- QUOTE: variable {G : Type*} [Group G]</p> <p>-- EXAMPLES:</p>"},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-mul_assoc-a-b-c-g-a-b-c-a-b-c","title":"check (mul_assoc : \u2200 a b c : G, a * b * c = a * (b * c))","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-one_mul-a-g-1-a-a","title":"check (one_mul : \u2200 a : G, 1 * a = a)","text":""},{"location":"MIL/C02_Basics/S02_Proving_Identities_in_Algebraic_Structures/#check-mul_left_inv-a-g-a1-a-1","title":"check (mul_left_inv : \u2200 a : G, a\u207b\u00b9 * a = 1)","text":"<p>-- QUOTE.</p> <p>/- TEXT: If you are feeling cocky, try proving the following facts about groups, using only these axioms. You will need to prove a number of helper lemmas along the way. The proofs we have carried out in this section provide some hints. TEXT. -/ -- BOTH: namespace MyGroup</p> <p>-- EXAMPLES: -- QUOTE: theorem mul_right_inv (a : G) : a * a\u207b\u00b9 = 1 := by   sorry</p> <p>theorem mul_one (a : G) : a * 1 = a := by   sorry</p> <p>theorem mul_inv_rev (a b : G) : (a * b)\u207b\u00b9 = b\u207b\u00b9 * a\u207b\u00b9 := by   sorry -- QUOTE.</p> <p>-- SOLUTIONS: theorem mul_right_inv\u03b1\u03b1 (a : G) : a * a\u207b\u00b9 = 1 := by   have h : (a * a\u207b\u00b9)\u207b\u00b9 * (a * a\u207b\u00b9 * (a * a\u207b\u00b9)) = 1 := by     rw [mul_assoc, \u2190 mul_assoc a\u207b\u00b9 a, mul_left_inv, one_mul, mul_left_inv]   rw [\u2190 h, \u2190 mul_assoc, mul_left_inv, one_mul]</p> <p>theorem mul_one\u03b1\u03b1 (a : G) : a * 1 = a := by   rw [\u2190 mul_left_inv a, \u2190 mul_assoc, mul_right_inv, one_mul]</p> <p>theorem mul_inv_rev\u03b1\u03b1 (a b : G) : (a * b)\u207b\u00b9 = b\u207b\u00b9 * a\u207b\u00b9 := by   rw [\u2190 one_mul (b\u207b\u00b9 * a\u207b\u00b9), \u2190 mul_left_inv (a * b), mul_assoc, mul_assoc, \u2190 mul_assoc b b\u207b\u00b9,     mul_right_inv, one_mul, mul_right_inv, mul_one]</p> <p>-- BOTH: end MyGroup</p> <p>end</p> <p>/- TEXT: .. index:: group (tactic), tactics ; group, tactics ; noncomm_ring, tactics ; abel</p> <p>Explicitly invoking those lemmas is tedious, so Mathlib provides tactics similar to <code>ring</code> in order to cover most uses: <code>group</code> is for non-commutative multiplicative groups, <code>abel</code> for abelian additive groups, and <code>noncomm_ring</code> for non-commutative rings. It may seem odd that the algebraic structures are called <code>Ring</code> and <code>CommRing</code> while the tactics are named <code>noncomm_ring</code> and <code>ring</code>. This is partly for historical reasons, but also for the convenience of using a shorter name for the tactic that deals with commutative rings, since it is used more often. TEXT. -/</p>"},{"location":"MIL/C02_Basics/S04_More_on_Order_and_Divisibility/","title":"S04 More on Order and Divisibility","text":""},{"location":"MIL/C02_Basics/S04_More_on_Order_and_Divisibility/#more-examples-using-apply-and-rw","title":"More examples using apply and rw","text":"<p>The <code>min</code> function on the real numbers is uniquely characterized by the following three facts: section</p> <pre><code>#check (min_le_left a b : min a b \u2264 a)\n#check (min_le_right a b : min a b \u2264 b)\n#check (le_min : c \u2264 a \u2192 c \u2264 b \u2192 c \u2264 min a b)\n</code></pre> <p>Can you guess the names of the theorems that characterize <code>max</code> in a similar way?</p> <p>Notice that we have to apply <code>min</code> to a pair of arguments <code>a</code> and <code>b</code> by writing <code>min a b</code> rather than <code>min (a, b)</code>. Formally, <code>min</code> is a function of type <code>\u211d \u2192 \u211d \u2192 \u211d</code>. When we write a type like this with multiple arrows, the convention is that the implicit parentheses associate to the right, so the type is interpreted as <code>\u211d \u2192 (\u211d \u2192 \u211d)</code>. The net effect is that if <code>a</code> and <code>b</code> have type <code>\u211d</code> then <code>min a</code> has type <code>\u211d \u2192 \u211d</code> and <code>min a b</code> has type <code>\u211d</code>, so <code>min</code> acts like a function of two arguments, as we expect. Handling multiple arguments in this way is known as currying, after the logician Haskell Curry.</p> <p>The order of operations in Lean can also take some getting used to. Function application binds tighter than infix operations, so the expression <code>min a b + c</code> is interpreted as <code>(min a b) + c</code>. With time, these conventions will become second nature.</p> <p>Using the theorem <code>le_antisymm</code>, we can show that two real numbers are equal if each is less than or equal to the other. Using this and the facts above, we can show that <code>min</code> is commutative:</p> <pre><code>example : min a b = min b a := by\n  apply le_antisymm\n  \u00b7 show min a b \u2264 min b a\n    apply le_min\n    \u00b7 apply min_le_right\n    apply min_le_left\n  \u00b7 show min b a \u2264 min a b\n    apply le_min\n    \u00b7 apply min_le_right\n    apply min_le_left\n</code></pre> <p>Here we have used dots to separate proofs of different goals. Our usage is inconsistent: at the outer level, we use dots and indentation for both goals, whereas for the nested proofs, we use dots only until a single goal remains. Both conventions are reasonable and useful. We also use the <code>show</code> tactic to structure the proof and indicate what is being proved in each block. The proof still works without the <code>show</code> commands, but using them makes the proof easier to read and maintain.</p> <p>It may bother you that the proof is repetitive. To foreshadow skills you will learn later on, we note that one way to avoid the repetition is to state a local lemma and then use it:</p> <pre><code>example : min a b = min b a := by\n  have h : \u2200 x y : \u211d, min x y \u2264 min y x := by\n    intro x y\n    apply le_min\n    apply min_le_right\n    apply min_le_left\n  apply le_antisymm\n  apply h\n  apply h\n</code></pre> <p>We will say more about the universal quantifier in :numref:<code>implication_and_the_universal_quantifier</code>, but suffice it to say here that the hypothesis <code>h</code> says that the desired inequality holds for any <code>x</code> and <code>y</code>, and the <code>intro</code> tactic introduces an arbitrary <code>x</code> and <code>y</code> to establish the conclusion. The first <code>apply</code> after <code>le_antisymm</code> implicitly uses <code>h a b</code>, whereas the second one uses <code>h b a</code>.</p> <p>Another solution is to use the <code>repeat</code> tactic, which applies a tactic (or a block) as many times as it can.</p> <pre><code>example : min a b = min b a := by\n  apply le_antisymm\n  repeat\n    apply le_min\n    apply min_le_right\n    apply min_le_left\n</code></pre> <p>We encourage you to prove the following as exercises. You can use either of the tricks just described to shorten the first.</p> <pre><code>example : max a b = max b a := by\n  sorry\nexample : min (min a b) c = min a (min b c) := by\n  sorry\n</code></pre> <p>Of course, you are welcome to prove the associativity of <code>max</code> as well.</p> <p>It is an interesting fact that <code>min</code> distributes over <code>max</code> the way that multiplication distributes over addition, and vice-versa. In other words, on the real numbers, we have the identity <code>min a (max b c) = max (min a b) (min a c)</code> as well as the corresponding version with <code>max</code> and <code>min</code> switched. But in the next section we will see that this does not follow from the transitivity and reflexivity of <code>\u2264</code> and the characterizing properties of <code>min</code> and <code>max</code> enumerated above. We need to use the fact that <code>\u2264</code> on the real numbers is a total order, which is to say, it satisfies <code>\u2200 x y, x \u2264 y \u2228 y \u2264 x</code>. Here the disjunction symbol, <code>\u2228</code>, represents \"or\". In the first case, we have <code>min x y = x</code>, and in the second case, we have <code>min x y = y</code>. We will learn how to reason by cases in :numref:<code>disjunction</code>, but for now we will stick to examples that don't require the case split.</p> <p>Here is one such example:</p> <pre><code>theorem aux : min a b + c \u2264 min (a + c) (b + c) := by\n  sorry\nexample : min a b + c = min (a + c) (b + c) := by\n  sorry\n</code></pre> <p>It is clear that <code>aux</code> provides one of the two inequalities needed to prove the equality, but applying it to suitable values yields the other direction as well. As a hint, you can use the theorem <code>add_neg_cancel_right</code> and the <code>linarith</code> tactic.</p> <p>Lean's naming convention is made manifest in the library's name for the triangle inequality:</p> <pre><code>#check (abs_add : \u2200 a b : \u211d, |a + b| \u2264 |a| + |b|)\n</code></pre> <p>Use it to prove the following variant, using also <code>add_sub_cancel_right</code>:</p> <pre><code>example : |a| - |b| \u2264 |a - b| :=\n  sorry\nend\n</code></pre> <p>See if you can do this in three lines or less. You can use the theorem <code>sub_add_cancel</code>.</p> <p>Another important relation that we will make use of in the sections to come is the divisibility relation on the natural numbers, <code>x \u2223 y</code>. Be careful: the divisibility symbol is not the ordinary bar on your keyboard. Rather, it is a unicode character obtained by typing <code>\\|</code> in VS Code. By convention, Mathlib uses <code>dvd</code> to refer to it in theorem names.</p> <pre><code>example (h\u2080 : x \u2223 y) (h\u2081 : y \u2223 z) : x \u2223 z :=\n  dvd_trans h\u2080 h\u2081\n\nexample : x \u2223 y * x * z := by\n  apply dvd_mul_of_dvd_left\n  apply dvd_mul_left\n\nexample : x \u2223 x ^ 2 := by\n  apply dvd_mul_left\n</code></pre> <p>In the last example, the exponent is a natural number, and applying <code>dvd_mul_left</code> forces Lean to expand the definition of <code>x^2</code> to <code>x^1 * x</code>. See if you can guess the names of the theorems you need to prove the following:</p> <pre><code>example (h : x \u2223 w) : x \u2223 y * (x * z) + x ^ 2 + w ^ 2 := by\n  sorry\nend\n</code></pre> <p>With respect to divisibility, the greatest common divisor, <code>gcd</code>, and least common multiple, <code>lcm</code>, are analogous to <code>min</code> and <code>max</code>. Since every number divides <code>0</code>, <code>0</code> is really the greatest element with respect to divisibility:</p> <pre><code>variable (m n : \u2115)\n\n#check (Nat.gcd_zero_right n : Nat.gcd n 0 = n)\n#check (Nat.gcd_zero_left n : Nat.gcd 0 n = n)\n#check (Nat.lcm_zero_right n : Nat.lcm n 0 = 0)\n#check (Nat.lcm_zero_left n : Nat.lcm 0 n = 0)\n</code></pre> <p>See if you can guess the names of the theorems you will need to prove the following:</p> <pre><code>example : Nat.gcd m n = Nat.gcd n m := by\n  sorry\n</code></pre> <p>Hint: you can use <code>dvd_antisymm</code>, but if you do, Lean will complain that the expression is ambiguous between the generic theorem and the version <code>Nat.dvd_antisymm</code>, the one specifically for the natural numbers. You can use <code>_root_.dvd_antisymm</code> to specify the generic one; either one will work.</p>"},{"location":"MIL/C02_Basics/S05_Proving_Facts_about_Algebraic_Structures/","title":"S05 Proving Facts about Algebraic Structures","text":""},{"location":"MIL/C02_Basics/S05_Proving_Facts_about_Algebraic_Structures/#proving-facts-about-algebraic-structures","title":"Proving Facts about Algebraic Structures","text":"<p>In :numref:<code>proving_identities_in_algebraic_structures</code>, we saw that many common identities governing the real numbers hold in more general classes of algebraic structures, such as commutative rings. We can use any axioms we want to describe an algebraic structure, not just equations. For example, a partial order consists of a set with a binary relation that is reflexive, transitive, and antisymmetric. like <code>\u2264</code> on the real numbers. Lean knows about partial orders: TEXT. -/ section</p> <pre><code>variable {\u03b1 : Type*} [PartialOrder \u03b1]\nvariable (x y z : \u03b1)\n\n#check x \u2264 y\n#check (le_refl x : x \u2264 x)\n#check (le_trans : x \u2264 y \u2192 y \u2264 z \u2192 x \u2264 z)\n#check (le_antisymm : x \u2264 y \u2192 y \u2264 x \u2192 x = y)\n</code></pre> <p>Here we are adopting the Mathlib convention of using letters like <code>\u03b1</code>, <code>\u03b2</code>, and <code>\u03b3</code> (entered as <code>\\a</code>, <code>\\b</code>, and <code>\\g</code>) for arbitrary types. The library often uses letters like <code>R</code> and <code>G</code> for the carriers of algebraic structures like rings and groups, respectively, but in general Greek letters are used for types, especially when there is little or no structure associated with them.</p> <p>Associated to any partial order, <code>\u2264</code>, there is also a strict partial order, <code>&lt;</code>, which acts somewhat like <code>&lt;</code> on the real numbers. Saying that <code>x</code> is less than <code>y</code> in this order is equivalent to saying that it is less-than-or-equal to <code>y</code> and not equal to <code>y</code>.</p> <pre><code>#check x &lt; y\n#check (lt_irrefl x : \u00ac (x &lt; x))\n#check (lt_trans : x &lt; y \u2192 y &lt; z \u2192 x &lt; z)\n#check (lt_of_le_of_lt : x \u2264 y \u2192 y &lt; z \u2192 x &lt; z)\n#check (lt_of_lt_of_le : x &lt; y \u2192 y \u2264 z \u2192 x &lt; z)\n\nexample : x &lt; y \u2194 x \u2264 y \u2227 x \u2260 y :=\n  lt_iff_le_and_ne\n</code></pre> <p>In this example, the symbol <code>\u2227</code> stands for \"and,\" the symbol <code>\u00ac</code> stands for \"not,\" and <code>x \u2260 y</code> abbreviates <code>\u00ac (x = y)</code>. In :numref:<code>Chapter %s &lt;logic&gt;</code>, you will learn how to use these logical connectives to prove that <code>&lt;</code> has the properties indicated.</p> <p>A lattice is a structure that extends a partial order with operations <code>\u2293</code> and <code>\u2294</code> that are analogous to <code>min</code> and <code>max</code> on the real numbers:</p> <pre><code>variable {\u03b1 : Type*} [Lattice \u03b1]\nvariable (x y z : \u03b1)\n\n#check x \u2293 y\n#check (inf_le_left : x \u2293 y \u2264 x)\n#check (inf_le_right : x \u2293 y \u2264 y)\n#check (le_inf : z \u2264 x \u2192 z \u2264 y \u2192 z \u2264 x \u2293 y)\n#check x \u2294 y\n#check (le_sup_left : x \u2264 x \u2294 y)\n#check (le_sup_right : y \u2264 x \u2294 y)\n#check (sup_le : x \u2264 z \u2192 y \u2264 z \u2192 x \u2294 y \u2264 z)\n</code></pre> <p>The characterizations of <code>\u2293</code> and <code>\u2294</code> justify calling them the greatest lower bound and least upper bound, respectively. You can type them in VS code using <code>\\glb</code> and <code>\\lub</code>. The symbols are also often called then infimum and the supremum, and Mathlib refers to them as <code>inf</code> and <code>sup</code> in theorem names. To further complicate matters, they are also often called meet and join. Therefore, if you work with lattices, you have to keep the following dictionary in mind:</p> <ul> <li> <p><code>\u2293</code> is the greatest lower bound, infimum, or meet.</p> </li> <li> <p><code>\u2294</code> is the least upper bound, supremum, or join.</p> </li> </ul> <p>Some instances of lattices include:</p> <ul> <li> <p><code>min</code> and <code>max</code> on any total order, such as the integers or real numbers with <code>\u2264</code></p> </li> <li> <p><code>\u2229</code> and <code>\u222a</code> on the collection of subsets of some domain, with the ordering <code>\u2286</code></p> </li> <li> <p><code>\u2227</code> and <code>\u2228</code> on boolean truth values, with ordering <code>x \u2264 y</code> if either <code>x</code> is false or <code>y</code> is true</p> </li> <li> <p><code>gcd</code> and <code>lcm</code> on the natural numbers (or positive natural numbers), with the divisibility ordering, <code>\u2223</code></p> </li> <li> <p>the collection of linear subspaces of a vector space,   where the greatest lower bound is given by the intersection,   the least upper bound is given by the sum of the two spaces,   and the ordering is inclusion</p> </li> <li> <p>the collection of topologies on a set (or, in Lean, a type),   where the greatest lower bound of two topologies consists of   the topology that is generated by their union,   the least upper bound is their intersection,   and the ordering is reverse inclusion</p> </li> </ul> <p>You can check that, as with <code>min</code> / <code>max</code> and <code>gcd</code> / <code>lcm</code>, you can prove the commutativity and associativity of the infimum and supremum using only their characterizing axioms, together with <code>le_refl</code> and <code>le_trans</code>.</p> <p>Using <code>apply le_trans</code> when seeing a goal <code>x \u2264 z</code> is not a great idea. Indeed Lean has no way to guess which intermediate element <code>y</code> we want to use. So <code>apply le_trans</code> produces three goals that look like<code>x \u2264 ?a</code>, <code>?a \u2264 z</code> and <code>\u03b1</code> where <code>?a</code> (probably with a more complicated auto-generated name) stands for the mysterious <code>y</code>. The last goal, with type <code>\u03b1</code>, is to provide the value of <code>y</code>. It comes lasts because Lean hopes to automatically infer it from the proof of the first goal <code>x \u2264 ?a</code>. In order to avoid this unappealing situation, you can use the <code>calc</code> tactic to explicitly provide <code>y</code>. Alternatively, you can use the <code>trans</code> tactic which takes <code>y</code> as an argument and produces the expected goals <code>x \u2264 y</code> and <code>y \u2264 z</code>. Of course you can also avoid this issue by providing directly a full proof such as <code>exact le_trans inf_le_left inf_le_right</code>, but this requires a lot more planning.</p> <pre><code>example : x \u2293 y = y \u2293 x := by\n  sorry\n\nexample : x \u2293 y \u2293 z = x \u2293 (y \u2293 z) := by\n  sorry\n\nexample : x \u2294 y = y \u2294 x := by\n  sorry\n\nexample : x \u2294 y \u2294 z = x \u2294 (y \u2294 z) := by\n  sorry\n</code></pre> <p>You can find these theorems in the Mathlib as <code>inf_comm</code>, <code>inf_assoc</code>, <code>sup_comm</code>, and <code>sup_assoc</code>, respectively.</p> <p>Another good exercise is to prove the absorption laws using only those axioms:</p> <pre><code>theorem absorb1 : x \u2293 (x \u2294 y) = x := by\n  sorry\n\ntheorem absorb2 : x \u2294 x \u2293 y = x := by\n  sorry\n</code></pre> <p>These can be found in Mathlib with the names <code>inf_sup_self</code> and <code>sup_inf_self</code>.</p> <p>A lattice that satisfies the additional identities <code>x \u2293 (y \u2294 z) = (x \u2293 y) \u2294 (x \u2293 z)</code> and <code>x \u2294 (y \u2293 z) = (x \u2294 y) \u2293 (x \u2294 z)</code> is called a distributive lattice. Lean knows about these too:</p> <pre><code>variable {\u03b1 : Type*} [DistribLattice \u03b1]\nvariable (x y z : \u03b1)\n\n#check (inf_sup_left x y z : x \u2293 (y \u2294 z) = x \u2293 y \u2294 x \u2293 z)\n#check (inf_sup_right x y z : (x \u2294 y) \u2293 z = x \u2293 z \u2294 y \u2293 z)\n#check (sup_inf_left x y z : x \u2294 y \u2293 z = (x \u2294 y) \u2293 (x \u2294 z))\n#check (sup_inf_right x y z : x \u2293 y \u2294 z = (x \u2294 z) \u2293 (y \u2294 z))\n</code></pre> <p>The left and right versions are easily shown to be equivalent, given the commutativity of <code>\u2293</code> and <code>\u2294</code>. It is a good exercise to show that not every lattice is distributive by providing an explicit description of a nondistributive lattice with finitely many elements. It is also a good exercise to show that in any lattice, either distributivity law implies the other:</p> <pre><code>variable {\u03b1 : Type*} [Lattice \u03b1]\nvariable (a b c : \u03b1)\n\nexample (h : \u2200 x y z : \u03b1, x \u2293 (y \u2294 z) = x \u2293 y \u2294 x \u2293 z) : a \u2294 b \u2293 c = (a \u2294 b) \u2293 (a \u2294 c) := by\n  sorry\n\nexample (h : \u2200 x y z : \u03b1, x \u2294 y \u2293 z = (x \u2294 y) \u2293 (x \u2294 z)) : a \u2293 (b \u2294 c) = a \u2293 b \u2294 a \u2293 c := by\n  sorry\n</code></pre> <p>It is possible to combine axiomatic structures into larger ones. For example, a strict ordered ring consists of a commutative ring together with a partial order on the carrier satisfying additional axioms that say that the ring operations are compatible with the order:</p> <pre><code>variable {R : Type*} [StrictOrderedRing R]\nvariable (a b c : R)\n\n#check (add_le_add_left : a \u2264 b \u2192 \u2200 c, c + a \u2264 c + b)\n#check (mul_pos : 0 &lt; a \u2192 0 &lt; b \u2192 0 &lt; a * b)\n</code></pre> <p>:numref:<code>Chapter %s &lt;logic&gt;</code> will provide the means to derive the following from <code>mul_pos</code> and the definition of <code>&lt;</code>:</p> <pre><code>#check (mul_nonneg : 0 \u2264 a \u2192 0 \u2264 b \u2192 0 \u2264 a * b)\n</code></pre> <p>It is then an extended exercise to show that many common facts used to reason about arithmetic and the ordering on the real numbers hold generically for any ordered ring. Here are a couple of examples you can try, using only properties of rings, partial orders, and the facts enumerated in the last two examples:</p> <pre><code>example (h : a \u2264 b) : 0 \u2264 b - a := by\n  sorry\n\nexample (h: 0 \u2264 b - a) : a \u2264 b := by\n  sorry\n\nexample (h : a \u2264 b) (h' : 0 \u2264 c) : a * c \u2264 b * c := by\n  sorry\n</code></pre> <p>Finally, here is one last example. A metric space consists of a set equipped with a notion of distance, <code>dist x y</code>, mapping any pair of elements to a real number. The distance function is assumed to satisfy the following axioms:</p> <pre><code>variable {X : Type*} [MetricSpace X]\nvariable (x y z : X)\n\n#check (dist_self x : dist x x = 0)\n#check (dist_comm x y : dist x y = dist y x)\n#check (dist_triangle x y z : dist x z \u2264 dist x y + dist y z)\n</code></pre> <p>Having mastered this section, you can show that it follows from these axioms that distances are always nonnegative:</p> <pre><code>example (x y : X) : 0 \u2264 dist x y := by\n  sorry\n</code></pre> <p>We recommend making use of the theorem <code>nonneg_of_mul_nonneg_left</code>. As you may have guessed, this theorem is called <code>dist_nonneg</code> in Mathlib.</p>"},{"location":"MIL/C03_Logic/S01_Implication_and_the_Universal_Quantifier/","title":"S01 Implication and the Universal Quantifier","text":""},{"location":"MIL/C03_Logic/S01_Implication_and_the_Universal_Quantifier/#implication-and-the-universal-quantifier","title":"Implication and the Universal Quantifier","text":"<p>Consider the statement after the <code>#check</code>:</p> <pre><code>#check \u2200 x : \u211d, 0 \u2264 x \u2192 |x| = x\n</code></pre> <p>In words, we would say \"for every real number <code>x</code>, if <code>0 \u2264 x</code> then the absolute value of <code>x</code> equals <code>x</code>\". We can also have more complicated statements like:</p> <pre><code>#check \u2200 x y \u03b5 : \u211d, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5\n</code></pre> <p>In words, we would say \"for every <code>x</code>, <code>y</code>, and <code>\u03b5</code>, if <code>0 &lt; \u03b5 \u2264 1</code>, the absolute value of <code>x</code> is less than <code>\u03b5</code>, and the absolute value of <code>y</code> is less than <code>\u03b5</code>, then the absolute value of <code>x * y</code> is less than <code>\u03b5</code>.\" In Lean, in a sequence of implications there are implicit parentheses grouped to the right. So the expression above means \"if <code>0 &lt; \u03b5</code> then if <code>\u03b5 \u2264 1</code> then if <code>|x| &lt; \u03b5</code> ...\" As a result, the expression says that all the assumptions together imply the conclusion.</p> <p>You have already seen that even though the universal quantifier in this statement ranges over objects and the implication arrows introduce hypotheses, Lean treats the two in very similar ways. In particular, if you have proved a theorem of that form, you can apply it to objects and hypotheses in the same way. We will use as an example the following statement that we will help you to prove a bit later:</p> <pre><code>theorem my_lemma : \u2200 x y \u03b5 : \u211d, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 :=\n  sorry\n\nsection\nvariable (a b \u03b4 : \u211d)\nvariable (h\u2080 : 0 &lt; \u03b4) (h\u2081 : \u03b4 \u2264 1)\nvariable (ha : |a| &lt; \u03b4) (hb : |b| &lt; \u03b4)\n\n#check my_lemma a b \u03b4\n#check my_lemma a b \u03b4 h\u2080 h\u2081\n#check my_lemma a b \u03b4 h\u2080 h\u2081 ha hb\n\nend\n</code></pre> <p>You have also already seen that it is common in Lean to use curly brackets to make quantified variables implicit when they can be inferred from subsequent hypotheses. When we do that, we can just apply a lemma to the hypotheses without mentioning the objects.</p> <pre><code>theorem my_lemma2 : \u2200 {x y \u03b5 : \u211d}, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 :=\n  sorry\n\nsection\nvariable (a b \u03b4 : \u211d)\nvariable (h\u2080 : 0 &lt; \u03b4) (h\u2081 : \u03b4 \u2264 1)\nvariable (ha : |a| &lt; \u03b4) (hb : |b| &lt; \u03b4)\n\n#check my_lemma2 h\u2080 h\u2081 ha hb\n\nend\n</code></pre> <p>At this stage, you also know that if you use the <code>apply</code> tactic to apply <code>my_lemma</code> to a goal of the form <code>|a * b| &lt; \u03b4</code>, you are left with new goals that require you to prove each of the hypotheses.</p> <p>To prove a statement like this, use the <code>intro</code> tactic. Take a look at what it does in this example:</p> <pre><code>theorem my_lemma3 :\n    \u2200 {x y \u03b5 : \u211d}, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 := by\n  intro x y \u03b5 epos ele1 xlt ylt\n  sorry\n</code></pre> <p>We can use any names we want for the universally quantified variables; they do not have to be <code>x</code>, <code>y</code>, and <code>\u03b5</code>. Notice that we have to introduce the variables even though they are marked implicit: making them implicit means that we leave them out when we write an expression using <code>my_lemma</code>, but they are still an essential part of the statement that we are proving. After the <code>intro</code> command, the goal is what it would have been at the start if we listed all the variables and hypotheses before the colon, as we did in the last section. In a moment, we will see why it is sometimes necessary to introduce variables and hypotheses after the proof begins.</p> <p>To help you prove the lemma, we will start you off:</p> <pre><code>theorem my_lemma4 :\n    \u2200 {x y \u03b5 : \u211d}, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 := by\n  intro x y \u03b5 epos ele1 xlt ylt\n  calc\n    |x * y| = |x| * |y| := sorry\n    _ \u2264 |x| * \u03b5 := sorry\n    _ &lt; 1 * \u03b5 := sorry\n    _ = \u03b5 := sorry\n</code></pre> <p>Finish the proof using the theorems <code>abs_mul</code>, <code>mul_le_mul</code>, <code>abs_nonneg</code>, <code>mul_lt_mul_right</code>, and <code>one_mul</code>. Remember that you can find theorems like these using Ctrl-space completion (or Cmd-space completion on a Mac). Remember also that you can use <code>.mp</code> and <code>.mpr</code> or <code>.1</code> and <code>.2</code> to extract the two directions of an if-and-only-if statement.</p> <p>Universal quantifiers are often hidden in definitions, and Lean will unfold definitions to expose them when necessary. For example, let's define two predicates, <code>FnUb f a</code> and <code>FnLb f a</code>, where <code>f</code> is a function from the real numbers to the real numbers and <code>a</code> is a real number. The first says that <code>a</code> is an upper bound on the values of <code>f</code>, and the second says that <code>a</code> is a lower bound on the values of <code>f</code>.</p> <pre><code>def FnUb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, f x \u2264 a\n\ndef FnLb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, a \u2264 f x\n</code></pre> <p>In the next example, <code>fun x \u21a6 f x + g x</code> is the function that maps <code>x</code> to <code>f x + g x</code>. Going from the expression <code>f x + g x</code> to this function is called a lambda abstraction in type theory.</p> <pre><code>example (hfa : FnUb f a) (hgb : FnUb g b) : FnUb (fun x \u21a6 f x + g x) (a + b) := by\n  intro x\n  dsimp\n  apply add_le_add\n  apply hfa\n  apply hgb\n</code></pre> <p>Applying <code>intro</code> to the goal <code>FnUb (fun x \u21a6 f x + g x) (a + b)</code> forces Lean to unfold the definition of <code>FnUb</code> and introduce <code>x</code> for the universal quantifier. The goal is then <code>(fun (x : \u211d) \u21a6 f x + g x) x \u2264 a + b</code>. But applying <code>(fun x \u21a6 f x + g x)</code> to <code>x</code> should result in <code>f x + g x</code>, and the <code>dsimp</code> command performs that simplification. (The \"d\" stands for \"definitional.\") You can delete that command and the proof still works; Lean would have to perform that contraction anyhow to make sense of the next <code>apply</code>. The <code>dsimp</code> command simply makes the goal more readable and helps us figure out what to do next. Another option is to use the <code>change</code> tactic by writing <code>change f x + g x \u2264 a + b</code>. This helps make the proof more readable, and gives you more control over how the goal is transformed.</p> <p>The rest of the proof is routine. The last two <code>apply</code> commands force Lean to unfold the definitions of <code>FnUb</code> in the hypotheses. Try carrying out similar proofs of these:</p> <pre><code>example (hfa : FnLb f a) (hgb : FnLb g b) : FnLb (fun x \u21a6 f x + g x) (a + b) :=\n  sorry\n\nexample (nnf : FnLb f 0) (nng : FnLb g 0) : FnLb (fun x \u21a6 f x * g x) 0 :=\n  sorry\n\nexample (hfa : FnUb f a) (hgb : FnUb g b) (nng : FnLb g 0) (nna : 0 \u2264 a) :\n    FnUb (fun x \u21a6 f x * g x) (a * b) :=\n  sorry\n</code></pre> <p>Even though we have defined <code>FnUb</code> and <code>FnLb</code> for functions from the reals to the reals, you should recognize that the definitions and proofs are much more general. The definitions make sense for functions between any two types for which there is a notion of order on the codomain. Checking the type of the theorem <code>add_le_add</code> shows that it holds of any structure that is an \"ordered additive commutative monoid\"; the details of what that means don't matter now, but it is worth knowing that the natural numbers, integers, rationals, and real numbers are all instances. So if we prove the theorem <code>fnUb_add</code> at that level of generality, it will apply in all these instances.</p> <pre><code>variable {\u03b1 : Type*} {R : Type*} [OrderedCancelAddCommMonoid R]\n\n#check add_le_add\n\ndef FnUb' (f : \u03b1 \u2192 R) (a : R) : Prop :=\n  \u2200 x, f x \u2264 a\n\ntheorem fnUb_add {f g : \u03b1 \u2192 R} {a b : R} (hfa : FnUb' f a) (hgb : FnUb' g b) :\n    FnUb' (fun x \u21a6 f x + g x) (a + b) := fun x \u21a6 add_le_add (hfa x) (hgb x)\n</code></pre> <p>You have already seen square brackets like these in Section :numref:<code>proving_identities_in_algebraic_structures</code>, though we still haven't explained what they mean. For concreteness, we will stick to the real numbers for most of our examples, but it is worth knowing that Mathlib contains definitions and theorems that work at a high level of generality.</p> <p>For another example of a hidden universal quantifier, Mathlib defines a predicate <code>Monotone</code>, which says that a function is nondecreasing in its arguments:</p> <pre><code>example (f : \u211d \u2192 \u211d) (h : Monotone f) : \u2200 {a b}, a \u2264 b \u2192 f a \u2264 f b :=\n  @h\n</code></pre> <p>The property <code>Monotone f</code> is defined to be exactly the expression after the colon. We need to put the <code>@</code> symbol before <code>h</code> because if we don't, Lean expands the implicit arguments to <code>h</code> and inserts placeholders.</p> <p>Proving statements about monotonicity involves using <code>intro</code> to introduce two variables, say, <code>a</code> and <code>b</code>, and the hypothesis <code>a \u2264 b</code>. To use a monotonicity hypothesis, you can apply it to suitable arguments and hypotheses, and then apply the resulting expression to the goal. Or you can apply it to the goal and let Lean help you work backwards by displaying the remaining hypotheses as new subgoals.</p> <pre><code>example (mf : Monotone f) (mg : Monotone g) : Monotone fun x \u21a6 f x + g x := by\n  intro a b aleb\n  apply add_le_add\n  apply mf aleb\n  apply mg aleb\n</code></pre> <p>When a proof is this short, it is often convenient to give a proof term instead. To describe a proof that temporarily introduces objects <code>a</code> and <code>b</code> and a hypothesis <code>aleb</code>, Lean uses the notation <code>fun a b aleb \u21a6 ...</code>. This is analogous to the way that an expression like <code>fun x \u21a6 x^2</code> describes a function by temporarily naming an object, <code>x</code>, and then using it to describe a value. So the <code>intro</code> command in the previous proof corresponds to the lambda abstraction in the next proof term. The <code>apply</code> commands then correspond to building the application of the theorem to its arguments.</p> <pre><code>example (mf : Monotone f) (mg : Monotone g) : Monotone fun x \u21a6 f x + g x :=\n  fun a b aleb \u21a6 add_le_add (mf aleb) (mg aleb)\n</code></pre> <p>Here is a useful trick: if you start writing the proof term <code>fun a b aleb \u21a6 _</code> using an underscore where the rest of the expression should go, Lean will flag an error, indicating that it can't guess the value of that expression. If you check the Lean Goal window in VS Code or hover over the squiggly error marker, Lean will show you the goal that the remaining expression has to solve.</p> <p>Try proving these, with either tactics or proof terms:</p> <pre><code>example {c : \u211d} (mf : Monotone f) (nnc : 0 \u2264 c) : Monotone fun x \u21a6 c * f x :=\n  sorry\n\nexample (mf : Monotone f) (mg : Monotone g) : Monotone fun x \u21a6 f (g x) :=\n  sorry\n</code></pre> <p>Here are some more examples. A function :math:<code>f</code> from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) is said to be even if \\(f(-x) = f(x)\\) for every \\(x\\), and odd if \\(f(-x) = -f(x)\\) for every \\(x\\). The following example defines these two notions formally and establishes one fact about them. You can complete the proofs of the others.</p> <pre><code>def FnEven (f : \u211d \u2192 \u211d) : Prop :=\n  \u2200 x, f x = f (-x)\n\ndef FnOdd (f : \u211d \u2192 \u211d) : Prop :=\n  \u2200 x, f x = -f (-x)\n\nexample (ef : FnEven f) (eg : FnEven g) : FnEven fun x \u21a6 f x + g x := by\n  intro x\n  calc\n    (fun x \u21a6 f x + g x) x = f x + g x := rfl\n    _ = f (-x) + g (-x) := by rw [ef, eg]\n\n\nexample (of : FnOdd f) (og : FnOdd g) : FnEven fun x \u21a6 f x * g x := by\n  sorry\n\nexample (ef : FnEven f) (og : FnOdd g) : FnOdd fun x \u21a6 f x * g x := by\n  sorry\n\nexample (ef : FnEven f) (og : FnOdd g) : FnEven fun x \u21a6 f (g x) := by\n  sorry\n</code></pre> <p>The first proof can be shortened using <code>dsimp</code> or <code>change</code> to get rid of the lambda abstraction. But you can check that the subsequent <code>rw</code> won't work unless we get rid of the lambda abstraction explicitly, because otherwise it cannot find the patterns <code>f x</code> and <code>g x</code> in the expression. Contrary to some other tactics, <code>rw</code> operates on the syntactic level, it won't unfold definitions or apply reductions for you (it has a variant called <code>erw</code> that tries a little harder in this direction, but not much harder).</p> <p>You can find implicit universal quantifiers all over the place, once you know how to spot them.</p> <p>Mathlib includes a good library for manipulating sets. Recall that Lean does not use foundations based on set theory, so here the word set has its mundane meaning of a collection of mathematical objects of some given type <code>\u03b1</code>. If <code>x</code> has type <code>\u03b1</code> and <code>s</code> has type <code>Set \u03b1</code>, then <code>x \u2208 s</code> is a proposition that asserts that <code>x</code> is an element of <code>s</code>. If <code>y</code> has some different type <code>\u03b2</code> then the expression <code>y \u2208 s</code> makes no sense. Here \"makes no sense\" means \"has no type hence Lean does not accept it as a well-formed statement\". This contrasts with Zermelo-Fraenkel set theory for instance where <code>a \u2208 b</code> is a well-formed statement for every mathematical objects <code>a</code> and <code>b</code>. For instance <code>sin \u2208 cos</code> is a well-formed statement in ZF. This defect of set theoretic foundations is an important motivation for not using it in a proof assistant which is meant to assist us by detecting meaningless expressions. In Lean <code>sin</code> has type <code>\u211d \u2192 \u211d</code> and <code>cos</code> has type <code>\u211d \u2192 \u211d</code> which is not equal to <code>Set (\u211d \u2192 \u211d)</code>, even after unfolding definitions, so the statement <code>sin \u2208 cos</code> makes no sense. One can also use Lean to work on set theory itself. For instance the independence of the continuum hypothesis from the axioms of Zermelo-Fraenkel has been formalized in Lean. But such a meta-theory of set theory is completely beyond the scope of this book.</p> <p>If <code>s</code> and <code>t</code> are of type <code>Set \u03b1</code>, then the subset relation <code>s \u2286 t</code> is defined to mean <code>\u2200 {x : \u03b1}, x \u2208 s \u2192 x \u2208 t</code>. The variable in the quantifier is marked implicit so that given <code>h : s \u2286 t</code> and <code>h' : x \u2208 s</code>, we can write <code>h h'</code> as justification for <code>x \u2208 t</code>. The following example provides a tactic proof and a proof term justifying the reflexivity of the subset relation, and asks you to do the same for transitivity.</p> <pre><code>variable {\u03b1 : Type*} (r s t : Set \u03b1)\n\nexample : s \u2286 s := by\n  intro x xs\n  exact xs\n\ntheorem Subset.refl : s \u2286 s := fun x xs \u21a6 xs\n\ntheorem Subset.trans : r \u2286 s \u2192 s \u2286 t \u2192 r \u2286 t := by\n  sorry\n</code></pre> <p>Just as we defined <code>FnUb</code> for functions, we can define <code>SetUb s a</code> to mean that <code>a</code> is an upper bound on the set <code>s</code>, assuming <code>s</code> is a set of elements of some type that has an order associated with it. In the next example, we ask you to prove that if <code>a</code> is a bound on <code>s</code> and <code>a \u2264 b</code>, then <code>b</code> is a bound on <code>s</code> as well.</p> <pre><code>variable {\u03b1 : Type*} [PartialOrder \u03b1]\nvariable (s : Set \u03b1) (a b : \u03b1)\n\ndef SetUb (s : Set \u03b1) (a : \u03b1) :=\n  \u2200 x, x \u2208 s \u2192 x \u2264 a\n\nexample (h : SetUb s a) (h' : a \u2264 b) : SetUb s b :=\n  sorry\n</code></pre> <p>We close this section with one last important example. A function :math:<code>f</code> is said to be injective if for every :math:<code>x_1</code> and :math:<code>x_2</code>, if :math:<code>f(x_1) = f(x_2)</code> then :math:<code>x_1 = x_2</code>. Mathlib defines <code>Function.Injective f</code> with <code>x\u2081</code> and <code>x\u2082</code> implicit. The next example shows that, on the real numbers, any function that adds a constant is injective. We then ask you to show that multiplication by a nonzero constant is also injective, using the lemma name in the example as a source of inspiration. Recall you should use Ctrl-space completion after guessing the beginning of a lemma name.</p> <pre><code>open Function\n\nexample (c : \u211d) : Injective fun x \u21a6 x + c := by\n  intro x\u2081 x\u2082 h'\n  exact (add_left_inj c).mp h'\n\nexample {c : \u211d} (h : c \u2260 0) : Injective fun x \u21a6 c * x := by\n  sorry\n</code></pre> <p>Finally, show that the composition of two injective functions is injective:</p> <pre><code>variable {\u03b1 : Type*} {\u03b2 : Type*} {\u03b3 : Type*}\nvariable {g : \u03b2 \u2192 \u03b3} {f : \u03b1 \u2192 \u03b2}\n\nexample (injg : Injective g) (injf : Injective f) : Injective fun x \u21a6 g (f x) := by\n  sorry\n</code></pre>"},{"location":"MIL/C03_Logic/S02_The_Existential_Quantifier/","title":"S02 The Existential Quantifier","text":"<p>import MIL.Common</p>"},{"location":"MIL/C03_Logic/S02_The_Existential_Quantifier/#the-existential-quantifier","title":"The Existential Quantifier","text":"<p>The existential quantifier, which can be entered as <code>\\ex</code> in VS Code, is used to represent the phrase \"there exists.\" The formal expression <code>\u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3</code> in Lean says that there is a real number between 2 and 3. (We will discuss the conjunction symbol, <code>\u2227</code>, in :numref:<code>conjunction_and_biimplication</code>.) The canonical way to prove such a statement is to exhibit a real number and show that it has the stated property. The number 2.5, which we can enter as <code>5 / 2</code> or <code>(5 : \u211d) / 2</code> when Lean cannot infer from context that we have the real numbers in mind, has the required property, and the <code>norm_num</code> tactic can prove that it meets the description.</p> <p>There are a few ways we can put the information together. Given a goal that begins with an existential quantifier, the <code>use</code> tactic is used to provide the object, leaving the goal of proving the property.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 := by\n  use 5 / 2\n  norm_num\n</code></pre> <p>You can give the <code>use</code> tactic proofs as well as data:</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 := by\n  have h1 : 2 &lt; (5 : \u211d) / 2 := by norm_num\n  have h2 : (5 : \u211d) / 2 &lt; 3 := by norm_num\n  use 5 / 2, h1, h2\n</code></pre> <p>In fact, the <code>use</code> tactic automatically tries to use available assumptions as well.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 := by\n  have h : 2 &lt; (5 : \u211d) / 2 \u2227 (5 : \u211d) / 2 &lt; 3 := by norm_num\n  use 5 / 2\n</code></pre> <p>Alternatively, we can use Lean's anonymous constructor notation to construct a proof of an existential quantifier.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 :=\n  have h : 2 &lt; (5 : \u211d) / 2 \u2227 (5 : \u211d) / 2 &lt; 3 := by norm_num\n  \u27e85 / 2, h\u27e9\n</code></pre> <p>Notice that there is no <code>by</code>; here we are giving an explicit proof term. The left and right angle brackets, which can be entered as <code>\\&lt;</code> and <code>\\&gt;</code> respectively, tell Lean to put together the given data using whatever construction is appropriate for the current goal. We can use the notation without going first into tactic mode:</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 :=\n  \u27e85 / 2, by norm_num\u27e9\n</code></pre> <p>So now we know how to prove an exists statement. But how do we use one? If we know that there exists an object with a certain property, we should be able to give a name to an arbitrary one and reason about it. For example, remember the predicates <code>FnUb f a</code> and <code>FnLb f a</code> from the last section, which say that <code>a</code> is an upper bound or lower bound on <code>f</code>, respectively. We can use the existential quantifier to say that \"<code>f</code> is bounded\" without specifying the bound:</p> <pre><code>def FnUb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, f x \u2264 a\n\ndef FnLb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, a \u2264 f x\n\ndef FnHasUb (f : \u211d \u2192 \u211d) :=\n  \u2203 a, FnUb f a\n\ndef FnHasLb (f : \u211d \u2192 \u211d) :=\n  \u2203 a, FnLb f a\n</code></pre> <p>We can use the theorem <code>FnUb_add</code> from the last section to prove that if <code>f</code> and <code>g</code> have upper bounds, then so does <code>fun x \u21a6 f x + g x</code>.</p> <pre><code>variable {f g : \u211d \u2192 \u211d}\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  rcases ubf with \u27e8a, ubfa\u27e9\n  rcases ubg with \u27e8b, ubgb\u27e9\n  use a + b\n  apply fnUb_add ubfa ubgb\n</code></pre> <p>The <code>rcases</code> tactic unpacks the information in the existential quantifier. The annotations like <code>\u27e8a, ubfa\u27e9</code>, written with the same angle brackets as the anonymous constructors, are known as patterns, and they describe the information that we expect to find when we unpack the main argument. Given the hypothesis <code>ubf</code> that there is an upper bound for <code>f</code>, <code>rcases ubf with \u27e8a, ubfa\u27e9</code> adds a new variable <code>a</code> for an upper bound to the context, together with the hypothesis <code>ubfa</code> that it has the given property. The goal is left unchanged; what has changed is that we can now use the new object and the new hypothesis to prove the goal. This is a common method of reasoning in mathematics: we unpack objects whose existence is asserted or implied by some hypothesis, and then use it to establish the existence of something else.</p> <p>Try using this method to establish the following. You might find it useful to turn some of the examples from the last section into named theorems, as we did with <code>fn_ub_add</code>, or you can insert the arguments directly into the proofs.</p> <pre><code>example (lbf : FnHasLb f) (lbg : FnHasLb g) : FnHasLb fun x \u21a6 f x + g x := by\n  sorry\n\nexample {c : \u211d} (ubf : FnHasUb f) (h : c \u2265 0) : FnHasUb fun x \u21a6 c * f x := by\n  sorry\n</code></pre> <p>The \"r\" in <code>rcases</code> stands for \"recursive,\" because it allows us to use arbitrarily complex patterns to unpack nested data. The <code>rintro</code> tactic is a combination of <code>intro</code> and <code>rcases</code>:</p> <pre><code>example : FnHasUb f \u2192 FnHasUb g \u2192 FnHasUb fun x \u21a6 f x + g x := by\n  rintro \u27e8a, ubfa\u27e9 \u27e8b, ubgb\u27e9\n  exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>In fact, Lean also supports a pattern-matching fun in expressions and proof terms:</p> <pre><code>example : FnHasUb f \u2192 FnHasUb g \u2192 FnHasUb fun x \u21a6 f x + g x :=\n  fun \u27e8a, ubfa\u27e9 \u27e8b, ubgb\u27e9 \u21a6 \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>The task of unpacking information in a hypothesis is so important that Lean and Mathlib provide a number of ways to do it. For example, the <code>obtain</code> tactic provides suggestive syntax:</p> <pre><code>example (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  obtain \u27e8a, ubfa\u27e9 := ubf\n  obtain \u27e8b, ubgb\u27e9 := ubg\n  exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>Think of the first <code>obtain</code> instruction as matching the \"contents\" of <code>ubf</code> with the given pattern and assigning the components to the named variables. <code>rcases</code> and <code>obtain</code> are said to <code>destruct</code> their arguments, though there is a small difference in that <code>rcases</code> clears <code>ubf</code> from the context when it is done, whereas it is still present after <code>obtain</code>.</p> <p>Lean also supports syntax that is similar to that used in other functional programming languages:</p> <pre><code>example (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  cases ubf\n  case intro a ubfa =&gt;\n    cases ubg\n    case intro b ubgb =&gt;\n      exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  cases ubf\n  next a ubfa =&gt;\n    cases ubg\n    next b ubgb =&gt;\n      exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  match ubf, ubg with\n    | \u27e8a, ubfa\u27e9, \u27e8b, ubgb\u27e9 =&gt;\n      exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x :=\n  match ubf, ubg with\n    | \u27e8a, ubfa\u27e9, \u27e8b, ubgb\u27e9 =&gt;\n      \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>In the first example, if you put your cursor after <code>cases ubf</code>, you will see that the tactic produces a single goal, which Lean has tagged <code>intro</code>. (The particular name chosen comes from the internal name for the axiomatic primitive that builds a proof of an existential statement.) The <code>case</code> tactic then names the components. The second example is similar, except using <code>next</code> instead of <code>case</code> means that you can avoid mentioning <code>intro</code>. The word <code>match</code> in the last two examples highlights that what we are doing here is what computer scientists call \"pattern matching.\" Notice that the third proof begins by <code>by</code>, after which the tactic version of <code>match</code> expects a tactic proof on the right side of the arrow. The last example is a proof term: there are no tactics in sight.</p> <p>For the rest of this book, we will stick to <code>rcases</code>, <code>rintro</code>, and <code>obtain</code>, as the preferred ways of using an existential quantifier. But it can't hurt to see the alternative syntax, especially if there is a chance you will find yourself in the company of computer scientists.</p> <p>To illustrate one way that <code>rcases</code> can be used, we prove an old mathematical chestnut: if two integers <code>x</code> and <code>y</code> can each be written as a sum of two squares, then so can their product, <code>x * y</code>. In fact, the statement is true for any commutative ring, not just the integers. In the next example, <code>rcases</code> unpacks two existential quantifiers at once. We then provide the magic values needed to express <code>x * y</code> as a sum of squares as a list to the <code>use</code> statement, and we use <code>ring</code> to verify that they work.</p> <pre><code>variable {\u03b1 : Type*} [CommRing \u03b1]\n\ndef SumOfSquares (x : \u03b1) :=\n  \u2203 a b, x = a ^ 2 + b ^ 2\n\ntheorem sumOfSquares_mul {x y : \u03b1} (sosx : SumOfSquares x) (sosy : SumOfSquares y) :\n    SumOfSquares (x * y) := by\n  rcases sosx with \u27e8a, b, xeq\u27e9\n  rcases sosy with \u27e8c, d, yeq\u27e9\n  rw [xeq, yeq]\n  use a * c - b * d, a * d + b * c\n  ring\n</code></pre> <p>This proof doesn't provide much insight, but here is one way to motivate it. A Gaussian integer is a number of the form \\(a + bi\\) where \\(a\\) and \\(b\\) are integers and \\(i = \\sqrt{-1}\\). The norm of the Gaussian integer \\(a + bi\\) is, by definition, \\(a^2 + b^2\\). So the norm of a Gaussian integer is a sum of squares, and any sum of squares can be expressed in this way. The theorem above reflects the fact that norm of a product of Gaussian integers is the product of their norms: if \\(x\\) is the norm of \\(a + bi\\) and \\(y\\) in the norm of \\(c + di\\), then \\(xy\\) is the norm of \\((a + bi) (c + di)\\). Our cryptic proof illustrates the fact that the proof that is easiest to formalize isn't always the most perspicuous one. In :numref:<code>section_building_the_gaussian_integers</code>, we will provide you with the means to define the Gaussian integers and use them to provide an alternative proof.</p> <p>The pattern of unpacking an equation inside an existential quantifier and then using it to rewrite an expression in the goal comes up often, so much so that the <code>rcases</code> tactic provides an abbreviation: if you use the keyword <code>rfl</code> in place of a new identifier, <code>rcases</code> does the rewriting automatically (this trick doesn't work with pattern-matching lambdas).</p> <pre><code>theorem sumOfSquares_mul' {x y : \u03b1} (sosx : SumOfSquares x) (sosy : SumOfSquares y) :\n    SumOfSquares (x * y) := by\n  rcases sosx with \u27e8a, b, rfl\u27e9\n  rcases sosy with \u27e8c, d, rfl\u27e9\n  use a * c - b * d, a * d + b * c\n  ring\n</code></pre> <p>As with the universal quantifier, you can find existential quantifiers hidden all over if you know how to spot them. For example, divisibility is implicitly an \"exists\" statement.</p> <pre><code>example (divab : a \u2223 b) (divbc : b \u2223 c) : a \u2223 c := by\n  rcases divab with \u27e8d, beq\u27e9\n  rcases divbc with \u27e8e, ceq\u27e9\n  rw [ceq, beq]\n  use d * e; ring\n</code></pre> <p>And once again, this provides a nice setting for using <code>rcases</code> with <code>rfl</code>. Try it out in the proof above. It feels pretty good!</p> <p>Then try proving the following:</p> <pre><code>example (divab : a \u2223 b) (divac : a \u2223 c) : a \u2223 b + c := by\n  sorry\n</code></pre> <p>For another important example, a function \\(f : \\alpha \\to \\beta\\) is said to be surjective if for every \\(y\\) in the codomain, \\(\\beta\\), there is an \\(x\\) in the domain, \\(\\alpha\\), such that $f(x) = $<code>. Notice that this statement includes both a universal and an existential quantifier, which explains why the next example makes use of both</code>intro<code>and</code>use`.</p> <pre><code>example {c : \u211d} : Surjective fun x \u21a6 x + c := by\n  intro x\n  use x - c\n  dsimp; ring\n</code></pre> <p>Try this example yourself using the theorem <code>mul_div_cancel\u2080</code>.:</p> <pre><code>example {c : \u211d} (h : c \u2260 0) : Surjective fun x \u21a6 c * x := by\n  sorry\n</code></pre> <p>At this point, it is worth mentioning that there is a tactic, <code>field_simp</code>, that will often clear denominators in a useful way. It can be used in conjunction with the <code>ring</code> tactic.</p> <pre><code>example (x y : \u211d) (h : x - y \u2260 0) : (x ^ 2 - y ^ 2) / (x - y) = x + y := by\n  field_simp [h]\n  ring\n</code></pre> <p>The next example uses a surjectivity hypothesis by applying it to a suitable value. Note that you can use <code>rcases</code> with any expression, not just a hypothesis.</p> <pre><code>example {f : \u211d \u2192 \u211d} (h : Surjective f) : \u2203 x, f x ^ 2 = 4 := by\n  rcases h 2 with \u27e8x, hx\u27e9\n  use x\n  rw [hx]\n  norm_num\n</code></pre> <p>See if you can use these methods to show that the composition of surjective functions is surjective.</p> <pre><code>variable {\u03b1 : Type*} {\u03b2 : Type*} {\u03b3 : Type*}\nvariable {g : \u03b2 \u2192 \u03b3} {f : \u03b1 \u2192 \u03b2}\n\nexample (surjg : Surjective g) (surjf : Surjective f) : Surjective fun x \u21a6 g (f x) := by\n  sorry\n</code></pre>"},{"location":"MIL/C03_Logic/S03_Negation/","title":"S03 Negation","text":""},{"location":"MIL/C03_Logic/S03_Negation/#negation","title":"Negation","text":"<p>The symbol <code>\u00ac</code> is meant to express negation, so <code>\u00ac x &lt; y</code> says that <code>x</code> is not less than <code>y</code>, <code>\u00ac x = y</code> (or, equivalently, <code>x \u2260 y</code>) says that <code>x</code> is not equal to <code>y</code>, and <code>\u00ac \u2203 z, x &lt; z \u2227 z &lt; y</code> says that there does not exist a <code>z</code> strictly between <code>x</code> and <code>y</code>. In Lean, the notation <code>\u00ac A</code> abbreviates <code>A \u2192 False</code>, which you can think of as saying that <code>A</code> implies a contradiction. Practically speaking, this means that you already know something about how to work with negations: you can prove <code>\u00ac A</code> by introducing a hypothesis <code>h : A</code> and proving <code>False</code>, and if you have <code>h : \u00ac A</code> and <code>h' : A</code>, then applying <code>h</code> to <code>h'</code> yields <code>False</code>.</p> <p>To illustrate, consider the irreflexivity principle <code>lt_irrefl</code> for a strict order, which says that we have <code>\u00ac a &lt; a</code> for every <code>a</code>. The asymmetry principle <code>lt_asymm</code> says that we have <code>a &lt; b \u2192 \u00ac b &lt; a</code>. Let's show that <code>lt_asymm</code> follows from <code>lt_irrefl</code>. section</p> <pre><code>example (h : a &lt; b) : \u00acb &lt; a := by\n  intro h'\n  have : a &lt; a := lt_trans h h'\n  apply lt_irrefl a this\n</code></pre> <p>This example introduces a couple of new tricks. First, when you use <code>have</code> without providing a label, Lean uses the name <code>this</code>, providing a convenient way to refer back to it. Because the proof is so short, we provide an explicit proof term. But what you should really be paying attention to in this proof is the result of the <code>intro</code> tactic, which leaves a goal of <code>False</code>, and the fact that we eventually prove <code>False</code> by applying <code>lt_irrefl</code> to a proof of <code>a &lt; a</code>.</p> <p>Here is another example, which uses the predicate <code>FnHasUb</code> defined in the last section, which says that a function has an upper bound.</p> <pre><code>example (h : \u2200 a, \u2203 x, f x &gt; a) : \u00acFnHasUb f := by\n  intro fnub\n  rcases fnub with \u27e8a, fnuba\u27e9\n  rcases h a with \u27e8x, hx\u27e9\n  have : f x \u2264 a := fnuba x\n  linarith\n</code></pre> <p>Remember that it is often convenient to use <code>linarith</code> when a goal follows from linear equations and inequalities that are in the context.</p> <p>See if you can prove these in a similar way:</p> <pre><code>example (h : \u2200 a, \u2203 x, f x &lt; a) : \u00acFnHasLb f :=\n  sorry\n\nexample : \u00acFnHasUb fun x \u21a6 x :=\n  sorry\n</code></pre> <p>Mathlib offers a number of useful theorems for relating orders and negations:</p> <pre><code>#check (not_le_of_gt : a &gt; b \u2192 \u00aca \u2264 b)\n#check (not_lt_of_ge : a \u2265 b \u2192 \u00aca &lt; b)\n#check (lt_of_not_ge : \u00aca \u2265 b \u2192 a &lt; b)\n#check (le_of_not_gt : \u00aca &gt; b \u2192 a \u2264 b)\n</code></pre> <p>Recall the predicate <code>Monotone f</code>, which says that <code>f</code> is nondecreasing. Use some of the theorems just enumerated to prove the following:</p> <pre><code>example (h : Monotone f) (h' : f a &lt; f b) : a &lt; b := by\n  sorry\n\nexample (h : a \u2264 b) (h' : f b &lt; f a) : \u00acMonotone f := by\n  sorry\n</code></pre> <p>We can show that the first example in the last snippet cannot be proved if we replace <code>&lt;</code> by <code>\u2264</code>. Notice that we can prove the negation of a universally quantified statement by giving a counterexample. Complete the proof.</p> <pre><code>example : \u00ac\u2200 {f : \u211d \u2192 \u211d}, Monotone f \u2192 \u2200 {a b}, f a \u2264 f b \u2192 a \u2264 b := by\n  intro h\n  let f := fun x : \u211d \u21a6 (0 : \u211d)\n  have monof : Monotone f := by sorry\n  have h' : f 1 \u2264 f 0 := le_refl _\n  sorry\n</code></pre> <p>This example introduces the <code>let</code> tactic, which adds a local definition to the context. If you put the cursor after the <code>let</code> command, in the goal window you will see that the definition <code>f : \u211d \u2192 \u211d := fun x \u21a6 0</code> has been added to the context. Lean will unfold the definition of <code>f</code> when it has to. In particular, when we prove <code>f 1 \u2264 f 0</code> with <code>le_refl</code>, Lean reduces <code>f 1</code> and <code>f 0</code> to <code>0</code>.</p> <p>Use <code>le_of_not_gt</code> to prove the following:</p> <pre><code>example (x : \u211d) (h : \u2200 \u03b5 &gt; 0, x &lt; \u03b5) : x \u2264 0 := by\n  sorry\n</code></pre> <p>Implicit in many of the proofs we have just done is the fact that if <code>P</code> is any property, saying that there is nothing with property <code>P</code> is the same as saying that everything fails to have property <code>P</code>, and saying that not everything has property <code>P</code> is equivalent to saying that something fails to have property <code>P</code>. In other words, all four of the following implications are valid (but one of them cannot be proved with what we explained so far):</p> <pre><code>variable {\u03b1 : Type*} (P : \u03b1 \u2192 Prop) (Q : Prop)\n\nexample (h : \u00ac\u2203 x, P x) : \u2200 x, \u00acP x := by\n  sorry\n\nexample (h : \u2200 x, \u00acP x) : \u00ac\u2203 x, P x := by\n  sorry\n\nexample (h : \u00ac\u2200 x, P x) : \u2203 x, \u00acP x := by\n  sorry\n\nexample (h : \u2203 x, \u00acP x) : \u00ac\u2200 x, P x := by\n  sorry\n</code></pre> <p>The first, second, and fourth are straightforward to prove using the methods you have already seen. We encourage you to try it. The third is more difficult, however, because it concludes that an object exists from the fact that its nonexistence is contradictory. This is an instance of classical mathematical reasoning. We can use proof by contradiction to prove the third implication as follows.</p> <pre><code>example (h : \u00ac\u2200 x, P x) : \u2203 x, \u00acP x := by\n  by_contra h'\n  apply h\n  intro x\n  show P x\n  by_contra h''\n  exact h' \u27e8x, h''\u27e9\n</code></pre> <p>Make sure you understand how this works. The <code>by_contra</code> tactic allows us to prove a goal <code>Q</code> by assuming <code>\u00ac Q</code> and deriving a contradiction. In fact, it is equivalent to using the equivalence <code>not_not : \u00ac \u00ac Q \u2194 Q</code>. Confirm that you can prove the forward direction of this equivalence using <code>by_contra</code>, while the reverse direction follows from the ordinary rules for negation.</p> <pre><code>example (h : \u00ac\u00acQ) : Q := by\n  sorry\n\nexample (h : Q) : \u00ac\u00acQ := by\n  sorry\n</code></pre> <p>Use proof by contradiction to establish the following, which is the converse of one of the implications we proved above. (Hint: use <code>intro</code> first.)</p> <pre><code>example (h : \u00acFnHasUb f) : \u2200 a, \u2203 x, f x &gt; a := by\n  sorry\n</code></pre> <p>It is often tedious to work with compound statements with a negation in front, and it is a common mathematical pattern to replace such statements with equivalent forms in which the negation has been pushed inward. To facilitate this, Mathlib offers a <code>push_neg</code> tactic, which restates the goal in this way. The command <code>push_neg at h</code> restates the hypothesis <code>h</code>.</p> <pre><code>example (h : \u00ac\u2200 a, \u2203 x, f x &gt; a) : FnHasUb f := by\n  push_neg at h\n  exact h\n\nexample (h : \u00acFnHasUb f) : \u2200 a, \u2203 x, f x &gt; a := by\n  dsimp only [FnHasUb, FnUb] at h\n  push_neg at h\n  exact h\n</code></pre> <p>In the second example, we use dsimp to expand the definitions of <code>FnHasUb</code> and <code>FnUb</code>. (We need to use <code>dsimp</code> rather than <code>rw</code> to expand <code>FnUb</code>, because it appears in the scope of a quantifier.) You can verify that in the examples above with <code>\u00ac\u2203 x, P x</code> and <code>\u00ac\u2200 x, P x</code>, the <code>push_neg</code> tactic does the expected thing. Without even knowing how to use the conjunction symbol, you should be able to use <code>push_neg</code> to prove the following:</p> <pre><code>example (h : \u00acMonotone f) : \u2203 x y, x \u2264 y \u2227 f y &lt; f x := by\n  sorry\n</code></pre> <p>Mathlib also has a tactic, <code>contrapose</code>, which transforms a goal <code>A \u2192 B</code> to <code>\u00acB \u2192 \u00acA</code>. Similarly, given a goal of proving <code>B</code> from hypothesis <code>h : A</code>, <code>contrapose h</code> leaves you with a goal of proving <code>\u00acA</code> from hypothesis <code>\u00acB</code>. Using <code>contrapose!</code> instead of <code>contrapose</code> applies <code>push_neg</code> to the goal and the relevant hypothesis as well.</p> <pre><code>example (h : \u00acFnHasUb f) : \u2200 a, \u2203 x, f x &gt; a := by\n  contrapose! h\n  exact h\n\nexample (x : \u211d) (h : \u2200 \u03b5 &gt; 0, x \u2264 \u03b5) : x \u2264 0 := by\n  contrapose! h\n  use x / 2\n  constructor &lt;;&gt; linarith\n</code></pre> <p>We have not yet explained the <code>constructor</code> command or the use of the semicolon after it, but we will do that in the next section.</p> <p>We close this section with the principle of ex falso, which says that anything follows from a contradiction. In Lean, this is represented by <code>False.elim</code>, which establishes <code>False \u2192 P</code> for any proposition <code>P</code>. This may seem like a strange principle, but it comes up fairly often. We often prove a theorem by splitting on cases, and sometimes we can show that one of the cases is contradictory. In that case, we need to assert that the contradiction establishes the goal so we can move on to the next one. (We will see instances of reasoning by cases in :numref:<code>disjunction</code>.)</p> <p>Lean provides a number of ways of closing a goal once a contradiction has been reached.</p> <pre><code>example (h : 0 &lt; 0) : a &gt; 37 := by\n  exfalso\n  apply lt_irrefl 0 h\n\nexample (h : 0 &lt; 0) : a &gt; 37 :=\n  absurd h (lt_irrefl 0)\n\nexample (h : 0 &lt; 0) : a &gt; 37 := by\n  have h' : \u00ac0 &lt; 0 := lt_irrefl 0\n  contradiction\n</code></pre> <p>The <code>exfalso</code> tactic replaces the current goal with the goal of proving <code>False</code>. Given <code>h : P</code> and <code>h' : \u00ac P</code>, the term <code>absurd h h'</code> establishes any proposition. Finally, the <code>contradiction</code> tactic tries to close a goal by finding a contradiction in the hypotheses, such as a pair of the form <code>h : P</code> and <code>h' : \u00ac P</code>. Of course, in this example, <code>linarith</code> also works.</p>"},{"location":"MIL/C03_Logic/S04_Conjunction_and_Iff/","title":"S04 Conjunction and Iff","text":""},{"location":"MIL/C03_Logic/S04_Conjunction_and_Iff/#conjunction-and-iff","title":"Conjunction and Iff","text":"<p>You have already seen that the conjunction symbol, <code>\u2227</code>, is used to express \"and.\" The <code>constructor</code> tactic allows you to prove a statement of the form <code>A \u2227 B</code> by proving <code>A</code> and then proving <code>B</code>.</p> <pre><code>example {x y : \u211d} (h\u2080 : x \u2264 y) (h\u2081 : \u00acy \u2264 x) : x \u2264 y \u2227 x \u2260 y := by\n  constructor\n  \u00b7 assumption\n  intro h\n  apply h\u2081\n  rw [h]\n</code></pre> <p>In this example, the <code>assumption</code> tactic tells Lean to find an assumption that will solve the goal. Notice that the final <code>rw</code> finishes the goal by applying the reflexivity of <code>\u2264</code>. The following are alternative ways of carrying out the previous examples using the anonymous constructor angle brackets. The first is a slick proof-term version of the previous proof, which drops into tactic mode at the keyword <code>by</code>.</p> <pre><code>example {x y : \u211d} (h\u2080 : x \u2264 y) (h\u2081 : \u00acy \u2264 x) : x \u2264 y \u2227 x \u2260 y :=\n  \u27e8h\u2080, fun h \u21a6 h\u2081 (by rw [h])\u27e9\n\nexample {x y : \u211d} (h\u2080 : x \u2264 y) (h\u2081 : \u00acy \u2264 x) : x \u2264 y \u2227 x \u2260 y :=\n  have h : x \u2260 y := by\n    contrapose! h\u2081\n    rw [h\u2081]\n  \u27e8h\u2080, h\u27e9\n</code></pre> <p>Using a conjunction instead of proving one involves unpacking the proofs of the two parts. You can use the <code>rcases</code> tactic for that, as well as <code>rintro</code> or a pattern-matching <code>fun</code>, all in a manner similar to the way they are used with the existential quantifier.</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  rcases h with \u27e8h\u2080, h\u2081\u27e9\n  contrapose! h\u2081\n  exact le_antisymm h\u2080 h\u2081\n\nexample {x y : \u211d} : x \u2264 y \u2227 x \u2260 y \u2192 \u00acy \u2264 x := by\n  rintro \u27e8h\u2080, h\u2081\u27e9 h'\n  exact h\u2081 (le_antisymm h\u2080 h')\n\nexample {x y : \u211d} : x \u2264 y \u2227 x \u2260 y \u2192 \u00acy \u2264 x :=\n  fun \u27e8h\u2080, h\u2081\u27e9 h' \u21a6 h\u2081 (le_antisymm h\u2080 h')\n</code></pre> <p>In analogy to the <code>obtain</code> tactic, there is also a pattern-matching <code>have</code>:</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  have \u27e8h\u2080, h\u2081\u27e9 := h\n  contrapose! h\u2081\n  exact le_antisymm h\u2080 h\u2081\n</code></pre> <p>In contrast to <code>rcases</code>, here the <code>have</code> tactic leaves <code>h</code> in the context. And even though we won't use them, once again we have the computer scientists' pattern-matching syntax:</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  cases h\n  case intro h\u2080 h\u2081 =&gt;\n    contrapose! h\u2081\n    exact le_antisymm h\u2080 h\u2081\n\nexample {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  cases h\n  next h\u2080 h\u2081 =&gt;\n    contrapose! h\u2081\n    exact le_antisymm h\u2080 h\u2081\n\nexample {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  match h with\n    | \u27e8h\u2080, h\u2081\u27e9 =&gt;\n        contrapose! h\u2081\n        exact le_antisymm h\u2080 h\u2081\n</code></pre> <p>In contrast to using an existential quantifier, you can also extract proofs of the two components of a hypothesis <code>h : A \u2227 B</code> by writing <code>h.left</code> and <code>h.right</code>, or, equivalently, <code>h.1</code> and <code>h.2</code>.</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  intro h'\n  apply h.right\n  exact le_antisymm h.left h'\n\nexample {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x :=\n  fun h' \u21a6 h.right (le_antisymm h.left h')\n</code></pre> <p>Try using these techniques to come up with various ways of proving of the following:</p> <pre><code>example {m n : \u2115} (h : m \u2223 n \u2227 m \u2260 n) : m \u2223 n \u2227 \u00acn \u2223 m :=\n  sorry\n</code></pre> <p>You can nest uses of <code>\u2203</code> and <code>\u2227</code> with anonymous constructors, <code>rintro</code>, and <code>rcases</code>.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 4 :=\n  \u27e85 / 2, by norm_num, by norm_num\u27e9\n\nexample (x y : \u211d) : (\u2203 z : \u211d, x &lt; z \u2227 z &lt; y) \u2192 x &lt; y := by\n  rintro \u27e8z, xltz, zlty\u27e9\n  exact lt_trans xltz zlty\n\nexample (x y : \u211d) : (\u2203 z : \u211d, x &lt; z \u2227 z &lt; y) \u2192 x &lt; y :=\n  fun \u27e8z, xltz, zlty\u27e9 \u21a6 lt_trans xltz zlty\n</code></pre> <p>You can also use the <code>use</code> tactic:</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 4 := by\n  use 5 / 2\n  constructor &lt;;&gt; norm_num\n\nexample : \u2203 m n : \u2115, 4 &lt; m \u2227 m &lt; n \u2227 n &lt; 10 \u2227 Nat.Prime m \u2227 Nat.Prime n := by\n  use 5\n  use 7\n  norm_num\n\nexample {x y : \u211d} : x \u2264 y \u2227 x \u2260 y \u2192 x \u2264 y \u2227 \u00acy \u2264 x := by\n  rintro \u27e8h\u2080, h\u2081\u27e9\n  use h\u2080\n  exact fun h' \u21a6 h\u2081 (le_antisymm h\u2080 h')\n</code></pre> <p>In the first example, the semicolon after the <code>constructor</code> command tells Lean to use the <code>norm_num</code> tactic on both of the goals that result.</p> <p>In Lean, <code>A \u2194 B</code> is not defined to be <code>(A \u2192 B) \u2227 (B \u2192 A)</code>, but it could have been, and it behaves roughly the same way. You have already seen that you can write <code>h.mp</code> and <code>h.mpr</code> or <code>h.1</code> and <code>h.2</code> for the two directions of <code>h : A \u2194 B</code>. You can also use <code>cases</code> and friends. To prove an if-and-only-if statement, you can use <code>constructor</code> or angle brackets, just as you would if you were proving a conjunction.</p> <pre><code>example {x y : \u211d} (h : x \u2264 y) : \u00acy \u2264 x \u2194 x \u2260 y := by\n  constructor\n  \u00b7 contrapose!\n    rintro rfl\n    rfl\n  contrapose!\n  exact le_antisymm h\n\nexample {x y : \u211d} (h : x \u2264 y) : \u00acy \u2264 x \u2194 x \u2260 y :=\n  \u27e8fun h\u2080 h\u2081 \u21a6 h\u2080 (by rw [h\u2081]), fun h\u2080 h\u2081 \u21a6 h\u2080 (le_antisymm h h\u2081)\u27e9\n</code></pre> <p>The last proof term is inscrutable. Remember that you can use underscores while writing an expression like that to see what Lean expects.</p> <p>Try out the various techniques and gadgets you have just seen in order to prove the following:</p> <pre><code>example {x y : \u211d} : x \u2264 y \u2227 \u00acy \u2264 x \u2194 x \u2264 y \u2227 x \u2260 y :=\n  sorry\n</code></pre> <p>For a more interesting exercise, show that for any two real numbers <code>x</code> and <code>y</code>, <code>x^2 + y^2 = 0</code> if and only if <code>x = 0</code> and <code>y = 0</code>. We suggest proving an auxiliary lemma using <code>linarith</code>, <code>pow_two_nonneg</code>, and <code>pow_eq_zero</code>.</p> <pre><code>theorem aux {x y : \u211d} (h : x ^ 2 + y ^ 2 = 0) : x = 0 :=\n  have h' : x ^ 2 = 0 := by sorry\n  pow_eq_zero h'\n\nexample (x y : \u211d) : x ^ 2 + y ^ 2 = 0 \u2194 x = 0 \u2227 y = 0 :=\n  sorry\n</code></pre> <p>In Lean, bi-implication leads a double-life. You can treat it like a conjunction and use its two parts separately. But Lean also knows that it is a reflexive, symmetric, and transitive relation between propositions, and you can also use it with <code>calc</code> and <code>rw</code>. It is often convenient to rewrite a statement to an equivalent one. In the next example, we use <code>abs_lt</code> to replace an expression of the form <code>|x| &lt; y</code> by the equivalent expression <code>- y &lt; x \u2227 x &lt; y</code>, and in the one after that we use <code>Nat.dvd_gcd_iff</code> to replace an expression of the form <code>m \u2223 Nat.gcd n k</code> by the equivalent expression <code>m \u2223 n \u2227 m \u2223 k</code>.</p> <pre><code>example (x : \u211d) : |x + 3| &lt; 5 \u2192 -8 &lt; x \u2227 x &lt; 2 := by\n  rw [abs_lt]\n  intro h\n  constructor &lt;;&gt; linarith\n\nexample : 3 \u2223 Nat.gcd 6 15 := by\n  rw [Nat.dvd_gcd_iff]\n  constructor &lt;;&gt; norm_num\n</code></pre> <p>See if you can use <code>rw</code> with the theorem below to provide a short proof that negation is not a nondecreasing function. (Note that <code>push_neg</code> won't unfold definitions for you, so the <code>rw [Monotone]</code> in the proof of the theorem is needed.)</p> <pre><code>theorem not_monotone_iff {f : \u211d \u2192 \u211d} : \u00acMonotone f \u2194 \u2203 x y, x \u2264 y \u2227 f x &gt; f y := by\n  rw [Monotone]\n  push_neg\n  rfl\n\nexample : \u00acMonotone fun x : \u211d \u21a6 -x := by\n  sorry\n</code></pre> <p>The remaining exercises in this section are designed to give you some more practice with conjunction and bi-implication. Remember that a partial order is a binary relation that is transitive, reflexive, and antisymmetric. An even weaker notion sometimes arises: a preorder is just a reflexive, transitive relation. For any pre-order <code>\u2264</code>, Lean axiomatizes the associated strict pre-order by <code>a &lt; b \u2194 a \u2264 b \u2227 \u00ac b \u2264 a</code>. Show that if <code>\u2264</code> is a partial order, then <code>a &lt; b</code> is equivalent to <code>a \u2264 b \u2227 a \u2260 b</code>:</p> <pre><code>variable {\u03b1 : Type*} [PartialOrder \u03b1]\nvariable (a b : \u03b1)\n\nexample : a &lt; b \u2194 a \u2264 b \u2227 a \u2260 b := by\n  rw [lt_iff_le_not_le]\n  sorry\n</code></pre> <p>Beyond logical operations, you do not need anything more than <code>le_refl</code> and <code>le_trans</code>. Show that even in the case where <code>\u2264</code> is only assumed to be a preorder, we can prove that the strict order is irreflexive and transitive. In the second example, for convenience, we use the simplifier rather than <code>rw</code> to express <code>&lt;</code> in terms of <code>\u2264</code> and <code>\u00ac</code>. We will come back to the simplifier later, but here we are only relying on the fact that it will use the indicated lemma repeatedly, even if it needs to be instantiated to different values.</p> <pre><code>variable {\u03b1 : Type*} [Preorder \u03b1]\nvariable (a b c : \u03b1)\n\n-- EXAMPLES:\nexample : \u00aca &lt; a := by\n  rw [lt_iff_le_not_le]\n  sorry\n\nexample : a &lt; b \u2192 b &lt; c \u2192 a &lt; c := by\n  simp only [lt_iff_le_not_le]\n  sorry\n</code></pre>"},{"location":"MIL/C03_Logic/S05_Disjunction/","title":"S05 Disjunction","text":""},{"location":"MIL/C03_Logic/S05_Disjunction/#disjunction","title":"Disjunction","text":"<p>The canonical way to prove a disjunction <code>A \u2228 B</code> is to prove <code>A</code> or to prove <code>B</code>. The <code>left</code> tactic chooses <code>A</code>, and the <code>right</code> tactic chooses <code>B</code>.</p> <pre><code>variable {x y : \u211d}\n\nexample (h : y &gt; x ^ 2) : y &gt; 0 \u2228 y &lt; -1 := by\n  left\n  linarith [pow_two_nonneg x]\n\nexample (h : -y &gt; x ^ 2 + 1) : y &gt; 0 \u2228 y &lt; -1 := by\n  right\n  linarith [pow_two_nonneg x]\n</code></pre> <p>We cannot use an anonymous constructor to construct a proof of an \"or\" because Lean would have to guess which disjunct we are trying to prove. When we write proof terms we can use <code>Or.inl</code> and <code>Or.inr</code> instead to make the choice explicitly. Here, <code>inl</code> is short for \"introduction left\" and <code>inr</code> is short for \"introduction right.\"</p> <pre><code>example (h : y &gt; 0) : y &gt; 0 \u2228 y &lt; -1 :=\n  Or.inl h\n\nexample (h : y &lt; -1) : y &gt; 0 \u2228 y &lt; -1 :=\n  Or.inr h\n</code></pre> <p>It may seem strange to prove a disjunction by proving one side or the other. In practice, which case holds usually depends on a case distinction that is implicit or explicit in the assumptions and the data. The <code>rcases</code> tactic allows us to make use of a hypothesis of the form <code>A \u2228 B</code>. In contrast to the use of <code>rcases</code> with conjunction or an existential quantifier, here the <code>rcases</code> tactic produces two goals. Both have the same conclusion, but in the first case, <code>A</code> is assumed to be true, and in the second case, <code>B</code> is assumed to be true. In other words, as the name suggests, the <code>rcases</code> tactic carries out a proof by cases. As usual, we can tell Lean what names to use for the hypotheses. In the next example, we tell Lean to use the name <code>h</code> on each branch.</p> <pre><code>example : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  rcases le_or_gt 0 y with h | h\n  \u00b7 rw [abs_of_nonneg h]\n    intro h; left; exact h\n  . rw [abs_of_neg h]\n    intro h; right; exact h\n</code></pre> <p>Notice that the pattern changes from <code>\u27e8h\u2080, h\u2081\u27e9</code> in the case of a conjunction to <code>h\u2080 | h\u2081</code> in the case of a disjunction. Think of the first pattern as matching against data the contains both an <code>h\u2080</code> and a <code>h\u2081</code>, whereas second pattern, with the bar, matches against data that contains either an <code>h\u2080</code> or <code>h\u2081</code>. In this case, because the two goals are separate, we have chosen to use the same name, <code>h</code>, in each case.</p> <p>The absolute value function is defined in such a way that we can immediately prove that <code>x \u2265 0</code> implies <code>|x| = x</code> (this is the theorem <code>abs_of_nonneg</code>) and <code>x &lt; 0</code> implies <code>|x| = -x</code> (this is <code>abs_of_neg</code>). The expression <code>le_or_gt 0 x</code> establishes <code>0 \u2264 x \u2228 x &lt; 0</code>, allowing us to split on those two cases.</p> <p>Lean also supports the computer scientists' pattern-matching syntax for disjunction. Now the <code>cases</code> tactic is more attractive, because it allows us to name each <code>case</code>, and name the hypothesis that is introduced closer to where it is used.</p> <pre><code>example : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  cases le_or_gt 0 y\n  case inl h =&gt;\n    rw [abs_of_nonneg h]\n    intro h; left; exact h\n  case inr h =&gt;\n    rw [abs_of_neg h]\n    intro h; right; exact h\n</code></pre> <p>The names <code>inl</code> and <code>inr</code> are short for \"intro left\" and \"intro right,\" respectively. Using <code>case</code> has the advantage that you can prove the cases in either order; Lean uses the tag to find the relevant goal. If you don't care about that, you can use <code>next</code>, or <code>match</code>, or even a pattern-matching <code>have</code>.</p> <pre><code>example : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  cases le_or_gt 0 y\n  next h =&gt;\n    rw [abs_of_nonneg h]\n    intro h; left; exact h\n  next h =&gt;\n    rw [abs_of_neg h]\n    intro h; right; exact h\n\nexample : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  match le_or_gt 0 y with\n    | Or.inl h =&gt;\n      rw [abs_of_nonneg h]\n      intro h; left; exact h\n    | Or.inr h =&gt;\n      rw [abs_of_neg h]\n      intro h; right; exact h\n</code></pre> <p>In the case of <code>match</code>, we need to use the full names <code>Or.inl</code> and <code>Or.inr</code> of the canonical ways to prove a disjunction. In this textbook, we will generally use <code>rcases</code> to split on the cases of a disjunction.</p> <p>Try proving the triangle inequality using the first two theorems in the next snippet. They are given the same names they have in Mathlib.</p> <pre><code>namespace MyAbs\n\ntheorem le_abs_self (x : \u211d) : x \u2264 |x| := by\n  sorry\n\ntheorem neg_le_abs_self (x : \u211d) : -x \u2264 |x| := by\n  sorry\n\ntheorem abs_add (x y : \u211d) : |x + y| \u2264 |x| + |y| := by\n  sorry\n</code></pre> <p>In case you enjoyed these (pun intended) and you want more practice with disjunction, try these.</p> <pre><code>theorem lt_abs : x &lt; |y| \u2194 x &lt; y \u2228 x &lt; -y := by\n  sorry\n\ntheorem abs_lt : |x| &lt; y \u2194 -y &lt; x \u2227 x &lt; y := by\n  sorry\n</code></pre> <p>You can also use <code>rcases</code> and <code>rintro</code> with nested disjunctions. When these result in a genuine case split with multiple goals, the patterns for each new goal are separated by a vertical bar.</p> <pre><code>example {x : \u211d} (h : x \u2260 0) : x &lt; 0 \u2228 x &gt; 0 := by\n  rcases lt_trichotomy x 0 with xlt | xeq | xgt\n  \u00b7 left\n    exact xlt\n  \u00b7 contradiction\n  . right; exact xgt\n</code></pre> <p>You can still nest patterns and use the <code>rfl</code> keyword to substitute equations:</p> <pre><code>example {m n k : \u2115} (h : m \u2223 n \u2228 m \u2223 k) : m \u2223 n * k := by\n  rcases h with \u27e8a, rfl\u27e9 | \u27e8b, rfl\u27e9\n  \u00b7 rw [mul_assoc]\n    apply dvd_mul_right\n  . rw [mul_comm, mul_assoc]\n    apply dvd_mul_right\n</code></pre> <p>See if you can prove the following with a single (long) line. Use <code>rcases</code> to unpack the hypotheses and split on cases, and use a semicolon and <code>linarith</code> to solve each branch.</p> <pre><code>example {z : \u211d} (h : \u2203 x y, z = x ^ 2 + y ^ 2 \u2228 z = x ^ 2 + y ^ 2 + 1) : z \u2265 0 := by\n  sorry\n</code></pre> <p>On the real numbers, an equation <code>x * y = 0</code> tells us that <code>x = 0</code> or <code>y = 0</code>. In Mathlib, this fact is known as <code>eq_zero_or_eq_zero_of_mul_eq_zero</code>, and it is another nice example of how a disjunction can arise. See if you can use it to prove the following:</p> <pre><code>example {x : \u211d} (h : x ^ 2 = 1) : x = 1 \u2228 x = -1 := by\n  sorry\n\nexample {x y : \u211d} (h : x ^ 2 = y ^ 2) : x = y \u2228 x = -y := by\n  sorry\n</code></pre> <p>Remember that you can use the <code>ring</code> tactic to help with calculations.</p> <p>In an arbitrary ring :math:<code>R</code>, an element :math:<code>x</code> such that :math:<code>x y = 0</code> for some nonzero :math:<code>y</code> is called a left zero divisor, an element :math:<code>x</code> such that :math:<code>y x = 0</code> for some nonzero :math:<code>y</code> is called a right zero divisor, and an element that is either a left or right zero divisor is called simply a zero divisor. The theorem <code>eq_zero_or_eq_zero_of_mul_eq_zero</code> says that the real numbers have no nontrivial zero divisors. A commutative ring with this property is called an integral domain. Your proofs of the two theorems above should work equally well in any integral domain:</p> <pre><code>variable {R : Type*} [CommRing R] [IsDomain R]\nvariable (x y : R)\n\nexample (h : x ^ 2 = 1) : x = 1 \u2228 x = -1 := by\n  sorry\n\nexample (h : x ^ 2 = y ^ 2) : x = y \u2228 x = -y := by\n  sorry\n</code></pre> <p>In fact, if you are careful, you can prove the first theorem without using commutativity of multiplication. In that case, it suffices to assume that <code>R</code> is a <code>Ring</code> instead of an <code>CommRing</code>.</p> <p>.. index:: excluded middle</p> <p>Sometimes in a proof we want to split on cases depending on whether some statement is true or not. For any proposition <code>P</code>, we can use <code>em P : P \u2228 \u00ac P</code>. The name <code>em</code> is short for \"excluded middle.\"</p> <pre><code>example (P : Prop) : \u00ac\u00acP \u2192 P := by\n  intro h\n  cases em P\n  \u00b7 assumption\n  . contradiction\n</code></pre> <p>Alternatively, you can use the <code>by_cases</code> tactic.</p> <pre><code>-- EXAMPLES:\nexample (P : Prop) : \u00ac\u00acP \u2192 P := by\n  intro h\n  by_cases h' : P\n  \u00b7 assumption\n  contradiction\n</code></pre> <p>Notice that the <code>by_cases</code> tactic lets you specify a label for the hypothesis that is introduced in each branch, in this case, <code>h' : P</code> in one and <code>h' : \u00ac P</code> in the other. If you leave out the label, Lean uses <code>h</code> by default. Try proving the following equivalence, using <code>by_cases</code> to establish one direction.</p> <pre><code>example (P Q : Prop) : P \u2192 Q \u2194 \u00acP \u2228 Q := by\n  sorry\n</code></pre>"},{"location":"MIL/C03_Logic/S06_Sequences_and_Convergence/","title":"S06 Sequences and Convergence","text":""},{"location":"MIL/C03_Logic/S06_Sequences_and_Convergence/#sequences-and-convergence","title":"Sequences and Convergence","text":"<p>We now have enough skills at our disposal to do some real mathematics. In Lean, we can represent a sequence \\(s_0, s_1, s_2, \\ldots\\) of real numbers as a function <code>s : \u2115 \u2192 \u211d</code>. Such a sequence is said to converge to a number :math:<code>a</code> if for every \\(\\varepsilon &gt; 0\\) there is a point beyond which the sequence remains within \\(\\varepsilon\\) of \\(a\\), that is, there is a number \\(N\\) such that for every \\(n \\ge N\\), \\(| s_n - a | &lt; \\varepsilon\\). In Lean, we can render this as follows:</p> <pre><code>def ConvergesTo (s : \u2115 \u2192 \u211d) (a : \u211d) :=\n  \u2200 \u03b5 &gt; 0, \u2203 N, \u2200 n \u2265 N, |s n - a| &lt; \u03b5\n</code></pre> <p>The notation <code>\u2200 \u03b5 &gt; 0, ...</code> is a convenient abbreviation for <code>\u2200 \u03b5, \u03b5 &gt; 0 \u2192 ...</code>, and, similarly, <code>\u2200 n \u2265 N, ...</code> abbreviates <code>\u2200 n, n \u2265 N \u2192  ...</code>. And remember that <code>\u03b5 &gt; 0</code>, in turn, is defined as <code>0 &lt; \u03b5</code>, and <code>n \u2265 N</code> is defined as <code>N \u2264 n</code>.</p> <p>.. index:: extensionality, ext, tactics ; ext</p> <p>In this section, we'll establish some properties of convergence. But first, we will discuss three tactics for working with equality that will prove useful. The first, the <code>ext</code> tactic, gives us a way of proving that two functions are equal. Let \\(f(x) = x + 1\\) and \\(g(x) = 1 + x\\) be functions from reals to reals. Then, of course, \\(f = g\\), because they return the same value for every \\(x\\). The <code>ext</code> tactic enables us to prove an equation between functions by proving that their values are the same at all the values of their arguments.</p> <pre><code>example : (fun x y : \u211d \u21a6 (x + y) ^ 2) = fun x y : \u211d \u21a6 x ^ 2 + 2 * x * y + y ^ 2 := by\n  ext\n  ring\n</code></pre> <p>.. index:: congr, tactics ; congr</p> <p>We'll see later that <code>ext</code> is actually more general, and also one can specify the name of the variables that appear. For instance you can try to replace <code>ext</code> with <code>ext u v</code> in the above proof. The second tactic, the <code>congr</code> tactic, allows us to prove an equation between two expressions by reconciling the parts that are different:</p> <pre><code>example (a b : \u211d) : |a| = |a - b + b| := by\n  congr\n  ring\n</code></pre> <p>Here the <code>congr</code> tactic peels off the <code>abs</code> on each side, leaving us to prove <code>a = a - b + b</code>.</p> <p>.. index:: convert, tactics ; convert</p> <p>Finally, the <code>convert</code> tactic is used to apply a theorem to a goal when the conclusion of the theorem doesn't quite match. For example, suppose we want to prove <code>a &lt; a * a</code> from <code>1 &lt; a</code>. A theorem in the library, <code>mul_lt_mul_right</code>, will let us prove <code>1 * a &lt; a * a</code>. One possibility is to work backwards and rewrite the goal so that it has that form. Instead, the <code>convert</code> tactic lets us apply the theorem as it is, and leaves us with the task of proving the equations that are needed to make the goal match.</p> <pre><code>example {a : \u211d} (h : 1 &lt; a) : a &lt; a * a := by\n  convert (mul_lt_mul_right _).2 h\n  \u00b7 rw [one_mul]\n  exact lt_trans zero_lt_one h\n</code></pre> <p>This example illustrates another useful trick: when we apply an expression with an underscore and Lean can't fill it in for us automatically, it simply leaves it for us as another goal.</p> <p>The following shows that any constant sequence \\(a, a, a, \\ldots\\) converges.</p> <pre><code>theorem convergesTo_const (a : \u211d) : ConvergesTo (fun x : \u2115 \u21a6 a) a := by\n  intro \u03b5 \u03b5pos\n  use 0\n  intro n nge\n  rw [sub_self, abs_zero]\n  apply \u03b5pos\n</code></pre> <p>Lean has a tactic, <code>simp</code>, which can often save you the trouble of carrying out steps like <code>rw [sub_self, abs_zero]</code> by hand. We will tell you more about it soon.</p> <p>For a more interesting theorem, let's show that if <code>s</code> converges to <code>a</code> and <code>t</code> converges to <code>b</code>, then <code>fun n \u21a6 s n + t n</code> converges to <code>a + b</code>. It is helpful to have a clear pen-and-paper proof in mind before you start writing a formal one. Given <code>\u03b5</code> greater than <code>0</code>, the idea is to use the hypotheses to obtain an <code>Ns</code> such that beyond that point, <code>s</code> is within <code>\u03b5 / 2</code> of <code>a</code>, and an <code>Nt</code> such that beyond that point, <code>t</code> is within <code>\u03b5 / 2</code> of <code>b</code>. Then, whenever <code>n</code> is greater than or equal to the maximum of <code>Ns</code> and <code>Nt</code>, the sequence <code>fun n \u21a6 s n + t n</code> should be within <code>\u03b5</code> of <code>a + b</code>. The following example begins to implement this strategy. See if you can finish it off.</p> <pre><code>theorem convergesTo_add {s t : \u2115 \u2192 \u211d} {a b : \u211d}\n      (cs : ConvergesTo s a) (ct : ConvergesTo t b) :\n    ConvergesTo (fun n \u21a6 s n + t n) (a + b) := by\n  intro \u03b5 \u03b5pos\n  dsimp -- this line is not needed but cleans up the goal a bit.\n  have \u03b52pos : 0 &lt; \u03b5 / 2 := by linarith\n  rcases cs (\u03b5 / 2) \u03b52pos with \u27e8Ns, hs\u27e9\n  rcases ct (\u03b5 / 2) \u03b52pos with \u27e8Nt, ht\u27e9\n  use max Ns Nt\n  sorry\n</code></pre> <p>As hints, you can use <code>le_of_max_le_left</code> and <code>le_of_max_le_right</code>, and <code>norm_num</code> can prove <code>\u03b5 / 2 + \u03b5 / 2 = \u03b5</code>. Also, it is helpful to use the <code>congr</code> tactic to show that <code>|s n + t n - (a + b)|</code> is equal to <code>|(s n - a) + (t n - b)|,</code> since then you can use the triangle inequality. Notice that we marked all the variables <code>s</code>, <code>t</code>, <code>a</code>, and <code>b</code> implicit because they can be inferred from the hypotheses.</p> <p>Proving the same theorem with multiplication in place of addition is tricky. We will get there by proving some auxiliary statements first. See if you can also finish off the next proof, which shows that if <code>s</code> converges to <code>a</code>, then <code>fun n \u21a6 c * s n</code> converges to <code>c * a</code>. It is helpful to split into cases depending on whether <code>c</code> is equal to zero or not. We have taken care of the zero case, and we have left you to prove the result with the extra assumption that <code>c</code> is nonzero.</p> <pre><code>theorem convergesTo_mul_const {s : \u2115 \u2192 \u211d} {a : \u211d} (c : \u211d) (cs : ConvergesTo s a) :\n    ConvergesTo (fun n \u21a6 c * s n) (c * a) := by\n  by_cases h : c = 0\n  \u00b7 convert convergesTo_const 0\n    \u00b7 rw [h]\n      ring\n    rw [h]\n    ring\n  have acpos : 0 &lt; |c| := abs_pos.mpr h\n  sorry\n</code></pre> <p>The next theorem is also independently interesting: it shows that a convergent sequence is eventually bounded in absolute value. We have started you off; see if you can finish it.</p> <pre><code>theorem exists_abs_le_of_convergesTo {s : \u2115 \u2192 \u211d} {a : \u211d} (cs : ConvergesTo s a) :\n    \u2203 N b, \u2200 n, N \u2264 n \u2192 |s n| &lt; b := by\n  rcases cs 1 zero_lt_one with \u27e8N, h\u27e9\n  use N, |a| + 1\n  sorry\n</code></pre> <p>In fact, the theorem could be strengthened to assert that there is a bound <code>b</code> that holds for all values of <code>n</code>. But this version is strong enough for our purposes, and we will see at the end of this section that it holds more generally.</p> <p>The next lemma is auxiliary: we prove that if <code>s</code> converges to <code>a</code> and <code>t</code> converges to <code>0</code>, then <code>fun n \u21a6 s n * t n</code> converges to <code>0</code>. To do so, we use the previous theorem to find a <code>B</code> that bounds <code>s</code> beyond some point <code>N\u2080</code>. See if you can understand the strategy we have outlined and finish the proof.</p> <pre><code>theorem aux {s t : \u2115 \u2192 \u211d} {a : \u211d} (cs : ConvergesTo s a) (ct : ConvergesTo t 0) :\n    ConvergesTo (fun n \u21a6 s n * t n) 0 := by\n  intro \u03b5 \u03b5pos\n  dsimp\n  rcases exists_abs_le_of_convergesTo cs with \u27e8N\u2080, B, h\u2080\u27e9\n  have Bpos : 0 &lt; B := lt_of_le_of_lt (abs_nonneg _) (h\u2080 N\u2080 (le_refl _))\n  have pos\u2080 : \u03b5 / B &gt; 0 := div_pos \u03b5pos Bpos\n  rcases ct _ pos\u2080 with \u27e8N\u2081, h\u2081\u27e9\n  sorry\n</code></pre> <p>If you have made it this far, congratulations! We are now within striking distance of our theorem. The following proof finishes it off.</p> <pre><code>-- BOTH:\ntheorem convergesTo_mul {s t : \u2115 \u2192 \u211d} {a b : \u211d}\n      (cs : ConvergesTo s a) (ct : ConvergesTo t b) :\n    ConvergesTo (fun n \u21a6 s n * t n) (a * b) := by\n  have h\u2081 : ConvergesTo (fun n \u21a6 s n * (t n + -b)) 0 := by\n    apply aux cs\n    convert convergesTo_add ct (convergesTo_const (-b))\n    ring\n  have := convergesTo_add h\u2081 (convergesTo_mul_const b cs)\n  convert convergesTo_add h\u2081 (convergesTo_mul_const b cs) using 1\n  \u00b7 ext; ring\n  ring\n</code></pre> <p>For another challenging exercise, try filling out the following sketch of a proof that limits are unique. (If you are feeling bold, you can delete the proof sketch and try proving it from scratch.)</p> <pre><code>theorem convergesTo_unique {s : \u2115 \u2192 \u211d} {a b : \u211d}\n      (sa : ConvergesTo s a) (sb : ConvergesTo s b) :\n    a = b := by\n  by_contra abne\n  have : |a - b| &gt; 0 := by sorry\n  let \u03b5 := |a - b| / 2\n  have \u03b5pos : \u03b5 &gt; 0 := by\n    change |a - b| / 2 &gt; 0\n    linarith\n  rcases sa \u03b5 \u03b5pos with \u27e8Na, hNa\u27e9\n  rcases sb \u03b5 \u03b5pos with \u27e8Nb, hNb\u27e9\n  let N := max Na Nb\n  have absa : |s N - a| &lt; \u03b5 := by sorry\n  have absb : |s N - b| &lt; \u03b5 := by sorry\n  have : |a - b| &lt; |a - b| := by sorry\n  exact lt_irrefl _ this\n</code></pre> <p>We close the section with the observation that our proofs can be generalized. For example, the only properties that we have used of the natural numbers is that their structure carries a partial order with <code>min</code> and <code>max</code>. You can check that everything still works if you replace <code>\u2115</code> everywhere by any linear order <code>\u03b1</code>:</p> <pre><code>variable {\u03b1 : Type*} [LinearOrder \u03b1]\n\ndef ConvergesTo' (s : \u03b1 \u2192 \u211d) (a : \u211d) :=\n  \u2200 \u03b5 &gt; 0, \u2203 N, \u2200 n \u2265 N, |s n - a| &lt; \u03b5\n</code></pre> <p>In :numref:<code>filters</code>, we will see that Mathlib has mechanisms for dealing with convergence in vastly more general terms, not only abstracting away particular features of the domain and codomain, but also abstracting over different types of convergence.</p>"},{"location":"MIL/C04_Sets_and_Functions/S01_Sets/","title":"S01 Sets","text":""},{"location":"MIL/C04_Sets_and_Functions/S01_Sets/#sets","title":"Sets","text":"<p>.. index:: set operations</p> <p>If <code>\u03b1</code> is any type, the type <code>Set \u03b1</code> consists of sets of elements of <code>\u03b1</code>. This type supports the usual set-theoretic operations and relations. For example, <code>s \u2286 t</code> says that <code>s</code> is a subset of <code>t</code>, <code>s \u2229 t</code> denotes the intersection of <code>s</code> and <code>t</code>, and <code>s \u222a t</code> denotes their union. The subset relation can be typed with <code>\\ss</code> or <code>\\sub</code>, intersection can be typed with <code>\\i</code> or <code>\\cap</code>, and union can be typed with <code>\\un</code> or <code>\\cup</code>. The library also defines the set <code>univ</code>, which consists of all the elements of type <code>\u03b1</code>, and the empty set, <code>\u2205</code>, which can be typed as <code>\\empty</code>. Given <code>x : \u03b1</code> and <code>s : Set \u03b1</code>, the expression <code>x \u2208 s</code> says that <code>x</code> is a member of <code>s</code>. Theorems that mention set membership often include <code>mem</code> in their name. The expression <code>x \u2209 s</code> abbreviates <code>\u00ac x \u2208 s</code>. You can type <code>\u2208</code> as <code>\\in</code> or <code>\\mem</code> and <code>\u2209</code> as <code>\\notin</code>.</p> <p>.. index:: simp, tactics ; simp</p> <p>One way to prove things about sets is to use <code>rw</code> or the simplifier to expand the definitions. In the second example below, we use <code>simp only</code> to tell the simplifier to use only the list of identities we give it, and not its full database of identities. Unlike <code>rw</code>, <code>simp</code> can perform simplifications inside a universal or existential quantifier. If you step through the proof, you can see the effects of these commands.</p> <pre><code>variable {\u03b1 : Type*}\nvariable (s t u : Set \u03b1)\nopen Set\n\n-- EXAMPLES:\nexample (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u := by\n  rw [subset_def, inter_def, inter_def]\n  rw [subset_def] at h\n  simp only [mem_setOf]\n  rintro x \u27e8xs, xu\u27e9\n  exact \u27e8h _ xs, xu\u27e9\n\nexample (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u := by\n  simp only [subset_def, mem_inter_iff] at *\n  rintro x \u27e8xs, xu\u27e9\n  exact \u27e8h _ xs, xu\u27e9\n</code></pre> <p>In this example, we open the <code>set</code> namespace to have access to the shorter names for the theorems. But, in fact, we can delete the calls to <code>rw</code> and <code>simp</code> entirely:</p> <pre><code>example (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u := by\n  intro x xsu\n  exact \u27e8h xsu.1, xsu.2\u27e9\n</code></pre> <p>What is going on here is known as definitional reduction: to make sense of the <code>intro</code> command and the anonymous constructors Lean is forced to expand the definitions. The following example also illustrate the phenomenon:</p> <pre><code>example (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u :=\n  fun x \u27e8xs, xu\u27e9 \u21a6 \u27e8h xs, xu\u27e9\n</code></pre> <p>To deal with unions, we can use <code>Set.union_def</code> and <code>Set.mem_union</code>. Since <code>x \u2208 s \u222a t</code> unfolds to <code>x \u2208 s \u2228 x \u2208 t</code>, we can also use the <code>cases</code> tactic to force a definitional reduction.</p> <pre><code>example : s \u2229 (t \u222a u) \u2286 s \u2229 t \u222a s \u2229 u := by\n  intro x hx\n  have xs : x \u2208 s := hx.1\n  have xtu : x \u2208 t \u222a u := hx.2\n  rcases xtu with xt | xu\n  \u00b7 left\n    show x \u2208 s \u2229 t\n    exact \u27e8xs, xt\u27e9\n  . right\n    show x \u2208 s \u2229 u\n    exact \u27e8xs, xu\u27e9\n</code></pre> <p>Since intersection binds tighter than union, the use of parentheses in the expression <code>(s \u2229 t) \u222a (s \u2229 u)</code> is unnecessary, but they make the meaning of the expression clearer. The following is a shorter proof of the same fact:</p> <pre><code>example : s \u2229 (t \u222a u) \u2286 s \u2229 t \u222a s \u2229 u := by\n  rintro x \u27e8xs, xt | xu\u27e9\n  \u00b7 left; exact \u27e8xs, xt\u27e9\n  . right; exact \u27e8xs, xu\u27e9\n</code></pre> <p>As an exercise, try proving the other inclusion:</p> <pre><code>example : s \u2229 t \u222a s \u2229 u \u2286 s \u2229 (t \u222a u) := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  rintro x (\u27e8xs, xt\u27e9 | \u27e8xs, xu\u27e9)\n  \u00b7 use xs; left; exact xt\n  . use xs; right; exact xu\n</code></pre> <p>It might help to know that when using <code>rintro</code>, sometimes we need to use parentheses around a disjunctive pattern <code>h1 | h2</code> to get Lean to parse it correctly.</p> <p>The library also defines set difference, <code>s \\ t</code>, where the backslash is a special unicode character entered as <code>\\\\</code>. The expression <code>x \u2208 s \\ t</code> expands to <code>x \u2208 s \u2227 x \u2209 t</code>. (The <code>\u2209</code> can be entered as <code>\\notin</code>.) It can be rewritten manually using <code>Set.diff_eq</code> and <code>dsimp</code> or <code>Set.mem_diff</code>, but the following two proofs of the same inclusion show how to avoid using them.</p> <pre><code>example : (s \\ t) \\ u \u2286 s \\ (t \u222a u) := by\n  intro x xstu\n  have xs : x \u2208 s := xstu.1.1\n  have xnt : x \u2209 t := xstu.1.2\n  have xnu : x \u2209 u := xstu.2\n  constructor\n  \u00b7 exact xs\n  intro xtu\n  -- x \u2208 t \u2228 x \u2208 u\n  rcases xtu with xt | xu\n  \u00b7 show False; exact xnt xt\n  . show False; exact xnu xu\n\nexample : (s \\ t) \\ u \u2286 s \\ (t \u222a u) := by\n  rintro x \u27e8\u27e8xs, xnt\u27e9, xnu\u27e9\n  use xs\n  rintro (xt | xu) &lt;;&gt; contradiction\n</code></pre> <p>As an exercise, prove the reverse inclusion:</p> <pre><code>example : s \\ (t \u222a u) \u2286 (s \\ t) \\ u := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  rintro x \u27e8xs, xntu\u27e9\n  constructor\n  use xs\n  \u00b7 intro xt\n    exact xntu (Or.inl xt)\n  intro xu\n  apply xntu (Or.inr xu)\n</code></pre> <p>To prove that two sets are equal, it suffices to show that every element of one is an element of the other. This principle is known as \"extensionality,\" and, unsurprisingly, the <code>ext</code> tactic is equipped to handle it.</p> <pre><code>example : s \u2229 t = t \u2229 s := by\n  ext x\n  simp only [mem_inter_iff]\n  constructor\n  \u00b7 rintro \u27e8xs, xt\u27e9; exact \u27e8xt, xs\u27e9\n  . rintro \u27e8xt, xs\u27e9; exact \u27e8xs, xt\u27e9\n</code></pre> <p>Once again, deleting the line <code>simp only [mem_inter_iff]</code> does not harm the proof. In fact, if you like inscrutable proof terms, the following one-line proof is for you:</p> <pre><code>example : s \u2229 t = t \u2229 s :=\n  Set.ext fun x \u21a6 \u27e8fun \u27e8xs, xt\u27e9 \u21a6 \u27e8xt, xs\u27e9, fun \u27e8xt, xs\u27e9 \u21a6 \u27e8xs, xt\u27e9\u27e9\n</code></pre> <p>Here is an even shorter proof, using the simplifier:</p> <pre><code>example : s \u2229 t = t \u2229 s := by ext x; simp [and_comm]\n</code></pre> <p>An alternative to using <code>ext</code> is to use the theorem <code>Subset.antisymm</code> which allows us to prove an equation <code>s = t</code> between sets by proving <code>s \u2286 t</code> and <code>t \u2286 s</code>.</p> <pre><code>example : s \u2229 t = t \u2229 s := by\n  apply Subset.antisymm\n  \u00b7 rintro x \u27e8xs, xt\u27e9; exact \u27e8xt, xs\u27e9\n  . rintro x \u27e8xt, xs\u27e9; exact \u27e8xs, xt\u27e9\n</code></pre> <p>Try finishing this proof term:</p> <pre><code>example : s \u2229 t = t \u2229 s :=\n    Subset.antisymm sorry sorry\n    Subset.antisymm\n    (fun x \u27e8xs, xt\u27e9 \u21a6 \u27e8xt, xs\u27e9) fun x \u27e8xt, xs\u27e9 \u21a6 \u27e8xs, xt\u27e9\n</code></pre> <p>Remember that you can replace <code>sorry</code> by an underscore, and when you hover over it, Lean will show you what it expects at that point.</p> <p>Here are some set-theoretic identities you might enjoy proving:</p> <pre><code>example : s \u2229 (s \u222a t) = s := by\n  sorry\n\nexample : s \u222a s \u2229 t = s := by\n  sorry\n\nexample : s \\ t \u222a t = s \u222a t := by\n  sorry\n\nexample : s \\ t \u222a t \\ s = (s \u222a t) \\ (s \u2229 t) := by\n  sorry\n</code></pre> <p>When it comes to representing sets, here is what is going on underneath the hood. In type theory, a property or predicate on a type <code>\u03b1</code> is just a function <code>P : \u03b1 \u2192 Prop</code>. This makes sense: given <code>a : \u03b1</code>, <code>P a</code> is just the proposition that <code>P</code> holds of <code>a</code>. In the library, <code>Set \u03b1</code> is defined to be <code>\u03b1 \u2192 Prop</code> and <code>x \u2208 s</code> is defined to be <code>s x</code>. In other words, sets are really properties, treated as objects.</p> <p>The library also defines set-builder notation. The expression <code>{ y | P y }</code> unfolds to <code>(fun y \u21a6 P y)</code>, so <code>x \u2208 { y | P y }</code> reduces to <code>P x</code>. So we can turn the property of being even into the set of even numbers:</p> <pre><code>def evens : Set \u2115 :=\n  { n | Even n }\n\ndef odds : Set \u2115 :=\n  { n | \u00acEven n }\n\nexample : evens \u222a odds = univ := by\n  rw [evens, odds]\n  ext n\n  simp\n  apply Classical.em\n</code></pre> <p>You should step through this proof and make sure you understand what is going on. Try deleting the line <code>rw [evens, odds]</code> and confirm that the proof still works.</p> <p>In fact, set-builder notation is used to define</p> <ul> <li><code>s \u2229 t</code> as <code>{x | x \u2208 s \u2227 x \u2208 t}</code>,</li> <li><code>s \u222a t</code> as <code>{x | x \u2208 s \u2228 x \u2208 t}</code>,</li> <li><code>\u2205</code> as <code>{x | False}</code>, and</li> <li><code>univ</code> as <code>{x | True}</code>.</li> </ul> <p>We often need to indicate the type of <code>\u2205</code> and <code>univ</code> explicitly, because Lean has trouble guessing which ones we mean. The following examples show how Lean unfolds the last two definitions when needed. In the second one, <code>trivial</code> is the canonical proof of <code>True</code> in the library.</p> <pre><code>example (x : \u2115) (h : x \u2208 (\u2205 : Set \u2115)) : False :=\n  h\n\nexample (x : \u2115) : x \u2208 (univ : Set \u2115) :=\n  trivial\n</code></pre> <p>As an exercise, prove the following inclusion. Use <code>intro n</code> to unfold the definition of subset, and use the simplifier to reduce the set-theoretic constructions to logic. We also recommend using the theorems <code>Nat.Prime.eq_two_or_odd</code> and <code>Nat.even_iff</code>.</p> <pre><code>example : { n | Nat.Prime n } \u2229 { n | n &gt; 2 } \u2286 { n | \u00acEven n } := by\n  sorry\n</code></pre> <p>Be careful: it is somewhat confusing that the library has multiple versions of the predicate <code>Prime</code>. The most general one makes sense in any commutative monoid with a zero element. The predicate <code>Nat.Prime</code> is specific to the natural numbers. Fortunately, there is a theorem that says that in the specific case, the two notions agree, so you can always rewrite one to the other.</p> <pre><code>#print Prime\n\n#print Nat.Prime\n\nexample (n : \u2115) : Prime n \u2194 Nat.Prime n :=\n  Nat.prime_iff.symm\n\nexample (n : \u2115) (h : Prime n) : Nat.Prime n := by\n  rw [Nat.prime_iff]\n  exact h\n</code></pre> <p>.. index:: rwa, tactics ; rwa</p> <p>The <code>rwa</code> tactic follows a rewrite with the assumption tactic.</p> <pre><code>example (n : \u2115) (h : Prime n) : Nat.Prime n := by\n  rwa [Nat.prime_iff]\n</code></pre> <p>-- BOTH: end</p> <p>.. index:: bounded quantifiers</p> <p>Lean introduces the notation <code>\u2200 x \u2208 s, ...</code>, \"for every <code>x</code> in <code>s</code> .,\" as an abbreviation for <code>\u2200 x, x \u2208 s \u2192 ...</code>. It also introduces the notation <code>\u2203 x \u2208 s, ...,</code> \"there exists an <code>x</code> in <code>s</code> such that ..\" These are sometimes known as bounded quantifiers, because the construction serves to restrict their significance to the set <code>s</code>. As a result, theorems in the library that make use of them often contain <code>ball</code> or <code>bex</code> in the name. The theorem <code>bex_def</code> asserts that <code>\u2203 x \u2208 s, ...</code> is equivalent to <code>\u2203 x, x \u2208 s \u2227 ...,</code> but when they are used with <code>rintro</code>, <code>use</code>, and anonymous constructors, these two expressions behave roughly the same. As a result, we usually don't need to use <code>bex_def</code> to transform them explicitly. Here is are some examples of how they are used:</p> <pre><code>variable (s t : Set \u2115)\n\nexample (h\u2080 : \u2200 x \u2208 s, \u00acEven x) (h\u2081 : \u2200 x \u2208 s, Prime x) : \u2200 x \u2208 s, \u00acEven x \u2227 Prime x := by\n  intro x xs\n  constructor\n  \u00b7 apply h\u2080 x xs\n  apply h\u2081 x xs\n\nexample (h : \u2203 x \u2208 s, \u00acEven x \u2227 Prime x) : \u2203 x \u2208 s, Prime x := by\n  rcases h with \u27e8x, xs, _, prime_x\u27e9\n  use x, xs\n</code></pre> <p>See if you can prove these slight variations:</p> <pre><code>section\nvariable (ssubt : s \u2286 t)\n\nexample (h\u2080 : \u2200 x \u2208 t, \u00acEven x) (h\u2081 : \u2200 x \u2208 t, Prime x) : \u2200 x \u2208 s, \u00acEven x \u2227 Prime x := by\n  sorry\n\nexample (h : \u2203 x \u2208 s, \u00acEven x \u2227 Prime x) : \u2203 x \u2208 t, Prime x := by\n  sorry\n\nend\n</code></pre> <p>Indexed unions and intersections are another important set-theoretic construction. We can model a sequence \\(A_0, A_1, A_2, \\ldots\\) of sets of elements of <code>\u03b1</code> as a function <code>A : \u2115 \u2192 Set \u03b1</code>, in which case <code>\u22c3 i, A i</code> denotes their union, and <code>\u22c2 i, A i</code> denotes their intersection. There is nothing special about the natural numbers here, so <code>\u2115</code> can be replaced by any type <code>I</code> used to index the sets. The following illustrates their use.</p> <pre><code>variable {\u03b1 I : Type*}\nvariable (A B : I \u2192 Set \u03b1)\nvariable (s : Set \u03b1)\n\nopen Set\n\nexample : (s \u2229 \u22c3 i, A i) = \u22c3 i, A i \u2229 s := by\n  ext x\n  simp only [mem_inter_iff, mem_iUnion]\n  constructor\n  \u00b7 rintro \u27e8xs, \u27e8i, xAi\u27e9\u27e9\n    exact \u27e8i, xAi, xs\u27e9\n  rintro \u27e8i, xAi, xs\u27e9\n  exact \u27e8xs, \u27e8i, xAi\u27e9\u27e9\n\nexample : (\u22c2 i, A i \u2229 B i) = (\u22c2 i, A i) \u2229 \u22c2 i, B i := by\n  ext x\n  simp only [mem_inter_iff, mem_iInter]\n  constructor\n  \u00b7 intro h\n    constructor\n    \u00b7 intro i\n      exact (h i).1\n    intro i\n    exact (h i).2\n  rintro \u27e8h1, h2\u27e9 i\n  constructor\n  \u00b7 exact h1 i\n  exact h2 i\n</code></pre> <p>Parentheses are often needed with an indexed union or intersection because, as with the quantifiers, the scope of the bound variable extends as far as it can.</p> <p>Try proving the following identity. One direction requires classical logic! We recommend using <code>by_cases xs : x \u2208 s</code> at an appropriate point in the proof.</p> <pre><code>example : (s \u222a \u22c2 i, A i) = \u22c2 i, A i \u222a s := by\n  sorry\n</code></pre> <p>Mathlib also has bounded unions and intersections, which are analogous to the bounded quantifiers. You can unpack their meaning with <code>mem_iUnion\u2082</code> and <code>mem_iInter\u2082</code>. As the following examples show, Lean's simplifier carries out these replacements as well.</p> <pre><code>def primes : Set \u2115 :=\n  { x | Nat.Prime x }\n\nexample : (\u22c3 p \u2208 primes, { x | p ^ 2 \u2223 x }) = { x | \u2203 p \u2208 primes, p ^ 2 \u2223 x } :=by\n  ext\n  rw [mem_iUnion\u2082]\n  simp\n\nexample : (\u22c3 p \u2208 primes, { x | p ^ 2 \u2223 x }) = { x | \u2203 p \u2208 primes, p ^ 2 \u2223 x } := by\n  ext\n  simp\n\nexample : (\u22c2 p \u2208 primes, { x | \u00acp \u2223 x }) \u2286 { x | x = 1 } := by\n  intro x\n  contrapose!\n  simp\n  apply Nat.exists_prime_and_dvd\n</code></pre> <p>Try solving the following example, which is similar. If you start typing <code>eq_univ</code>, tab completion will tell you that <code>apply eq_univ_of_forall</code> is a good way to start the proof. We also recommend using the theorem <code>Nat.exists_infinite_primes</code>.</p> <pre><code>example : (\u22c3 p \u2208 primes, { x | x \u2264 p }) = univ := by\n  sorry\n</code></pre> <p>Give a collection of sets, <code>s : Set (Set \u03b1)</code>, their union, <code>\u22c3\u2080 s</code>, has type <code>Set \u03b1</code> and is defined as <code>{x | \u2203 t \u2208 s, x \u2208 t}</code>. Similarly, their intersection, <code>\u22c2\u2080 s</code>, is defined as <code>{x | \u2200 t \u2208 s, x \u2208 t}</code>. These operations are called <code>sUnion</code> and <code>sInter</code>, respectively. The following examples show their relationship to bounded union and intersection.</p> <pre><code>variable {\u03b1 : Type*} (s : Set (Set \u03b1))\n\nexample : \u22c3\u2080 s = \u22c3 t \u2208 s, t := by\n  ext x\n  rw [mem_iUnion\u2082]\n  simp\n\nexample : \u22c2\u2080 s = \u22c2 t \u2208 s, t := by\n  ext x\n  rw [mem_iInter\u2082]\n  rfl\n</code></pre> <p>In the library, these identities are called <code>sUnion_eq_biUnion</code> and <code>sInter_eq_biInter</code>.</p>"},{"location":"MIL/C04_Sets_and_Functions/S02_Functions/","title":"S02 Functions","text":""},{"location":"MIL/C04_Sets_and_Functions/S02_Functions/#functions","title":"Functions","text":"<p>If <code>f : \u03b1 \u2192 \u03b2</code> is a function and <code>p</code> is a set of elements of type <code>\u03b2</code>, the library defines <code>preimage f p</code>, written <code>f \u207b\u00b9' p</code>, to be <code>{x | f x \u2208 p}</code>. The expression <code>x \u2208 f \u207b\u00b9' p</code> reduces to <code>f x \u2208 p</code>. This is often convenient, as in the following example:</p> <pre><code>variable {\u03b1 \u03b2 : Type*}\nvariable (f : \u03b1 \u2192 \u03b2)\nvariable (s t : Set \u03b1)\nvariable (u v : Set \u03b2)\n\nopen Function\nopen Set\n\nexample : f \u207b\u00b9' (u \u2229 v) = f \u207b\u00b9' u \u2229 f \u207b\u00b9' v := by\n  ext\n  rfl\n</code></pre> <p>If <code>s</code> is a set of elements of type <code>\u03b1</code>, the library also defines <code>image f s</code>, written <code>f '' s</code>, to be <code>{y | \u2203 x, x \u2208 s \u2227 f x = y}</code>. So a hypothesis <code>y \u2208 f '' s</code> decomposes to a triple <code>\u27e8x, xs, xeq\u27e9</code> with <code>x : \u03b1</code> satisfying the hypotheses <code>xs : x \u2208 s</code> and <code>xeq : f x = y</code>. The <code>rfl</code> tag in the <code>rintro</code> tactic (see :numref:<code>the_existential_quantifier</code>) was made precisely for this sort of situation.</p> <pre><code>example : f '' (s \u222a t) = f '' s \u222a f '' t := by\n  ext y; constructor\n  \u00b7 rintro \u27e8x, xs | xt, rfl\u27e9\n    \u00b7 left\n      use x, xs\n    right\n    use x, xt\n  rintro (\u27e8x, xs, rfl\u27e9 | \u27e8x, xt, rfl\u27e9)\n  \u00b7 use x, Or.inl xs\n  use x, Or.inr xt\n</code></pre> <p>Notice also that the <code>use</code> tactic applies <code>rfl</code> to close goals when it can.</p> <p>Here is another example:</p> <pre><code>example : s \u2286 f \u207b\u00b9' (f '' s) := by\n  intro x xs\n  show f x \u2208 f '' s\n  use x, xs\n</code></pre> <p>We can replace the line <code>use x, xs</code> by <code>apply mem_image_of_mem f xs</code> if we want to use a theorem specifically designed for that purpose. But knowing that the image is defined in terms of an existential quantifier is often convenient.</p> <p>The following equivalence is a good exercise:</p> <pre><code>example : f '' s \u2286 v \u2194 s \u2286 f \u207b\u00b9' v := by\n  sorry\n</code></pre> <p>It shows that <code>image f</code> and <code>preimage f</code> are an instance of what is known as a Galois connection between <code>Set \u03b1</code> and <code>Set \u03b2</code>, each partially ordered by the subset relation. In the library, this equivalence is named <code>image_subset_iff</code>. In practice, the right-hand side is often the more useful representation, because <code>y \u2208 f \u207b\u00b9' t</code> unfolds to <code>f y \u2208 t</code> whereas working with <code>x \u2208 f '' s</code> requires decomposing an existential quantifier.</p> <p>Here is a long list of set-theoretic identities for you to enjoy. You don't have to do all of them at once; do a few of them, and set the rest aside for a rainy day.</p> <pre><code>example (h : Injective f) : f \u207b\u00b9' (f '' s) \u2286 s := by\n  sorry\n\nexample : f '' (f \u207b\u00b9' u) \u2286 u := by\n  sorry\n\nexample (h : Surjective f) : u \u2286 f '' (f \u207b\u00b9' u) := by\n  sorry\n\nexample (h : s \u2286 t) : f '' s \u2286 f '' t := by\n  sorry\n\nexample (h : u \u2286 v) : f \u207b\u00b9' u \u2286 f \u207b\u00b9' v := by\n  sorry\n\nexample : f \u207b\u00b9' (u \u222a v) = f \u207b\u00b9' u \u222a f \u207b\u00b9' v := by\n  sorry\n\nexample : f '' (s \u2229 t) \u2286 f '' s \u2229 f '' t := by\n  sorry\n\nexample (h : Injective f) : f '' s \u2229 f '' t \u2286 f '' (s \u2229 t) := by\n  sorry\n\nexample : f '' s \\ f '' t \u2286 f '' (s \\ t) := by\n  sorry\n\nexample : f \u207b\u00b9' u \\ f \u207b\u00b9' v \u2286 f \u207b\u00b9' (u \\ v) := by\n  sorry\n\nexample : f '' s \u2229 v = f '' (s \u2229 f \u207b\u00b9' v) := by\n  sorry\n\nexample : f '' (s \u2229 f \u207b\u00b9' u) \u2286 f '' s \u2229 u := by\n  sorry\n\nexample : s \u2229 f \u207b\u00b9' u \u2286 f \u207b\u00b9' (f '' s \u2229 u) := by\n  sorry\n\nexample : s \u222a f \u207b\u00b9' u \u2286 f \u207b\u00b9' (f '' s \u222a u) := by\n  sorry\n</code></pre> <p>You can also try your hand at the next group of exercises, which characterize the behavior of images and preimages with respect to indexed unions and intersections. In the third exercise, the argument <code>i : I</code> is needed to guarantee that the index set is nonempty. To prove any of these, we recommend using <code>ext</code> or <code>intro</code> to unfold the meaning of an equation or inclusion between sets, and then calling <code>simp</code> to unpack the conditions for membership.</p> <pre><code>variable {I : Type*} (A : I \u2192 Set \u03b1) (B : I \u2192 Set \u03b2)\n\nexample : (f '' \u22c3 i, A i) = \u22c3 i, f '' A i := by\n  sorry\n\nexample : (f '' \u22c2 i, A i) \u2286 \u22c2 i, f '' A i := by\n  sorry\n\nexample (i : I) (injf : Injective f) : (\u22c2 i, f '' A i) \u2286 f '' \u22c2 i, A i := by\n  sorry\n\nexample : (f \u207b\u00b9' \u22c3 i, B i) = \u22c3 i, f \u207b\u00b9' B i := by\n  sorry\n\nexample : (f \u207b\u00b9' \u22c2 i, B i) = \u22c2 i, f \u207b\u00b9' B i := by\n  sorry\n</code></pre> <p>The library defines a predicate <code>InjOn f s</code> to say that <code>f</code> is injective on <code>s</code>. It is defined as follows:</p> <pre><code>example : InjOn f s \u2194 \u2200 x\u2081 \u2208 s, \u2200 x\u2082 \u2208 s, f x\u2081 = f x\u2082 \u2192 x\u2081 = x\u2082 :=\n  Iff.refl _\n</code></pre> <p>The statement <code>Injective f</code> is provably equivalent to <code>InjOn f univ</code>. Similarly, the library defines <code>range f</code> to be <code>{x | \u2203y, f y = x}</code>, so <code>range f</code> is provably equal to <code>f '' univ</code>. This is a common theme in Mathlib: although many properties of functions are defined relative to their full domain, there are often relativized versions that restrict the statements to a subset of the domain type.</p> <p>Here is are some examples of <code>InjOn</code> and <code>range</code> in use:</p> <pre><code>open Set Real\n\nexample : InjOn log { x | x &gt; 0 } := by\n  intro x xpos y ypos\n  intro e\n  calc\n    x = exp (log x) := by rw [exp_log xpos]\n    _ = exp (log y) := by rw [e]\n    _ = y := by rw [exp_log ypos]\n\n\nexample : range exp = { y | y &gt; 0 } := by\n  ext y; constructor\n  \u00b7 rintro \u27e8x, rfl\u27e9\n    apply exp_pos\n  intro ypos\n  use log y\n  rw [exp_log ypos]\n</code></pre> <p>Try proving these:</p> <pre><code>example : InjOn sqrt { x | x \u2265 0 } := by\n  sorry\n\nexample : InjOn (fun x \u21a6 x ^ 2) { x : \u211d | x \u2265 0 } := by\n  sorry\n\nexample : sqrt '' { x | x \u2265 0 } = { y | y \u2265 0 } := by\n  sorry\n\nexample : (range fun x \u21a6 x ^ 2) = { y : \u211d | y \u2265 0 } := by\n  sorry\n</code></pre> <p>To define the inverse of a function <code>f : \u03b1 \u2192 \u03b2</code>, we will use two new ingredients. First, we need to deal with the fact that an arbitrary type in Lean may be empty. To define the inverse to <code>f</code> at <code>y</code> when there is no <code>x</code> satisfying <code>f x = y</code>, we want to assign a default value in <code>\u03b1</code>. Adding the annotation <code>[Inhabited \u03b1]</code> as a variable is tantamount to assuming that <code>\u03b1</code> has a preferred element, which is denoted <code>default</code>. Second, in the case where there is more than one <code>x</code> such that <code>f x = y</code>, the inverse function needs to choose one of them. This requires an appeal to the axiom of choice. Lean allows various ways of accessing it; one convenient method is to use the classical <code>choose</code> operator, illustrated below.</p> <pre><code>variable {\u03b1 \u03b2 : Type*} [Inhabited \u03b1]\n\n-- EXAMPLES:\n#check (default : \u03b1)\n\nvariable (P : \u03b1 \u2192 Prop) (h : \u2203 x, P x)\n\n#check Classical.choose h\n\nexample : P (Classical.choose h) :=\n  Classical.choose_spec h\n</code></pre> <p>Given <code>h : \u2203 x, P x</code>, the value of <code>Classical.choose h</code> is some <code>x</code> satisfying <code>P x</code>. The theorem <code>Classical.choose_spec h</code> says that <code>Classical.choose h</code> meets this specification.</p> <p>With these in hand, we can define the inverse function as follows:</p> <pre><code>noncomputable section\n\nopen Classical\n\ndef inverse (f : \u03b1 \u2192 \u03b2) : \u03b2 \u2192 \u03b1 := fun y : \u03b2 \u21a6\n  if h : \u2203 x, f x = y then Classical.choose h else default\n\ntheorem inverse_spec {f : \u03b1 \u2192 \u03b2} (y : \u03b2) (h : \u2203 x, f x = y) : f (inverse f y) = y := by\n  rw [inverse, dif_pos h]\n  exact Classical.choose_spec h\n</code></pre> <p>The lines <code>noncomputable section</code> and <code>open Classical</code> are needed because we are using classical logic in an essential way. On input <code>y</code>, the function <code>inverse f</code> returns some value of <code>x</code> satisfying <code>f x = y</code> if there is one, and a default element of <code>\u03b1</code> otherwise. This is an instance of a dependent if construction, since in the positive case, the value returned, <code>Classical.choose h</code>, depends on the assumption <code>h</code>. The identity <code>dif_pos h</code> rewrites <code>if h : e then a else b</code> to <code>a</code> given <code>h : e</code>, and, similarly, <code>dif_neg h</code> rewrites it to <code>b</code> given <code>h : \u00ac e</code>. There are also versions <code>if_pos</code> and <code>if_neg</code> that works for non-dependent if constructions and will be used in the next section. The theorem <code>inverse_spec</code> says that <code>inverse f</code> meets the first part of this specification.</p> <p>Don't worry if you do not fully understand how these work. The theorem <code>inverse_spec</code> alone should be enough to show that <code>inverse f</code> is a left inverse if and only if <code>f</code> is injective and a right inverse if and only if <code>f</code> is surjective. Look up the definition of <code>LeftInverse</code> and <code>RightInverse</code> by double-clicking or right-clicking on them in VS Code, or using the commands <code>#print LeftInverse</code> and <code>#print RightInverse</code>. Then try to prove the two theorems. They are tricky! It helps to do the proofs on paper before you start hacking through the details. You should be able to prove each of them with about a half-dozen short lines. If you are looking for an extra challenge, try to condense each proof to a single-line proof term.</p> <pre><code>variable (f : \u03b1 \u2192 \u03b2)\n\nopen Function\n\nexample : Injective f \u2194 LeftInverse (inverse f) f :=\n  sorry\n\nexample : Surjective f \u2194 RightInverse (inverse f) f :=\n  sorry\n</code></pre> <p>We close this section with a type-theoretic statement of Cantor's famous theorem that there is no surjective function from a set to its power set. See if you can understand the proof, and then fill in the two lines that are missing.</p> <pre><code>theorem Cantor : \u2200 f : \u03b1 \u2192 Set \u03b1, \u00acSurjective f := by\n  intro f surjf\n  let S := { i | i \u2209 f i }\n  rcases surjf S with \u27e8j, h\u27e9\n  have h\u2081 : j \u2209 f j := by\n    intro h'\n    have : j \u2209 f j := by rwa [h] at h'\n    contradiction\n  have h\u2082 : j \u2208 S\n  sorry\n  have h\u2083 : j \u2209 S\n  sorry\n  contradiction\n</code></pre> <p>-- COMMENTS: TODO: improve this -- SOLUTIONS: theorem Cantor\u03b1\u03b1 : \u2200 f : \u03b1 \u2192 Set \u03b1, \u00acSurjective f := by intro f surjf let S := { i | i \u2209 f i } rcases surjf S with \u27e8j, h\u27e9 have h\u2081 : j \u2209 f j := by intro h' have : j \u2209 f j := by rwa [h] at h' contradiction have h\u2082 : j \u2208 S := h\u2081 have h\u2083 : j \u2209 S := by rwa [h] at h\u2081 contradiction</p> <p>-- BOTH: end</p>"},{"location":"MIL/C04_Sets_and_Functions/S03_The_Schroeder_Bernstein_Theorem/","title":"S03 The Schroeder Bernstein Theorem","text":""},{"location":"MIL/C04_Sets_and_Functions/S03_The_Schroeder_Bernstein_Theorem/#the-schroder-bernstein-theorem","title":"The Schr\u00f6der-Bernstein Theorem","text":"<p>We close this chapter with an elementary but nontrivial theorem of set theory. Let \\(\\alpha\\) and \\(\\beta\\) be sets. (In our formalization, they will actually be types.) Suppose \\(f : \\alpha \u2192 \\beta\\) and \\(g : \\beta \u2192 \\alpha\\) are both injective. Intuitively, this means that \\(\\alpha\\) is no bigger than \\(\\beta\\) and vice-versa. If \\(\\alpha\\) and \\(\\beta\\) are finite, this implies that they have the same cardinality, which is equivalent to saying that there is a bijection between them. In the nineteenth century, Cantor stated that same result holds even in the case where \\(\\alpha\\) and \\(\\beta\\) are infinite. This was eventually established by Dedekind, Schr\u00f6der, and Bernstein independently.</p> <p>Our formalization will introduce some new methods that we will explain in greater detail in chapters to come. Don't worry if they go by too quickly here. Our goal is to show you that you already have the skills to contribute to the formal proof of a real mathematical result.</p> <p>To understand the idea behind the proof, consider the image of the map \\(g\\) in \\(\\alpha\\). On that image, the inverse of \\(g\\) is defined and is a bijection with \\(\\beta\\).</p> <p>The problem is that the bijection does not include the shaded region in the diagram, which is nonempty if \\(g\\) is not surjective. Alternatively, we can use \\(f\\) to map all of \\(\\alpha\\) to \\(\\beta\\), but in that case the problem is that if \\(f\\) is not surjective, it will miss some elements of \\(\\beta\\).</p> <p>But now consider the composition \\(g \\circ f\\) from \\(\\alpha\\) to itself. Because the composition is injective, it forms a bijection between \\(\\alpha\\) and its image, yielding a scaled-down copy of \\(alpha\\) inside itself.</p> <p>This composition maps the inner shaded ring to yet another such set, which we can think of as an even smaller concentric shaded ring, and so on. This yields a concentric sequence of shaded rings, each of which is in bijective correspondence with the next. If we map each ring to the next and leave the unshaded parts of \\(\\alpha\\) alone, we have a bijection of \\(\\alpha\\) with the image of \\(g\\). Composing with \\(g^{-1}\\), this yields the desired bijection between \\(\\alpha\\) and \\(\\beta\\).</p> <p>We can describe this bijection more simply. Let \\(A\\) be the union of the sequence of shaded regions, and define \\(h : \\alpha \\to \\beta\\) as follows:</p> \\[ h(x) = \\begin{cases} f(x) &amp; \\text{if $x \\in A$} \\\\ g^{-1}(x) &amp; \\text{otherwise.} \\end{cases} \\] <p>In other words, we use \\(f\\) on the shaded parts, and we use the inverse of \\(g\\) everywhere else. The resulting map \\(h\\) is injective because each component is injective and the images of the two components are disjoint. To see that it is surjective, suppose we are given a \\(y\\) in \\(\\beta\\), and consider \\(g(y)\\). If \\(g(y)\\) is in one of the shaded regions, it cannot be in the first ring, so we have \\(g(y) = g(f(x))\\) for some \\(x\\) is in the previous ring. By the injectivity of \\(g\\), we have \\(h(x) = f(x) = y\\). If \\(g(y)\\) is not in the shaded region, then by the definition of \\(h\\), we have \\(h(g(y))= y\\). Either way, \\(y\\) is in the image of \\(h\\).</p> <p>This argument should sound plausible, but the details are delicate. Formalizing the proof will not only improve our confidence in the result, but also help us understand it better. Because the proof uses classical logic, we tell Lean that our definitions will generally not be computable.</p> <pre><code>noncomputable section\nopen Classical\nvariable {\u03b1 \u03b2 : Type*} [Nonempty \u03b2]\n</code></pre> <p>The annotation <code>[Nonempty \u03b2]</code> specifies that <code>\u03b2</code> is nonempty. We use it because the Mathlib primitive that we will use to construct \\(g^{-1}\\) requires it. The case of the theorem where \\(\\beta\\) is empty is trivial, and even though it would not be hard to generalize the formalization to cover that case as well, we will not bother. Specifically, we need the hypothesis <code>[Nonempty \u03b2]</code> for the operation <code>invFun</code> that is defined in Mathlib. Given <code>x : \u03b1</code>, <code>invFun g x</code> chooses a preimage of <code>x</code> in <code>\u03b2</code> if there is one, and returns an arbitrary element of <code>\u03b2</code> otherwise. The function <code>invFun g</code> is always a left inverse if <code>g</code> is injective and a right inverse if <code>g</code> is surjective.</p> <p>We define the set corresponding to the union of the shaded regions as follows.</p> <pre><code>variable (f : \u03b1 \u2192 \u03b2) (g : \u03b2 \u2192 \u03b1)\n\ndef sbAux : \u2115 \u2192 Set \u03b1\n  | 0 =&gt; univ \\ g '' univ\n  | n + 1 =&gt; g '' (f '' sbAux n)\n\ndef sbSet :=\n  \u22c3 n, sbAux f g n\n</code></pre> <p>The definition <code>sbAux</code> is an example of a recursive definition, which we will explain in the next chapter. It defines a sequence of sets</p> \\[ S*0 &amp;= \\alpha \u2216 g(\\beta) \\\\ S*{n+1} &amp;= g(f(S_n)). \\] <p>The definition <code>sbSet</code> corresponds to the set \\(A = \\bigcup_{n \\in \\mathbb{N}} S_n\\) in our proof sketch. The function \\(h\\) described above is now defined as follows:</p> <pre><code>def sbFun (x : \u03b1) : \u03b2 :=\n  if x \u2208 sbSet f g then f x else invFun g x\n</code></pre> <p>We will need the fact that our definition of \\(g^{-1}\\) is a right inverse on the complement of \\(A\\), which is to say, on the non-shaded regions of \\(\\alpha\\). This is so because the outermost ring, \\(S_0\\), is equal to \\(\\alpha \\setminus g(\\beta)\\), so the complement of \\(A\\) is contained in \\(g(\\beta)\\). As a result, for every \\(x\\) in the complement of \\(A\\), there is a \\(y\\) such that \\(g(y) = x\\). (By the injectivity of \\(g\\), this \\(y\\) is unique, but next theorem says only that <code>invFun g x</code> returns some <code>y</code> such that <code>g y = x</code>.)</p> <p>Step through the proof below, make sure you understand what is going on, and fill in the remaining parts. You will need to use <code>invFun_eq</code> at the end. Notice that rewriting with <code>sbAux</code> here replaces <code>sbAux f g 0</code> with the right-hand side of the corresponding defining equation.</p> <pre><code>theorem sb*right_inv {x : \u03b1} (hx : x \u2209 sbSet f g) : g (invFun g x) = x := by\n  have : x \u2208 g '' univ := by\n    contrapose! hx\n    rw [sbSet, mem_iUnion]\n    use 0\n    rw [sbAux, mem_diff]\n    sorry\n  exact \u27e8mem_univ *, hx\u27e9\n  have : \u2203 y, g y = x := by\n    sorry\n  sorry\n</code></pre> <p>We now turn to the proof that \\(h\\) is injective. Informally, the proof goes as follows. First, suppose \\(h(x_1) = h(x_2)\\). If \\(x_1\\) is in \\(A\\), then \\(h(x_1) = f(x_1)\\), and we can show that \\(x_2\\) is in \\(A\\) as follows. If it isn't, then we have \\(h(x_2) = g^{-1}(x_2)\\). From \\(f(x_1) = h(x_1) = h(x_2)\\) we have \\(g(f(x_1)) = x_2\\). From the definition of \\(A\\), since \\(x_1\\) is in \\(A\\), \\(x_2\\) is in \\(A\\) as well, a contradiction. Hence, if $x_1` is in \\(A\\), so is \\(x_2\\), in which case we have \\(f(x_1) = h(x_1) = h(x_2) = f(x_2)\\). The injectivity of \\(f\\) then implies \\(x_1 = x_2\\). The symmetric argument shows that if \\(x_2\\) is in \\(A\\), then so is \\(x_1\\), which again implies \\(x_1 = x_2\\).</p> <p>The only remaining possibility is that neither \\(x_1\\) nor \\(x_2\\) is in \\(A\\). In that case, we have \\(g^{-1}(x_1) = h(x_1) = h(x_2) = g^{-1}(x_2)\\). Applying \\(g\\) to both sides yields \\(x_1 = x_2\\).</p> <p>Once again, we encourage you to step through the following proof to see how the argument plays out in Lean. See if you can finish off the proof using <code>sb_right_inv</code>.</p> <pre><code>theorem sb*injective (hf : Injective f) : Injective (sbFun f g) := by\n  set A := sbSet f g with A_def\n  set h := sbFun f g with h_def\n  intro x\u2081 x\u2082\n  intro (hxeq : h x\u2081 = h x\u2082)\n  show x\u2081 = x\u2082\n  simp only [h_def, sbFun, \u2190 A_def] at hxeq\n  by_cases xA : x\u2081 \u2208 A \u2228 x\u2082 \u2208 A\n  \u00b7 wlog x\u2081A : x\u2081 \u2208 A generalizing x\u2081 x\u2082 hxeq xA\n    \u00b7 symm\n      apply this hxeq.symm xA.symm (xA.resolve_left x\u2081A)\n    have x\u2082A : x\u2082 \u2208 A := by\n      apply \\_root*.not_imp_self.mp\n      intro (x\u2082nA : x\u2082 \u2209 A)\n      rw [if_pos x\u2081A, if_neg x\u2082nA] at hxeq\n      rw [A_def, sbSet, mem_iUnion] at x\u2081A\n      have x\u2082eq : x\u2082 = g (f x\u2081) := by\n        sorry\n      rcases x\u2081A with \u27e8n, hn\u27e9\n      rw [A_def, sbSet, mem_iUnion]\n      use n + 1\n      simp [sbAux]\n      exact \u27e8x\u2081, hn, x\u2082eq.symm\u27e9\n    sorry\n  push_neg at xA\n  sorry\n</code></pre> <p>The proof introduces some new tactics. To start with, notice the <code>set</code> tactic, which introduces abbreviations <code>A</code> and <code>h</code> for <code>sbSet f g</code> and <code>sb_fun f g</code> respectively. We name the corresponding defining equations <code>A_def</code> and <code>h_def</code>. The abbreviations are definitional, which is to say, Lean will sometimes unfold them automatically when needed. But not always; for example, when using <code>rw</code>, we generally need to use <code>A_def</code> and <code>h_def</code> explicitly. So the definitions bring a tradeoff: they can make expressions shorter and more readable, but they sometimes require us to do more work.</p> <p>A more interesting tactic is the <code>wlog</code> tactic, which encapsulates the symmetry argument in the informal proof above. We will not dwell on it now, but notice that it does exactly what we want. If you hover over the tactic you can take a look at its documentation.</p> <p>The argument for surjectivity is even easier. Given \\(y\\) in \\(\\beta\\), we consider two cases, depending on whether \\(g(y)\\) is in \\(A\\). If it is, it can't be in \\(S_0\\), the outermost ring, because by definition that is disjoint from the image of \\(g\\). Thus it is an element of \\(S_{n+1}\\) for some \\(n\\). This means that it is of the form \\(g(f(x))\\) for some \\(x\\) in \\(S_n\\). By the injectivity of \\(g\\), we have \\(f(x) = y\\). In the case where \\(g(y)\\) is in the complement of \\(A\\), we immediately have \\(h(g(y))= y\\), and we are done.</p> <p>Once again, we encourage you to step through the proof and fill in the missing parts. The tactic <code>rcases n with _ | n</code> splits on the cases <code>g y \u2208 sbAux f g 0</code> and <code>g y \u2208 sbAux f g (n + 1)</code>. In both cases, calling the simplifier with <code>simp [sbAux]</code> applies the corresponding defining equation of <code>sbAux</code>.</p> <pre><code>theorem sb_surjective (hg : Injective g) : Surjective (sbFun f g) := by\n  set A := sbSet f g with A_def\n  set h := sbFun f g with h_def\n  intro y\n  by_cases gyA : g y \u2208 A\n  \u00b7 rw [A_def, sbSet, mem_iUnion] at gyA\n    rcases gyA with \u27e8n, hn\u27e9\n    rcases n with * | n\n    \u00b7 simp [sbAux] at hn\n    simp [sbAux] at hn\n    rcases hn with \u27e8x, xmem, hx\u27e9\n    use x\n    have : x \u2208 A := by\n      rw [A_def, sbSet, mem_iUnion]\n      exact \u27e8n, xmem\u27e9\n    simp only [h_def, sbFun, if_pos this]\n    exact hg hx\n  sorry\n</code></pre> <p>We can now put it all together. The final statement is short and sweet, and the proof uses the fact that <code>Bijective h</code> unfolds to <code>Injective h \u2227 Surjective h</code>.</p> <pre><code>theorem schroeder_bernstein {f : \u03b1 \u2192 \u03b2} {g : \u03b2 \u2192 \u03b1} (hf : Injective f) (hg : Injective g) :\n  \u2203 h : \u03b1 \u2192 \u03b2, Bijective h :=\n  \u27e8sbFun f g, sb_injective f g hf, sb_surjective f g hg\u27e9\n</code></pre>"},{"location":"MIL/C05_Elementary_Number_Theory/S01_Irrational_Roots/","title":"S01 Irrational Roots","text":""},{"location":"MIL/C05_Elementary_Number_Theory/S01_Irrational_Roots/#irrational-roots","title":"Irrational Roots","text":"<p>Let's start with a fact known to the ancient Greeks, namely, that the square root of 2 is irrational. If we suppose otherwise, we can write \\(\\sqrt{2} = a / b\\) as a fraction in lowest terms. Squaring both sides yields \\(a^2 = 2 b^2\\), which implies that \\(a\\) is even. If we write \\(a = 2c\\), then we get \\(4c^2 = 2 b^2\\) and hence \\(b^2 = 2 c^2\\). This implies that \\(b\\) is also even, contradicting the fact that we have assumed that \\(a / b\\) has been reduced to lowest terms.</p> <p>Saying that \\(a / b\\) is a fraction in lowest terms means that \\(a\\) and \\(b\\) do not have any factors in common, which is to say, they are coprime. Mathlib defines the predicate <code>Nat.Coprime m n</code> to be <code>Nat.gcd m n = 1</code>. Using Lean's anonymous projection notation, if <code>s</code> and <code>t</code> are expressions of type <code>Nat</code>, we can write <code>s.Coprime t</code> instead of <code>Nat.Coprime s t</code>, and similarly for <code>Nat.gcd</code>. As usual, Lean will often unfold the definition of <code>Nat.Coprime</code> automatically when necessary, but we can also do it manually by rewriting or simplifying with the identifier <code>Nat.Coprime</code>. The <code>norm_num</code> tactic is smart enough to compute concrete values.</p> <pre><code>#print Nat.Coprime\n\nexample (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 :=\n  h\n\nexample (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 := by\n  rw [Nat.Coprime] at h\n  exact h\n\nexample : Nat.Coprime 12 7 := by norm_num\n\nexample : Nat.gcd 12 8 = 4 := by norm_num\n</code></pre> <p>We have already encountered the <code>gcd</code> function in :numref:<code>more_on_order_and_divisibility</code>. There is also a version of <code>gcd</code> for the integers; we will return to a discussion of the relationship between different number systems below. There are even a generic <code>gcd</code> function and generic notions of <code>Prime</code> and <code>Coprime</code> that make sense in general classes of algebraic structures. We will come to understand how Lean manages this generality in the next chapter. In the meanwhile, in this section, we will restrict attention to the natural numbers.</p> <p>We also need the notion of a prime number, <code>Nat.Prime</code>. The theorem <code>Nat.prime_def_lt</code> provides one familiar characterization, and <code>Nat.Prime.eq_one_or_self_of_dvd</code> provides another.</p> <pre><code>#check Nat.prime_def_lt\n\nexample (p : \u2115) (prime_p : Nat.Prime p) : 2 \u2264 p \u2227 \u2200 m : \u2115, m &lt; p \u2192 m \u2223 p \u2192 m = 1 := by\n  rwa [Nat.prime_def_lt] at prime_p\n\n#check Nat.Prime.eq_one_or_self_of_dvd\n\nexample (p : \u2115) (prime_p : Nat.Prime p) : \u2200 m : \u2115, m \u2223 p \u2192 m = 1 \u2228 m = p :=\n  prime_p.eq_one_or_self_of_dvd\n\nexample : Nat.Prime 17 := by norm_num\n\n-- commonly used\nexample : Nat.Prime 2 :=\n  Nat.prime_two\n\nexample : Nat.Prime 3 :=\n  Nat.prime_three\n</code></pre> <p>In the natural numbers, a prime number has the property that it cannot be written as a product of nontrivial factors. In a broader mathematical context, an element of a ring that has this property is said to be irreducible. An element of a ring is said to be prime if whenever it divides a product, it divides one of the factors. It is an important property of the natural numbers that in that setting the two notions coincide, giving rise to the theorem <code>Nat.Prime.dvd_mul</code>.</p> <p>We can use this fact to establish a key property in the argument above: if the square of a number is even, then that number is even as well. Mathlib defines the predicate <code>Even</code> in <code>Algebra.Group.Even</code>, but for reasons that will become clear below, we will simply use <code>2 \u2223 m</code> to express that <code>m</code> is even.</p> <pre><code>#check Nat.Prime.dvd_mul\n#check Nat.Prime.dvd_mul Nat.prime_two\n#check Nat.prime_two.dvd_mul\n\ntheorem even_of_even_sqr {m : \u2115} (h : 2 \u2223 m ^ 2) : 2 \u2223 m := by\n  rw [pow_two, Nat.prime_two.dvd_mul] at h\n  cases h &lt;;&gt; assumption\n\nexample {m : \u2115} (h : 2 \u2223 m ^ 2) : 2 \u2223 m :=\n  Nat.Prime.dvd_of_dvd_pow Nat.prime_two h\n</code></pre> <p>As we proceed, you will need to become proficient at finding the facts you need. Remember that if you can guess the prefix of the name and you have imported the relevant library, you can use tab completion (sometimes with <code>ctrl-tab</code>) to find what you are looking for. You can use <code>ctrl-click</code> on any identifier to jump to the file where it is defined, which enables you to browse definitions and theorems nearby. You can also use the search engine on the Lean community web pages, and if all else fails, don't hesitate to ask on Zulip.</p> <pre><code>example (a b c : Nat) (h : a * b = a * c) (h' : a \u2260 0) : b = c :=\n  -- apply? suggests the following:\n  (mul_right_inj' h').mp h\n</code></pre> <p>The heart of our proof of the irrationality of the square root of two is contained in the following theorem. See if you can fill out the proof sketch, using <code>even_of_even_sqr</code> and the theorem <code>Nat.dvd_gcd</code>.</p> <pre><code>example {m n : \u2115} (coprime_mn : m.Coprime n) : m ^ 2 \u2260 2 * n ^ 2 := by\n  intro sqr_eq\n  have : 2 \u2223 m := by\n    sorry\n  obtain \u27e8k, meq\u27e9 := dvd_iff_exists_eq_mul_left.mp this\n  have : 2 * (2 * k ^ 2) = 2 * n ^ 2 := by\n    rw [\u2190 sqr_eq, meq]\n    ring\n  have : 2 * k ^ 2 = n ^ 2 :=\n    sorry\n  have : 2 \u2223 n := by\n    sorry\n  have : 2 \u2223 m.gcd n := by\n    sorry\n  have : 2 \u2223 1 := by\n    sorry\n    convert this\n  norm_num at this\n</code></pre> <p>In fact, with very few changes, we can replace <code>2</code> by an arbitrary prime. Give it a try in the next example. At the end of the proof, you'll need to derive a contradiction from <code>p \u2223 1</code>. You can use <code>Nat.Prime.two_le</code>, which says that any prime number is greater than or equal to two, and <code>Nat.le_of_dvd</code>.</p> <pre><code>example {m n p : \u2115} (coprime_mn : m.Coprime n) (prime_p : p.Prime) : m ^ 2 \u2260 p * n ^ 2 := by\n  sorry\n</code></pre> <p>Let us consider another approach. Here is a quick proof that if \\(p\\) is prime, then \\(m^2 \\ne p n^2\\): if we assume \\(m^2 = p n^2\\) and consider the factorization of \\(m<code>and :math:</code>n\\) into primes, then \\(p\\) occurs an even number of times on the left side of the equation and an odd number of times on the right, a contradiction. Note that this argument requires that \\(n\\) and hence \\(m\\) are not equal to zero. The formalization below confirms that this assumption is sufficient.</p> <p>The unique factorization theorem says that any natural number other than zero can be written as the product of primes in a unique way. Mathlib contains a formal version of this, expressed in terms of a function <code>Nat.factors</code>, which returns the list of prime factors of a number in nondecreasing order. The library proves that all the elements of <code>Nat.factors n</code> are prime, that any <code>n</code> greater than zero is equal to the product of its factors, and that if <code>n</code> is equal to the product of another list of prime numbers, then that list is a permutation of <code>Nat.factors n</code>.</p> <pre><code>#check Nat.factors\n#check Nat.prime_of_mem_factors\n#check Nat.prod_factors\n#check Nat.factors_unique\n</code></pre> <p>You can browse these theorems and others nearby, even though we have not talked about list membership, products, or permutations yet. We won't need any of that for the task at hand. We will instead use the fact that Mathlib has a function <code>Nat.factorization</code>, that represents the same data as a function. Specifically, <code>Nat.factorization n p</code>, which we can also write <code>n.factorization p</code>, returns the multiplicity of <code>p</code> in the prime factorization of <code>n</code>. We will use the following three facts.</p> <pre><code>theorem factorization_mul' {m n : \u2115} (mnez : m \u2260 0) (nnez : n \u2260 0) (p : \u2115) :\n    (m * n).factorization p = m.factorization p + n.factorization p := by\n  rw [Nat.factorization_mul mnez nnez]\n  rfl\n\ntheorem factorization_pow' (n k p : \u2115) :\n    (n ^ k).factorization p = k * n.factorization p := by\n  rw [Nat.factorization_pow]\n  rfl\n\ntheorem Nat.Prime.factorization' {p : \u2115} (prime_p : p.Prime) :\n    p.factorization p = 1 := by\n  rw [prime_p.factorization]\n  simp\n</code></pre> <p>In fact, <code>n.factorization</code> is defined in Lean as a function of finite support, which explains the strange notation you will see as you step through the proofs above. Don't worry about this now. For our purposes here, we can use the three theorems above as a black box.</p> <p>The next example shows that the simplifier is smart enough to replace <code>n^2 \u2260 0</code> by <code>n \u2260 0</code>. The tactic <code>simpa</code> just calls <code>simp</code> followed by <code>assumption</code>.</p> <p>See if you can use the identities above to fill in the missing parts of the proof.</p> <pre><code>example {m n p : \u2115} (nnz : n \u2260 0) (prime_p : p.Prime) : m ^ 2 \u2260 p * n ^ 2 := by\n  intro sqr_eq\n  have nsqr_nez : n ^ 2 \u2260 0 := by simpa\n  have eq1 : Nat.factorization (m ^ 2) p = 2 * m.factorization p := by\n    sorry\n  have eq2 : (p * n ^ 2).factorization p = 2 * n.factorization p + 1 := by\n    sorry\n  have : 2 * m.factorization p % 2 = (2 * n.factorization p + 1) % 2 := by\n    rw [\u2190 eq1, sqr_eq, eq2]\n  rw [add_comm, Nat.add_mul_mod_self_left, Nat.mul_mod_right] at this\n  norm_num at this\n</code></pre> <p>A nice thing about this proof is that it also generalizes. There is nothing special about <code>2</code>; with small changes, the proof shows that whenever we write <code>m^k = r * n^k</code>, the multiplicity of any prime <code>p</code> in <code>r</code> has to be a multiple of <code>k</code>.</p> <p>To use <code>Nat.count_factors_mul_of_pos</code> with <code>r * n^k</code>, we need to know that <code>r</code> is positive. But when <code>r</code> is zero, the theorem below is trivial, and easily proved by the simplifier. So the proof is carried out in cases. The line <code>rcases r with _ | r</code> replaces the goal with two versions: one in which <code>r</code> is replaced by <code>0</code>, and the other in which <code>r</code> is replaces by <code>r + 1</code>. In the second case, we can use the theorem <code>r.succ_ne_zero</code>, which establishes <code>r + 1 \u2260 0</code> (<code>succ</code> stands for successor).</p> <p>Notice also that the line that begins <code>have : npow_nz</code> provides a short proof-term proof of <code>n^k \u2260 0</code>. To understand how it works, try replacing it with a tactic proof, and then think about how the tactics describe the proof term.</p> <p>See if you can fill in the missing parts of the proof below. At the very end, you can use <code>Nat.dvd_sub'</code> and <code>Nat.dvd_mul_right</code> to finish it off.</p> <p>Note that this example does not assume that <code>p</code> is prime, but the conclusion is trivial when <code>p</code> is not prime since <code>r.factorization p</code> is then zero by definition, and the proof works in all cases anyway.</p> <pre><code>example {m n k r : \u2115} (nnz : n \u2260 0) (pow_eq : m ^ k = r * n ^ k) {p : \u2115} :\n    k \u2223 r.factorization p := by\n  rcases r with _ | r\n  \u00b7 simp\n  have npow_nz : n ^ k \u2260 0 := fun npowz \u21a6 nnz (pow_eq_zero npowz)\n  have eq1 : (m ^ k).factorization p = k * m.factorization p := by\n    sorry\n  have eq2 : ((r + 1) * n ^ k).factorization p =\n      k * n.factorization p + (r + 1).factorization p := by\n    sorry\n  have : r.succ.factorization p = k * m.factorization p - k * n.factorization p := by\n    rw [\u2190 eq1, pow_eq, eq2, add_comm, Nat.add_sub_cancel]\n  rw [this]\n  sorry\n</code></pre> <p>There are a number of ways in which we might want to improve on these results. To start with, a proof that the square root of two is irrational should say something about the square root of two, which can be understood as an element of the real or complex numbers. And stating that it is irrational should say something about the rational numbers, namely, that no rational number is equal to it. Moreover, we should extend the theorems in this section to the integers. Although it is mathematically obvious that if we could write the square root of two as a quotient of two integers then we could write it as a quotient of two natural numbers, proving this formally requires some effort.</p> <p>In Mathlib, the natural numbers, the integers, the rationals, the reals, and the complex numbers are represented by separate data types. Restricting attention to the separate domains is often helpful: we will see that it is easy to do induction on the natural numbers, and it is easiest to reason about divisibility of integers when the real numbers are not part of the picture. But having to mediate between the different domains is a headache, one we will have to contend with. We will return to this issue later in this chapter.</p> <p>We should also expect to be able to strengthen the conclusion of the last theorem to say that the number <code>r</code> is a <code>k</code>-th power, since its <code>k</code>-th root is just the product of each prime dividing <code>r</code> raised to its multiplicity in <code>r</code> divided by <code>k</code>. To be able to do that we will need better means for reasoning about products and sums over a finite set, which is also a topic we will return to.</p> <p>In fact, the results in this section are all established in much greater generality in Mathlib, in <code>Data.Real.Irrational</code>. The notion of <code>multiplicity</code> is defined for an arbitrary commutative monoid, and that it takes values in the extended natural numbers <code>enat</code>, which adds the value infinity to the natural numbers. In the next chapter, we will begin to develop the means to appreciate the way that Lean supports this sort of generality.</p>"},{"location":"MIL/C05_Elementary_Number_Theory/S02_Induction_and_Recursion/","title":"S02 Induction and Recursion","text":""},{"location":"MIL/C05_Elementary_Number_Theory/S02_Induction_and_Recursion/#induction-and-recursion","title":"Induction and Recursion","text":"<p>The set of natural numbers \\(\\mathbb{N} = \\{ 0, 1, 2, \\ldots \\}\\) is not only fundamentally important in its own right, but also a plays a central role in the construction of new mathematical objects. Lean's foundation allows us to declare inductive types, which are types generated inductively by a given list of constructors. In Lean, the natural numbers are declared as follows.</p> <pre><code>inductive Nat\n  | zero : Nat\n  | succ (n : Nat) : Nat\n</code></pre> <p>You can find this in the library by writing <code>#check Nat</code> and then using <code>ctrl-click</code> on the identifier <code>Nat</code>. The command specifies that <code>Nat</code> is the datatype generated freely and inductively by the two constructors <code>zero : Nat</code> and <code>succ : Nat \u2192 Nat</code>. Of course, the library introduces notation <code>\u2115</code> and <code>0</code> for <code>nat</code> and <code>zero</code> respectively. (Numerals are translated to binary representations, but we don't have to worry about the details of that now.)</p> <p>What \"freely\" means for the working mathematician is that the type <code>Nat</code> has an element <code>zero</code> and an injective successor function <code>succ</code> whose image does not include <code>zero</code>.</p> <pre><code>example (n : Nat) : n.succ \u2260 Nat.zero :=\n  Nat.succ_ne_zero n\n\nexample (m n : Nat) (h : m.succ = n.succ) : m = n :=\n  Nat.succ.inj h\n</code></pre> <p>What the word \"inductively\" means for the working mathematician is that the natural numbers comes with a principle of proof by induction and a principle of definition by recursion. This section will show you how to use these.</p> <p>Here is an example of a recursive definition of the factorial function.</p> <pre><code>def fac : \u2115 \u2192 \u2115\n  | 0 =&gt; 1\n  | n + 1 =&gt; (n + 1) * fac n\n</code></pre> <p>The syntax takes some getting used to. Notice that there is no <code>:=</code> on the first line. The next two lines provide the base case and inductive step for a recursive definition. These equations hold definitionally, but they can also be used manually by giving the name <code>fac</code> to <code>simp</code> or <code>rw</code>.</p> <pre><code>example : fac 0 = 1 :=\n  rfl\n\nexample : fac 0 = 1 := by\n  rw [fac]\n\nexample : fac 0 = 1 := by\n  simp [fac]\n\nexample (n : \u2115) : fac (n + 1) = (n + 1) * fac n :=\n  rfl\n\nexample (n : \u2115) : fac (n + 1) = (n + 1) * fac n := by\n  rw [fac]\n\nexample (n : \u2115) : fac (n + 1) = (n + 1) * fac n := by\n  simp [fac]\n</code></pre> <p>The factorial function is actually already defined in Mathlib as <code>Nat.factorial</code>. Once again, you can jump to it by typing <code>#check Nat.factorial</code> and using <code>ctrl-click.</code> For illustrative purposes, we will continue using <code>fac</code> in the examples. The annotation <code>@[simp]</code> before the definition of <code>Nat.factorial</code> specifies that the defining equation should be added to the database of identities that the simplifier uses by default.</p> <p>The principle of induction says that we can prove a general statement about the natural numbers by proving that the statement holds of 0 and that whenever it holds of a natural number \\(n\\), it also holds of \\(n + 1\\). The line <code>induction' n with n ih</code> in the proof below therefore results in two goals: in the first we need to prove <code>0 &lt; fac 0</code>, and in the second we have the added assumption <code>ih : 0 &lt; fac n</code> and a required to prove <code>0 &lt; fac (n + 1)</code>. The phrase <code>with n ih</code> serves to name the variable and the assumption for the inductive hypothesis, and you can choose whatever names you want for them.</p> <pre><code>theorem fac_pos (n : \u2115) : 0 &lt; fac n := by\n  induction' n with n ih\n  \u00b7 rw [fac]\n    exact zero_lt_one\n  rw [fac]\n  exact mul_pos n.succ_pos ih\n</code></pre> <p>The <code>induction</code> tactic is smart enough to include hypotheses that depend on the induction variable as part of the induction hypothesis. Step through the next example to see what is going on.</p> <pre><code>theorem dvd_fac {i n : \u2115} (ipos : 0 &lt; i) (ile : i \u2264 n) : i \u2223 fac n := by\n  induction' n with n ih\n  \u00b7 exact absurd ipos (not_lt_of_ge ile)\n  rw [fac]\n  rcases Nat.of_le_succ ile with h | h\n  \u00b7 apply dvd_mul_of_dvd_right (ih h)\n  rw [h]\n  apply dvd_mul_right\n</code></pre> <p>The following example provides a crude lower bound for the factorial function. It turns out to be easier to start with a proof by cases, so that the remainder of the proof starts with the case $n = 1<code>. See if you can complete the argument with a proof by induction using</code>pow_succ<code>or</code>pow_succ'`. BOTH: -/</p> <pre><code>theorem pow_two_le_fac (n : \u2115) : 2 ^ (n - 1) \u2264 fac n := by\n  rcases n with _ | n\n  \u00b7 simp [fac]\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  induction' n with n ih\n  \u00b7 simp [fac]\n  simp at *\n  rw [pow_succ', fac]\n  apply Nat.mul_le_mul _ ih\n  repeat' apply Nat.succ_le_succ\n  apply zero_le\n\n-- BOTH:\n</code></pre> <p>/- TEXT: Induction is often used to prove identities involving finite sums and products. Mathlib defines the expressions <code>Finset.sum s f</code> where <code>s : Finset \u03b1</code> if a finite set of elements of the type <code>\u03b1</code> and <code>f</code> is a function defined on <code>\u03b1</code>. The codomain of <code>f</code> can be any type that supports a commutative, associative addition operation with a zero element. If you import <code>Algebra.BigOperators.Basic</code> and issue the command <code>open BigOperators</code>, you can use the more suggestive notation <code>\u2211 x in s, f x</code>. Of course, there is are an analogous operation and notation for finite products.</p> <p>We will talk about the <code>Finset</code> type and the operations it supports in the next section, and again in a later chapter. For now, we will only make use of <code>Finset.range n</code>, which is the finite set of natural numbers less than <code>n</code>. BOTH: -/ section</p> <pre><code>variable {\u03b1 : Type*} (s : Finset \u2115) (f : \u2115 \u2192 \u2115) (n : \u2115)\n\n-- EXAMPLES:\n#check Finset.sum s f\n#check Finset.prod s f\n\n-- BOTH:\nopen BigOperators\nopen Finset\n\n-- EXAMPLES:\nexample : s.sum f = \u2211 x in s, f x :=\n  rfl\n\nexample : s.prod f = \u220f x in s, f x :=\n  rfl\n\nexample : (range n).sum f = \u2211 x in range n, f x :=\n  rfl\n\nexample : (range n).prod f = \u220f x in range n, f x :=\n  rfl\n</code></pre> <p>/- TEXT: The facts <code>Finset.sum_range_zero</code> and <code>Finset.sum_range_succ</code> provide a recursive description summation up to $n`, and similarly for products. EXAMPLES: -/</p> <pre><code>example (f : \u2115 \u2192 \u2115) : \u2211 x in range 0, f x = 0 :=\n  Finset.sum_range_zero f\n\nexample (f : \u2115 \u2192 \u2115) (n : \u2115) : \u2211 x in range n.succ, f x = \u2211 x in range n, f x + f n :=\n  Finset.sum_range_succ f n\n\nexample (f : \u2115 \u2192 \u2115) : \u220f x in range 0, f x = 1 :=\n  Finset.prod_range_zero f\n\nexample (f : \u2115 \u2192 \u2115) (n : \u2115) : \u220f x in range n.succ, f x = (\u220f x in range n, f x) * f n :=\n  Finset.prod_range_succ f n\n</code></pre> <p>/- TEXT: The first identity in each pair holds definitionally, which is to say, you can replace the proofs by <code>rfl</code>.</p> <p>The following expresses the factorial function that we defined as a product. EXAMPLES: -/</p> <pre><code>example (n : \u2115) : fac n = \u220f i in range n, (i + 1) := by\n  induction' n with n ih\n  \u00b7 rw [fac, prod_range_zero]\n  rw [fac, ih, prod_range_succ, mul_comm]\n</code></pre> <p>/- TEXT: The fact that we include <code>mul_comm</code> as a simplification rule deserves comment. It should seem dangerous to simplify with the identity <code>x * y = y * x</code>, which would ordinarily loop indefinitely. Lean's simplifier is smart enough to recognize that, and applies the rule only in the case where the resulting term has a smaller value in some fixed but arbitrary ordering of the terms. The following example shows that simplifying using the three rules <code>mul_assoc</code>, <code>mul_comm</code>, and <code>mul_left_comm</code> manages to identify products that are the same up to the placement of parentheses and ordering of variables. EXAMPLES: -/</p> <pre><code>example (a b c d e f : \u2115) : a * (b * c * f * (d * e)) = d * (a * f * e) * (c * b) := by\n  simp [mul_assoc, mul_comm, mul_left_comm]\n</code></pre> <p>/- TEXT: Roughly, the rules work by pushing parentheses to the right and then re-ordering the expressions on both sides until they both follow the same canonical order. Simplifying with these rules, and the corresponding rules for addition, is a handy trick.</p> <p>Returning to summation identities, we suggest stepping through the following proof that the sum of the natural numbers up to and including $n<code>is $n (n + 1) / 2</code>. The first step of the proof clears the denominator. This is generally useful when formalizing identities, because calculations with division generally have side conditions. (It is similarly useful to avoid using subtraction on the natural numbers when possible.) EXAMPLES: -/</p> <pre><code>theorem sum_id (n : \u2115) : \u2211 i in range (n + 1), i = n * (n + 1) / 2 := by\n  symm; apply Nat.div_eq_of_eq_mul_right (by norm_num : 0 &lt; 2)\n  induction' n with n ih\n  \u00b7 simp\n  rw [Finset.sum_range_succ, mul_add 2, \u2190 ih]\n  ring\n</code></pre> <p>/- TEXT: We encourage you to prove the analogous identity for sums of squares, and other identities you can find on the web. BOTH: -/</p> <pre><code>theorem sum_sqr (n : \u2115) : \u2211 i in range (n + 1), i ^ 2 = n * (n + 1) * (2 * n + 1) / 6 := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  symm;\n  apply Nat.div_eq_of_eq_mul_right (by norm_num : 0 &lt; 6)\n  induction' n with n ih\n  \u00b7 simp\n  rw [Finset.sum_range_succ, mul_add 6, \u2190 ih]\n  ring\n</code></pre> <p>-- BOTH: end</p> <p>/- TEXT: In Lean's core library, addition and multiplication are themselves defined using recursive definitions, and their fundamental properties are established using induction. If you like thinking about foundational topics like that, you might enjoy working through proofs of the commutativity and associativity of multiplication and addition and the distributivity of multiplication over addition. You can do this on a copy of the natural numbers following the outline below. Notice that we can use the <code>induction</code> tactic with <code>MyNat</code>; Lean is smart enough to know to use the relevant induction principle (which is, of course, the same as that for <code>Nat</code>).</p> <p>We start you off with the commutativity of addition. A good rule of thumb is that because addition and multiplication are defined by recursion on the second argument, it is generally advantageous to do proofs by induction on a variable that occurs in that position. It is a bit tricky to decide which variable to use in the proof of associativity.</p> <p>It can be confusing to write things without the usual notation for zero, one, addition, and multiplication. We will learn how to define such notation later. Working in the namespace <code>MyNat</code> means that we can write <code>zero</code> and <code>succ</code> rather than <code>MyNat.zero</code> and <code>MyNat.succ</code>, and that these interpretations of the names take precedence over others. Outside the namespace, the full name of the <code>add</code> defined below, for example, is <code>MyNat.add</code>.</p> <p>If you find that you really enjoy this sort of thing, try defining truncated subtraction and exponentiation and proving some of their properties as well. Remember that truncated subtraction cuts off at zero. To define that, it is useful to define a predecessor function, <code>pred</code>, that subtracts one from any nonzero number and fixes zero. The function <code>pred</code> can be defined by a simple instance of recursion. BOTH: -/</p> <pre><code>inductive MyNat\n  | zero : MyNat\n  | succ : MyNat \u2192 MyNat\n\nnamespace MyNat\n\ndef add : MyNat \u2192 MyNat \u2192 MyNat\n  | x, zero =&gt; x\n  | x, succ y =&gt; succ (add x y)\n\ndef mul : MyNat \u2192 MyNat \u2192 MyNat\n  | x, zero =&gt; zero\n  | x, succ y =&gt; add (mul x y) x\n\ntheorem zero_add (n : MyNat) : add zero n = n := by\n  induction' n with n ih\n  \u00b7 rfl\n  rw [add, ih]\n\ntheorem succ_add (m n : MyNat) : add (succ m) n = succ (add m n) := by\n  induction' n with n ih\n  \u00b7 rfl\n  rw [add, ih]\n  rfl\n\ntheorem add_comm (m n : MyNat) : add m n = add n m := by\n  induction' n with n ih\n  \u00b7 rw [zero_add]\n    rfl\n  rw [add, succ_add, ih]\n\ntheorem add_assoc (m n k : MyNat) : add (add m n) k = add m (add n k) := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  induction' k with k ih\n  \u00b7 rfl\n  rw [add, ih]\n  rfl\n\n-- BOTH:\ntheorem mul_add (m n k : MyNat) : mul m (add n k) = add (mul m n) (mul m k) := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  induction' k with k ih\n  \u00b7 rfl\n  rw [add, mul, mul, ih, add_assoc]\n\n-- BOTH:\ntheorem zero_mul (n : MyNat) : mul zero n = zero := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  induction' n with n ih\n  \u00b7 rfl\n  rw [mul, ih]\n  rfl\n\n-- BOTH:\ntheorem succ_mul (m n : MyNat) : mul (succ m) n = add (mul m n) n := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  induction' n with n ih\n  \u00b7 rfl\n  rw [mul, mul, ih, add_assoc, add_assoc, add_comm n, succ_add]\n  rfl\n\n-- BOTH:\ntheorem mul_comm (m n : MyNat) : mul m n = mul n m := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  induction' n with n ih\n  \u00b7 rw [zero_mul]\n    rfl\n  rw [mul, ih, succ_mul]\n\n-- BOTH:\nend MyNat\n</code></pre>"},{"location":"MIL/C05_Elementary_Number_Theory/S03_Infinitely_Many_Primes/","title":"S03 Infinitely Many Primes","text":"<p>import Mathlib.Data.Nat.Prime.Basic import MIL.Common</p> <p>open BigOperators</p> <p>namespace C05S03</p> <p>/- TEXT: .. _section_infinitely_many_primes:</p>"},{"location":"MIL/C05_Elementary_Number_Theory/S03_Infinitely_Many_Primes/#infinitely-many-primes","title":"Infinitely Many Primes","text":"<p>Let us continue our exploration of induction and recursion with another mathematical standard: a proof that there are infinitely many primes. One way to formulate this is as the statement that for every natural number :math:<code>n</code>, there is a prime number greater than :math:<code>n</code>. To prove this, let :math:<code>p</code> be any prime factor of :math:<code>n! + 1</code>. If :math:<code>p</code> is less than :math:<code>n</code>, it divides :math:<code>n!</code>. Since it also divides :math:<code>n! + 1</code>, it divides 1, a contradiction. Hence :math:<code>p</code> is greater than :math:<code>n</code>.</p> <p>To formalize that proof, we need to show that any number greater than or equal to 2 has a prime factor. To do that, we will need to show that any natural number that is not equal to 0 or 1 is greater-than or equal to 2. And this brings us to a quirky feature of formalization: it is often trivial statements like this that are among the most annoying to formalize. Here we consider a few ways to do it.</p> <p>To start with, we can use the <code>cases</code> tactic and the fact that the successor function respects the ordering on the natural numbers. BOTH: -/ -- QUOTE: theorem two_le {m : \u2115} (h0 : m \u2260 0) (h1 : m \u2260 1) : 2 \u2264 m := by   cases m; contradiction   case succ m =&gt;     cases m; contradiction     repeat' apply Nat.succ_le_succ     apply zero_le -- QUOTE.</p> <p>/- TEXT: Another strategy is to use the tactic <code>interval_cases</code>, which automatically splits the goal into cases when the variable in question is contained in an interval of natural numbers or integers. Remember that you can hover over it to see its documentation. EXAMPLES: -/ -- QUOTE: example {m : \u2115} (h0 : m \u2260 0) (h1 : m \u2260 1) : 2 \u2264 m := by   by_contra h   push_neg at h   interval_cases m &lt;;&gt; contradiction -- QUOTE.</p> <p>/- TEXT: .. index:: decide, tactics ; decide</p> <p>Recall that the semicolon after <code>interval_cases m</code> means that the next tactic is applied to each of the cases that it generates. Yet another option is to use the tactic, <code>decide</code>, which tries to find a decision procedure to solve the problem. Lean knows that you can decide the truth value of a statement that begins with a bounded quantifier <code>\u2200 x, x &lt; n \u2192 ...</code> or <code>\u2203 x, x &lt; n \u2227 ...</code> by deciding each of the finitely many instances. EXAMPLES: -/ -- QUOTE: example {m : \u2115} (h0 : m \u2260 0) (h1 : m \u2260 1) : 2 \u2264 m := by   by_contra h   push_neg at h   revert h0 h1   revert h m   decide -- QUOTE.</p> <p>/- TEXT: With the theorem <code>two_le</code> in hand, let's start by showing that every natural number greater than two has a prime divisor. Mathlib contains a function <code>Nat.minFac</code> that returns the smallest prime divisor, but for the sake of learning new parts of the library, we'll avoid using it and prove the theorem directly.</p> <p>Here, ordinary induction isn't enough. We want to use strong induction, which allows us to prove that every natural number :math:<code>n</code> has a property :math:<code>P</code> by showing that for every number :math:<code>n</code>, if :math:<code>P</code> holds of all values less than :math:<code>n</code>, it holds at :math:<code>n</code> as well. In Lean, this principle is called <code>Nat.strong_induction_on</code>, and we can use the <code>using</code> keyword to tell the induction tactic to use it. Notice that when we do that, there is no base case; it is subsumed by the general induction step.</p> <p>The argument is simply as follows. Assuming :math:<code>n \u2265 2</code>, if :math:<code>n</code> is prime, we're done. If it isn't, then by one of the characterizations of what it means to be a prime number, it has a nontrivial factor, :math:<code>m</code>, and we can apply the inductive hypothesis to that. Step through the next proof to see how that plays out. The line <code>dsimp at ih</code> simplifies the expression of the inductive hypothesis to make it more readable. The proof still works if you delete that line. BOTH: -/ -- QUOTE: theorem exists_prime_factor {n : Nat} (h : 2 \u2264 n) : \u2203 p : Nat, p.Prime \u2227 p \u2223 n := by   by_cases np : n.Prime   \u00b7 use n, np   induction' n using Nat.strong_induction_on with n ih   rw [Nat.prime_def_lt] at np   push_neg at np   rcases np h with \u27e8m, mltn, mdvdn, mne1\u27e9   have : m \u2260 0 := by     intro mz     rw [mz, zero_dvd_iff] at mdvdn     linarith   have mgt2 : 2 \u2264 m := two_le this mne1   by_cases mp : m.Prime   \u00b7 use m, mp   . rcases ih m mltn mgt2 mp with \u27e8p, pp, pdvd\u27e9     use p, pp     apply pdvd.trans mdvdn -- QUOTE.</p> <p>/- TEXT: We can now prove the following formulation of our theorem. See if you can fill out the sketch. You can use <code>Nat.factorial_pos</code>, <code>Nat.dvd_factorial</code>, and <code>Nat.dvd_sub'</code>. BOTH: -/ -- QUOTE: theorem primes_infinite : \u2200 n, \u2203 p &gt; n, Nat.Prime p := by   intro n   have : 2 \u2264 Nat.factorial (n + 1) + 1 := by /- EXAMPLES:     sorry SOLUTIONS: -/     apply Nat.succ_le_succ     exact Nat.succ_le_of_lt (Nat.factorial_pos _) -- BOTH:   rcases exists_prime_factor this with \u27e8p, pp, pdvd\u27e9   refine' \u27e8p, _, pp\u27e9   show p &gt; n   by_contra ple   push_neg  at ple   have : p \u2223 Nat.factorial (n + 1) := by /- EXAMPLES:     sorry SOLUTIONS: -/     apply Nat.dvd_factorial     apply pp.pos     linarith -- BOTH:   have : p \u2223 1 := by /- EXAMPLES:     sorry SOLUTIONS: -/     convert Nat.dvd_sub' pdvd this     simp -- BOTH:   show False /- EXAMPLES:   sorry SOLUTIONS: -/   have := Nat.le_of_dvd zero_lt_one this   linarith [pp.two_le]</p> <p>-- BOTH: -- QUOTE. /- TEXT: Let's consider a variation of the proof above, where instead of using the factorial function, we suppose that we are given by a finite set :math:<code>\\{ p_1, \\ldots, p_n \\}</code> and we consider a prime factor of :math:<code>\\prod_{i = 1}^n p_i + 1</code>. That prime factor has to be distinct from each :math:<code>p_i</code>, showing that there is no finite set that contains all the prime numbers.</p> <p>Formalizing this argument requires us to reason about finite sets. In Lean, for any type <code>\u03b1</code>, the type <code>Finset \u03b1</code> represents finite sets of elements of type <code>\u03b1</code>. Reasoning about finite sets computationally requires having a procedure to test equality on <code>\u03b1</code>, which is why the snippet below includes the assumption <code>[DecidableEq \u03b1]</code>. For concrete data types like <code>\u2115</code>, <code>\u2124</code>, and <code>\u211a</code>, the assumption is satisfied automatically. When reasoning about the real numbers, it can be satisfied using classical logic and abandoning the computational interpretation.</p> <p>We use the command <code>open Finset</code> to avail ourselves of shorter names for the relevant theorems. Unlike the case with sets, most equivalences involving finsets do not hold definitionally, so they need to be expanded manually using equivalences like <code>Finset.subset_iff</code>, <code>Finset.mem_union</code>, <code>Finset.mem_inter</code>, and <code>Finset.mem_sdiff</code>. The <code>ext</code> tactic can still be used to reduce show that two finite sets are equal by showing that every element of one is an element of the other. BOTH: -/ -- QUOTE: open Finset</p> <p>-- EXAMPLES: section variable {\u03b1 : Type*} [DecidableEq \u03b1] (r s t : Finset \u03b1)</p> <p>example : r \u2229 (s \u222a t) \u2286 r \u2229 s \u222a r \u2229 t := by   rw [subset_iff]   intro x   rw [mem_inter, mem_union, mem_union, mem_inter, mem_inter]   tauto</p> <p>example : r \u2229 (s \u222a t) \u2286 r \u2229 s \u222a r \u2229 t := by   simp [subset_iff]   intro x   tauto</p> <p>example : r \u2229 s \u222a r \u2229 t \u2286 r \u2229 (s \u222a t) := by   simp [subset_iff]   intro x   tauto</p> <p>example : r \u2229 s \u222a r \u2229 t = r \u2229 (s \u222a t) := by   ext x   simp   tauto</p> <p>end -- QUOTE.</p> <p>/- TEXT: We have used a new trick: the <code>tauto</code> tactic (and a strengthened version, <code>tauto!</code>, which uses classical logic) can be used to dispense with propositional tautologies. See if you can use these methods to prove the two examples below. BOTH: -/ section variable {\u03b1 : Type*} [DecidableEq \u03b1] (r s t : Finset \u03b1)</p> <p>-- QUOTE: example : (r \u222a s) \u2229 (r \u222a t) = r \u222a s \u2229 t := by /- EXAMPLES:   sorry SOLUTIONS: -/   ext x   rw [mem_inter, mem_union, mem_union, mem_union, mem_inter]   tauto</p> <p>example : (r \u222a s) \u2229 (r \u222a t) = r \u222a s \u2229 t := by   ext x   simp   tauto</p> <p>-- BOTH: example : (r  s)  t = r  (s \u222a t) := by /- EXAMPLES:   sorry SOLUTIONS: -/   ext x   rw [mem_sdiff, mem_sdiff, mem_sdiff, mem_union]   tauto</p> <p>example : (r  s)  t = r  (s \u222a t) := by   ext x   simp   tauto -- QUOTE. -- BOTH:</p> <p>end</p> <p>/- TEXT: The theorem <code>Finset.dvd_prod_of_mem</code> tells us that if an <code>n</code> is an element of a finite set <code>s</code>, then <code>n</code> divides <code>\u220f i in s, i</code>. EXAMPLES: -/ -- QUOTE: example (s : Finset \u2115) (n : \u2115) (h : n \u2208 s) : n \u2223 \u220f i in s, i :=   Finset.dvd_prod_of_mem _ h -- QUOTE.</p> <p>/- TEXT: We also need to know that the converse holds in the case where <code>n</code> is prime and <code>s</code> is a set of primes. To show that, we need the following lemma, which you should be able to prove using the theorem <code>Nat.Prime.eq_one_or_self_of_dvd</code>. BOTH: -/ -- QUOTE: theorem root.Nat.Prime.eq_of_dvd_of_prime {p q : \u2115}       (prime_p : Nat.Prime p) (prime_q : Nat.Prime q) (h : p \u2223 q) :     p = q := by /- EXAMPLES:   sorry SOLUTIONS: -/   cases prime_q.eq_one_or_self_of_dvd _ h   \u00b7 linarith [prime_p.two_le]   assumption -- QUOTE. -- BOTH:</p> <p>/- TEXT: We can use this lemma to show that if a prime <code>p</code> divides a product of a finite set of primes, then it divides one of them. Mathlib provides a useful principle of induction on finite sets: to show that a property holds of an arbitrary finite set <code>s</code>, show that it holds of the empty set, and show that it is preserved when we add a single new element <code>a \u2209 s</code>. The principle is known as <code>Finset.induction_on</code>. When we tell the induction tactic to use it, we can also specify the names <code>a</code> and <code>s</code>, the name for the assumption <code>a \u2209 s</code> in the inductive step, and the name of the inductive hypothesis. The expression <code>Finset.insert a s</code> denotes the union of <code>s</code> with the singleton <code>a</code>. The identities <code>Finset.prod_empty</code> and <code>Finset.prod_insert</code> then provide the relevant rewrite rules for the product. In the proof below, the first <code>simp</code> applies <code>Finset.prod_empty</code>. Step through the beginning of the proof to see the induction unfold, and then finish it off. BOTH: -/ -- QUOTE: theorem mem_of_dvd_prod_primes {s : Finset \u2115} {p : \u2115} (prime_p : p.Prime) :     (\u2200 n \u2208 s, Nat.Prime n) \u2192 (p \u2223 \u220f n in s, n) \u2192 p \u2208 s := by   intro h\u2080 h\u2081   induction' s using Finset.induction_on with a s ans ih   \u00b7 simp at h\u2081     linarith [prime_p.two_le]   simp [Finset.prod_insert ans, prime_p.dvd_mul] at h\u2080 h\u2081   rw [mem_insert] /- EXAMPLES:   sorry SOLUTIONS: -/   rcases h\u2081 with h\u2081 | h\u2081   \u00b7 left     exact prime_p.eq_of_dvd_of_prime h\u2080.1 h\u2081   right   exact ih h\u2080.2 h\u2081</p> <p>-- BOTH: -- QUOTE. /- TEXT: We need one last property of finite sets. Given an element <code>s : Set \u03b1</code> and a predicate <code>P</code> on <code>\u03b1</code>, in  :numref:<code>Chapter %s &lt;sets_and_functions&gt;</code> we wrote <code>{ x \u2208 s | P x }</code> for the set of elements of <code>s</code> that satisfy <code>P</code>. Given <code>s : Finset \u03b1</code>, the analogous notion is written <code>s.filter P</code>. EXAMPLES: -/ -- QUOTE: example (s : Finset \u2115) (x : \u2115) : x \u2208 s.filter Nat.Prime \u2194 x \u2208 s \u2227 x.Prime :=   mem_filter -- QUOTE.</p> <p>/- TEXT: We now prove an alternative formulation of the statement that there are infinitely many primes, namely, that given any <code>s : Finset \u2115</code>, there is a prime <code>p</code> that is not an element of <code>s</code>. Aiming for a contradiction, we assume that all the primes are in <code>s</code>, and then cut down to a set <code>s'</code> that contains all and only the primes. Taking the product of that set, adding one, and finding a prime factor of the result leads to the contradiction we are looking for. See if you can complete the sketch below. You can use <code>Finset.prod_pos</code> in the proof of the first <code>have</code>. BOTH: -/ -- QUOTE: theorem primes_infinite' : \u2200 s : Finset Nat, \u2203 p, Nat.Prime p \u2227 p \u2209 s := by   intro s   by_contra h   push_neg  at h   set s' := s.filter Nat.Prime with s'_def   have mem_s' : \u2200 {n : \u2115}, n \u2208 s' \u2194 n.Prime := by     intro n     simp [s'_def]     apply h   have : 2 \u2264 (\u220f i in s', i) + 1 := by /- EXAMPLES:     sorry SOLUTIONS: -/     apply Nat.succ_le_succ     apply Nat.succ_le_of_lt     apply Finset.prod_pos     intro n ns'     apply (mem_s'.mp ns').pos -- BOTH:   rcases exists_prime_factor this with \u27e8p, pp, pdvd\u27e9   have : p \u2223 \u220f i in s', i := by /- EXAMPLES:     sorry SOLUTIONS: -/     apply dvd_prod_of_mem     rw [mem_s']     apply pp -- BOTH:   have : p \u2223 1 := by     convert Nat.dvd_sub' pdvd this     simp   show False /- EXAMPLES:   sorry SOLUTIONS: -/   have := Nat.le_of_dvd zero_lt_one this   linarith [pp.two_le]</p> <p>-- BOTH: -- QUOTE. /- TEXT: We have thus seen two ways of saying that there are infinitely many primes: saying that they are not bounded by any <code>n</code>, and saying that they are not contained in any finite set <code>s</code>. The two proofs below show that these formulations are equivalent. In the second, in order to form <code>s.filter Q</code>, we have to assume that there is a procedure for deciding whether or not <code>Q</code> holds. Lean knows that there is a procedure for <code>Nat.Prime</code>. In general, if we use classical logic by writing <code>open Classical</code>, we can dispense with the assumption.</p> <p>In Mathlib, <code>Finset.sup s f</code> denotes the supremum of the values of <code>f x</code> as <code>x</code> ranges over <code>s</code>, returning <code>0</code> in the case where <code>s</code> is empty and the codomain of <code>f</code> is <code>\u2115</code>. In the first proof, we use <code>s.sup id</code>, where <code>id</code> is the identity function, to refer to the maximum value in <code>s</code>. BOTH: -/ -- QUOTE: theorem bounded_of_ex_finset (Q : \u2115 \u2192 Prop) :     (\u2203 s : Finset \u2115, \u2200 k, Q k \u2192 k \u2208 s) \u2192 \u2203 n, \u2200 k, Q k \u2192 k &lt; n := by   rintro \u27e8s, hs\u27e9   use s.sup id + 1   intro k Qk   apply Nat.lt_succ_of_le   show id k \u2264 s.sup id   apply le_sup (hs k Qk)</p> <p>theorem ex_finset_of_bounded (Q : \u2115 \u2192 Prop) [DecidablePred Q] :     (\u2203 n, \u2200 k, Q k \u2192 k \u2264 n) \u2192 \u2203 s : Finset \u2115, \u2200 k, Q k \u2194 k \u2208 s := by   rintro \u27e8n, hn\u27e9   use (range (n + 1)).filter Q   intro k   simp [Nat.lt_succ_iff]   exact hn k -- QUOTE.</p> <p>/- TEXT: A small variation on our second proof that there are infinitely many primes shows that there are infinitely many primes congruent to 3 modulo 4. The argument goes as follows. First, notice that if the product of two numbers :math:<code>m</code> and :math:<code>n</code> is equal to 3 modulo 4, then one of the two numbers is congruent to three modulo 4. After all, both have to be odd, and if they are both congruent to 1 modulo 4, so is their product. We can use this observation to show that if some number greater than 2 is congruent to 3 modulo 4, then that number has a prime divisor that is also congruent to 3 modulo 4.</p> <p>Now suppose there are only finitely many prime numbers congruent to 3 modulo 4, say, :math:<code>p_1, \\ldots, p_k</code>. Without loss of generality, we can assume that :math:<code>p_1 = 3</code>. Consider the product :math:<code>4 \\prod_{i = 2}^k p_i + 3</code>. It is easy to see that this is congruent to 3 modulo 4, so it has a prime factor :math:<code>p</code> congruent to 3 modulo 4. It can't be the case that :math:<code>p = 3</code>; since :math:<code>p</code> divides :math:<code>4 \\prod_{i = 2}^k p_i + 3</code>, if :math:<code>p</code> were equal to 3 then it would also divide :math:<code>\\prod_{i = 2}^k p_i</code>, which implies that :math:<code>p</code> is equal to one of the :math:<code>p_i</code> for :math:<code>i = 2, \\ldots, k</code>; and we have excluded 3 from this list. So :math:<code>p</code> has to be one of the other elements :math:<code>p_i</code>. But in that case, :math:<code>p</code> divides :math:<code>4 \\prod_{i = 2}^k p_i</code> and hence 3, which contradicts the fact that it is not 3.</p> <p>In Lean, the notation <code>n % m</code>, read \"<code>n</code> modulo <code>m</code>,\" denotes the remainder of the division of <code>n</code> by <code>m</code>. EXAMPLES: -/ -- QUOTE: example : 27 % 4 = 3 := by norm_num -- QUOTE.</p> <p>/- TEXT: We can then render the statement \"<code>n</code> is congruent to 3 modulo 4\" as <code>n % 4 = 3</code>. The following example and theorems sum up the facts about this function that we will need to use below. The first named theorem is another illustration of reasoning by a small number of cases. In the second named theorem, remember that the semicolon means that the subsequent tactic block is applied to all the goals created by the preceding tactic. EXAMPLES: -/ -- QUOTE: example (n : \u2115) : (4 * n + 3) % 4 = 3 := by   rw [add_comm, Nat.add_mul_mod_self_left]</p> <p>-- BOTH: theorem mod_4_eq_3_or_mod_4_eq_3 {m n : \u2115} (h : m * n % 4 = 3) : m % 4 = 3 \u2228 n % 4 = 3 := by   revert h   rw [Nat.mul_mod]   have : m % 4 &lt; 4 := Nat.mod_lt m (by norm_num)   interval_cases m % 4 &lt;;&gt; simp [-Nat.mul_mod_mod]   have : n % 4 &lt; 4 := Nat.mod_lt n (by norm_num)   interval_cases n % 4 &lt;;&gt; simp</p> <p>theorem two_le_of_mod_4_eq_3 {n : \u2115} (h : n % 4 = 3) : 2 \u2264 n := by   apply two_le &lt;;&gt;     \u00b7 intro neq       rw [neq] at h       norm_num at h -- QUOTE.</p> <p>/- TEXT: We will also need the following fact, which says that if <code>m</code> is a nontrivial divisor of <code>n</code>, then so is <code>n / m</code>. See if you can complete the proof using <code>Nat.div_dvd_of_dvd</code> and <code>Nat.div_lt_self</code>. BOTH: -/ -- QUOTE: theorem aux {m n : \u2115} (h\u2080 : m \u2223 n) (h\u2081 : 2 \u2264 m) (h\u2082 : m &lt; n) : n / m \u2223 n \u2227 n / m &lt; n := by /- EXAMPLES:   sorry SOLUTIONS: -/   constructor   \u00b7 exact Nat.div_dvd_of_dvd h\u2080   exact Nat.div_lt_self (lt_of_le_of_lt (zero_le _) h\u2082) h\u2081 -- QUOTE.</p> <p>-- BOTH: /- TEXT: Now put all the pieces together to prove that any number congruent to 3 modulo 4 has a prime divisor with that same property. BOTH: -/ -- QUOTE: theorem exists_prime_factor_mod_4_eq_3 {n : Nat} (h : n % 4 = 3) :     \u2203 p : Nat, p.Prime \u2227 p \u2223 n \u2227 p % 4 = 3 := by   by_cases np : n.Prime   \u00b7 use n   induction' n using Nat.strong_induction_on with n ih   rw [Nat.prime_def_lt] at np   push_neg  at np   rcases np (two_le_of_mod_4_eq_3 h) with \u27e8m, mltn, mdvdn, mne1\u27e9   have mge2 : 2 \u2264 m := by     apply two_le _ mne1     intro mz     rw [mz, zero_dvd_iff] at mdvdn     linarith   have neq : m * (n / m) = n := Nat.mul_div_cancel' mdvdn   have : m % 4 = 3 \u2228 n / m % 4 = 3 := by     apply mod_4_eq_3_or_mod_4_eq_3     rw [neq, h]   rcases this with h1 | h1 /- EXAMPLES:   . sorry   . sorry SOLUTIONS: -/   \u00b7 by_cases mp : m.Prime     \u00b7 use m     rcases ih m mltn h1 mp with \u27e8p, pp, pdvd, p4eq\u27e9     use p     exact \u27e8pp, pdvd.trans mdvdn, p4eq\u27e9   obtain \u27e8nmdvdn, nmltn\u27e9 := aux mdvdn mge2 mltn   by_cases nmp : (n / m).Prime   \u00b7 use n / m   rcases ih (n / m) nmltn h1 nmp with \u27e8p, pp, pdvd, p4eq\u27e9   use p   exact \u27e8pp, pdvd.trans nmdvdn, p4eq\u27e9</p> <p>-- BOTH: -- QUOTE. /- TEXT: We are in the home stretch. Given a set <code>s</code> of prime numbers, we need to talk about the result of removing 3 from that set, if it is present. The function <code>Finset.erase</code> handles that. EXAMPLES: -/ -- QUOTE: example (m n : \u2115) (s : Finset \u2115) (h : m \u2208 erase s n) : m \u2260 n \u2227 m \u2208 s := by   rwa [mem_erase] at h</p> <p>example (m n : \u2115) (s : Finset \u2115) (h : m \u2208 erase s n) : m \u2260 n \u2227 m \u2208 s := by   simp at h   assumption -- QUOTE.</p> <p>/- TEXT: We are now ready to prove that there are infinitely many primes congruent to 3 modulo 4. Fill in the missing parts below. Our solution uses <code>Nat.dvd_add_iff_left</code> and <code>Nat.dvd_sub'</code> along the way. BOTH: -/ -- QUOTE: theorem primes_mod_4_eq_3_infinite : \u2200 n, \u2203 p &gt; n, Nat.Prime p \u2227 p % 4 = 3 := by   by_contra h   push_neg  at h   rcases h with \u27e8n, hn\u27e9   have : \u2203 s : Finset Nat, \u2200 p : \u2115, p.Prime \u2227 p % 4 = 3 \u2194 p \u2208 s := by     apply ex_finset_of_bounded     use n     contrapose! hn     rcases hn with \u27e8p, \u27e8pp, p4\u27e9, pltn\u27e9     exact \u27e8p, pltn, pp, p4\u27e9   rcases this with \u27e8s, hs\u27e9   have h\u2081 : ((4 * \u220f i in erase s 3, i) + 3) % 4 = 3 := by /- EXAMPLES:     sorry SOLUTIONS: -/     rw [add_comm, Nat.add_mul_mod_self_left] -- BOTH:   rcases exists_prime_factor_mod_4_eq_3 h\u2081 with \u27e8p, pp, pdvd, p4eq\u27e9   have ps : p \u2208 s := by /- EXAMPLES:     sorry SOLUTIONS: -/     rw [\u2190 hs p]     exact \u27e8pp, p4eq\u27e9 -- BOTH:   have pne3 : p \u2260 3 := by /- EXAMPLES:     sorry SOLUTIONS: -/     intro peq     rw [peq, \u2190 Nat.dvd_add_iff_left (dvd_refl 3)] at pdvd     rw [Nat.prime_three.dvd_mul] at pdvd     norm_num at pdvd     have : 3 \u2208 s.erase 3 := by       apply mem_of_dvd_prod_primes Nat.prime_three _ pdvd       intro n       simp [\u2190 hs n]       tauto     simp at this -- BOTH:   have : p \u2223 4 * \u220f i in erase s 3, i := by /- EXAMPLES:     sorry SOLUTIONS: -/     apply dvd_trans _ (dvd_mul_left _ _)     apply dvd_prod_of_mem     simp     constructor &lt;;&gt; assumption -- BOTH:   have : p \u2223 3 := by /- EXAMPLES:     sorry SOLUTIONS: -/     convert Nat.dvd_sub' pdvd this     simp -- BOTH:   have : p = 3 := by /- EXAMPLES:     sorry SOLUTIONS: -/     apply pp.eq_of_dvd_of_prime Nat.prime_three this -- BOTH:   contradiction -- QUOTE.</p> <p>/- TEXT: If you managed to complete the proof, congratulations! This has been a serious feat of formalization. TEXT. -/ -- OMIT: /- Later: o fibonacci numbers o binomial coefficients</p> <p>(The former is a good example of having more than one base case.)</p> <p>TODO: mention <code>local attribute</code> at some point. -/</p>"},{"location":"MIL/C06_Structures/S01_Structures/","title":"S01 Structures","text":"<p>import MIL.Common import Mathlib.Algebra.BigOperators.Ring import Mathlib.Data.Real.Basic</p> <p>namespace C06S01 noncomputable section</p> <p>/- TEXT: .. _section_structures:</p>"},{"location":"MIL/C06_Structures/S01_Structures/#defining-structures","title":"Defining structures","text":"<p>In the broadest sense of the term, a structure is a specification of a collection of data, possibly with constraints that the data is required to satisfy. An instance of the structure is a particular bundle of data satisfying the constraints. For example, we can specify that a point is a tuple of three real numbers: BOTH: -/ -- QUOTE: @[ext] structure Point where   x : \u211d   y : \u211d   z : \u211d -- QUOTE.</p> <p>/- TEXT: The <code>@[ext]</code> annotation tells Lean to automatically generate theorems that can be used to prove that two instances of a structure are equal when their components are equal, a property known as extensionality. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-pointext","title":"check Point.ext","text":"<p>example (a b : Point) (hx : a.x = b.x) (hy : a.y = b.y) (hz : a.z = b.z) : a = b := by   ext   repeat' assumption -- QUOTE.</p> <p>/- TEXT: We can then define particular instances of the <code>Point</code> structure. Lean provides multiple ways of doing that. EXAMPLES: -/ -- QUOTE: def myPoint1 : Point where   x := 2   y := -1   z := 4</p> <p>def myPoint2 : Point :=   \u27e82, -1, 4\u27e9</p> <p>def myPoint3 :=   Point.mk 2 (-1) 4 -- QUOTE.</p> <p>/- TEXT: ..   Because Lean knows that the expected type of   <code>myPoint1</code> is a <code>Point</code>, you can start the definition by   writing an underscore, <code>_</code>. Clicking on the light bulb   that appears nearby in VS Code will then   give you the option of inserting a template definition   with the field names listed for you.</p> <p>In the first example, the fields of the structure are named explicitly. The function <code>Point.mk</code> referred to in the definition of <code>myPoint3</code> is known as the constructor for the <code>Point</code> structure, because it serves to construct elements. You can specify a different name if you want, like <code>build</code>. EXAMPLES: -/ -- QUOTE: structure Point' where build ::   x : \u211d   y : \u211d   z : \u211d</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-pointbuild-2-1-4","title":"check Point'.build 2 (-1) 4","text":"<p>-- QUOTE.</p> <p>/- TEXT: The next two examples show how to define functions on structures. Whereas the second example makes the <code>Point.mk</code> constructor explicit, the first example uses an anonymous constructor for brevity. Lean can infer the relevant constructor from the indicated type of <code>add</code>. It is conventional to put definitions and theorems associated with a structure like <code>Point</code> in a namespace with the same name. In the example below, because we have opened the <code>Point</code> namespace, the full name of <code>add</code> is <code>Point.add</code>. When the namespace is not open, we have to use the full name. But remember that it is often convenient to use anonymous projection notation, which allows us to write <code>a.add b</code> instead of <code>Point.add a b</code>. Lean interprets the former as the latter because <code>a</code> has type <code>Point</code>. BOTH: -/ -- QUOTE: namespace Point</p> <p>def add (a b : Point) : Point :=   \u27e8a.x + b.x, a.y + b.y, a.z + b.z\u27e9</p> <p>-- EXAMPLES: def add' (a b : Point) : Point where   x := a.x + b.x   y := a.y + b.y   z := a.z + b.z</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-add-mypoint1-mypoint2","title":"check add myPoint1 myPoint2","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-mypoint1add-mypoint2","title":"check myPoint1.add myPoint2","text":"<p>end Point</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-pointadd-mypoint1-mypoint2","title":"check Point.add myPoint1 myPoint2","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-mypoint1add-mypoint2_1","title":"check myPoint1.add myPoint2","text":"<p>-- QUOTE.</p> <p>/- TEXT: Below we will continue to put definitions in the relevant namespace, but we will leave the namespacing commands out of the quoted snippets. To prove properties of the addition function, we can use <code>rw</code> to expand the definition and <code>ext</code> to reduce an equation between two elements of the structure to equations between the components. Below we use the <code>protected</code> keyword so that the name of the theorem is <code>Point.add_comm</code>, even when the namespace is open. This is helpful when we want to avoid ambiguity with a generic theorem like <code>add_comm</code>. EXAMPLES: -/ namespace Point</p> <p>-- QUOTE: protected theorem add_comm (a b : Point) : add a b = add b a := by   rw [add, add]   ext &lt;;&gt; dsimp   repeat' apply add_comm</p> <p>example (a b : Point) : add a b = add b a := by simp [add, add_comm] -- QUOTE.</p> <p>/- TEXT: Because Lean can unfold definitions and simplify projections internally, sometimes the equations we want hold definitionally. EXAMPLES: -/ -- QUOTE: theorem add_x (a b : Point) : (a.add b).x = a.x + b.x :=   rfl -- QUOTE.</p> <p>/- TEXT: It is also possible to define functions on structures using pattern matching, in a manner similar to the way we defined recursive functions in :numref:<code>section_induction_and_recursion</code>. The definitions <code>addAlt</code> and <code>addAlt'</code> below are essentially the same; the only difference is that we use anonymous constructor notation in the second. Although it is sometimes convenient to define functions this way, and structural eta-reduction makes this alternative definitionally equivalent, it can make things less convenient in later proofs. In particular, <code>rw [addAlt]</code> leaves us with a messier goal view containing a <code>match</code> statement. EXAMPLES: -/ -- QUOTE: def addAlt : Point \u2192 Point \u2192 Point   | Point.mk x\u2081 y\u2081 z\u2081, Point.mk x\u2082 y\u2082 z\u2082 =&gt; \u27e8x\u2081 + x\u2082, y\u2081 + y\u2082, z\u2081 + z\u2082\u27e9</p> <p>def addAlt' : Point \u2192 Point \u2192 Point   | \u27e8x\u2081, y\u2081, z\u2081\u27e9, \u27e8x\u2082, y\u2082, z\u2082\u27e9 =&gt; \u27e8x\u2081 + x\u2082, y\u2081 + y\u2082, z\u2081 + z\u2082\u27e9</p> <p>theorem addAlt_x (a b : Point) : (a.addAlt b).x = a.x + b.x := by   rfl</p> <p>theorem addAlt_comm (a b : Point) : addAlt a b = addAlt b a := by   rw [addAlt, addAlt]   -- the same proof still works, but the goal view here is harder to read   ext &lt;;&gt; dsimp   repeat' apply add_comm -- QUOTE.</p> <p>/- TEXT: Mathematical constructions often involve taking apart bundled information and putting it together again in different ways. It therefore makes sense that Lean and Mathlib offer so many ways of doing this efficiently. As an exercise, try proving that <code>Point.add</code> is associative. Then define scalar multiplication for a point and show that it distributes over addition. BOTH: -/ -- QUOTE: protected theorem add_assoc (a b c : Point) : (a.add b).add c = a.add (b.add c) := by /- EXAMPLES:   sorry SOLUTIONS: -/   simp [add, add_assoc] -- BOTH:</p> <p>def smul (r : \u211d) (a : Point) : Point := /- EXAMPLES:   sorry SOLUTIONS: -/   \u27e8r * a.x, r * a.y, r * a.z\u27e9 -- BOTH:</p> <p>theorem smul_distrib (r : \u211d) (a b : Point) :     (smul r a).add (smul r b) = smul r (a.add b) := by /- EXAMPLES:   sorry SOLUTIONS: -/   simp [add, smul, mul_add] -- BOTH: -- QUOTE.</p> <p>end Point</p> <p>/- TEXT: Using structures is only the first step on the road to algebraic abstraction. We don't yet have a way to link <code>Point.add</code> to the generic <code>+</code> symbol, or to connect <code>Point.add_comm</code> and <code>Point.add_assoc</code> to the generic <code>add_comm</code> and <code>add_assoc</code> theorems. These tasks belong to the algebraic aspect of using structures, and we will explain how to carry them out in the next section. For now, just think of a structure as a way of bundling together objects and information.</p> <p>It is especially useful that a structure can specify not only data types but also constraints that the data must satisfy. In Lean, the latter are represented as fields of type <code>Prop</code>. For example, the standard 2-simplex is defined to be the set of points :math:<code>(x, y, z)</code> satisfying :math:<code>x \u2265 0</code>, :math:<code>y \u2265 0</code>, :math:<code>z \u2265 0</code>, and :math:<code>x + y + z = 1</code>. If you are not familiar with the notion, you should draw a picture, and convince yourself that this set is the equilateral triangle in three-space with vertices :math:<code>(1, 0, 0)</code>, :math:<code>(0, 1, 0)</code>, and :math:<code>(0, 0, 1)</code>, together with its interior. We can represent it in Lean as follows: BOTH: -/ -- QUOTE: structure StandardTwoSimplex where   x : \u211d   y : \u211d   z : \u211d   x_nonneg : 0 \u2264 x   y_nonneg : 0 \u2264 y   z_nonneg : 0 \u2264 z   sum_eq : x + y + z = 1 -- QUOTE.</p> <p>/- TEXT: Notice that the last four fields refer to <code>x</code>, <code>y</code>, and <code>z</code>, that is, the first three fields. We can define a map from the two-simplex to itself that swaps <code>x</code> and <code>y</code>: BOTH: -/ namespace StandardTwoSimplex</p> <p>-- EXAMPLES: -- QUOTE: def swapXy (a : StandardTwoSimplex) : StandardTwoSimplex     where   x := a.y   y := a.x   z := a.z   x_nonneg := a.y_nonneg   y_nonneg := a.x_nonneg   z_nonneg := a.z_nonneg   sum_eq := by rw [add_comm a.y a.x, a.sum_eq] -- QUOTE.</p> <p>-- OMIT: (TODO) add a link when we have a good explanation of noncomputable section. /- TEXT: More interestingly, we can compute the midpoint of two points on the simplex. We have added the phrase <code>noncomputable section</code> at the beginning of this file in order to use division on the real numbers. BOTH: -/ -- QUOTE: noncomputable section</p> <p>-- EXAMPLES: def midpoint (a b : StandardTwoSimplex) : StandardTwoSimplex     where   x := (a.x + b.x) / 2   y := (a.y + b.y) / 2   z := (a.z + b.z) / 2   x_nonneg := div_nonneg (add_nonneg a.x_nonneg b.x_nonneg) (by norm_num)   y_nonneg := div_nonneg (add_nonneg a.y_nonneg b.y_nonneg) (by norm_num)   z_nonneg := div_nonneg (add_nonneg a.z_nonneg b.z_nonneg) (by norm_num)   sum_eq := by field_simp; linarith [a.sum_eq, b.sum_eq] -- QUOTE.</p> <p>/- TEXT: Here we have established <code>x_nonneg</code>, <code>y_nonneg</code>, and <code>z_nonneg</code> with concise proof terms, but establish <code>sum_eq</code> in tactic mode, using <code>by</code>.</p> <p>Given a parameter :math:<code>\\lambda</code> satisfying :math:<code>0 \\le \\lambda \\le 1</code>, we can take the weighted average :math:<code>\\lambda a + (1 - \\lambda) b</code> of two points :math:<code>a</code> and :math:<code>b</code> in the standard 2-simplex. We challenge you to define that function, in analogy to the <code>midpoint</code> function above. BOTH: -/ -- QUOTE: def weightedAverage (lambda : Real) (lambda_nonneg : 0 \u2264 lambda) (lambda_le : lambda \u2264 1) /- EXAMPLES:     (a b : StandardTwoSimplex) : StandardTwoSimplex :=   sorry SOLUTIONS: -/   (a b : StandardTwoSimplex) : StandardTwoSimplex where   x := lambda * a.x + (1 - lambda) * b.x   y := lambda * a.y + (1 - lambda) * b.y   z := lambda * a.z + (1 - lambda) * b.z   x_nonneg := add_nonneg (mul_nonneg lambda_nonneg a.x_nonneg) (mul_nonneg (by linarith) b.x_nonneg)   y_nonneg := add_nonneg (mul_nonneg lambda_nonneg a.y_nonneg) (mul_nonneg (by linarith) b.y_nonneg)   z_nonneg := add_nonneg (mul_nonneg lambda_nonneg a.z_nonneg) (mul_nonneg (by linarith) b.z_nonneg)   sum_eq := by     trans (a.x + a.y + a.z) * lambda + (b.x + b.y + b.z) * (1 - lambda)     \u00b7 ring     simp [a.sum_eq, b.sum_eq] -- QUOTE. -- BOTH:</p> <p>end</p> <p>end StandardTwoSimplex</p> <p>/- TEXT: Structures can depend on parameters. For example, we can generalize the standard 2-simplex to the standard :math:<code>n</code>-simplex for any :math:<code>n</code>. At this stage, you don't have to know anything about the type <code>Fin n</code> except that it has :math:<code>n</code> elements, and that Lean knows how to sum over it. BOTH: -/ -- QUOTE: open BigOperators</p> <p>structure StandardSimplex (n : \u2115) where   V : Fin n \u2192 \u211d   NonNeg : \u2200 i : Fin n, 0 \u2264 V i   sum_eq_one : (\u2211 i, V i) = 1</p> <p>namespace StandardSimplex</p> <p>def midpoint (n : \u2115) (a b : StandardSimplex n) : StandardSimplex n     where   V i := (a.V i + b.V i) / 2   NonNeg := by     intro i     apply div_nonneg     \u00b7 linarith [a.NonNeg i, b.NonNeg i]     norm_num   sum_eq_one := by     simp [div_eq_mul_inv, \u2190 Finset.sum_mul, Finset.sum_add_distrib,       a.sum_eq_one, b.sum_eq_one]     field_simp</p> <p>end StandardSimplex -- QUOTE.</p> <p>/- TEXT: As an exercise, see if you can define the weighted average of two points in the standard :math:<code>n</code>-simplex. You can use <code>Finset.sum_add_distrib</code> and <code>Finset.mul_sum</code> to manipulate the relevant sums. SOLUTIONS: -/ namespace StandardSimplex</p> <p>def weightedAverage {n : \u2115} (lambda : Real) (lambda_nonneg : 0 \u2264 lambda) (lambda_le : lambda \u2264 1)     (a b : StandardSimplex n) : StandardSimplex n     where   V i := lambda * a.V i + (1 - lambda) * b.V i   NonNeg i :=     add_nonneg (mul_nonneg lambda_nonneg (a.NonNeg i)) (mul_nonneg (by linarith) (b.NonNeg i))   sum_eq_one := by     trans (lambda * \u2211 i, a.V i) + (1 - lambda) * \u2211 i, b.V i     \u00b7 rw [Finset.sum_add_distrib, Finset.mul_sum, Finset.mul_sum]     simp [a.sum_eq_one, b.sum_eq_one]</p> <p>end StandardSimplex</p> <p>/- TEXT: We have seen that structures can be used to bundle together data and properties. Interestingly, they can also be used to bundle together properties without the data. For example, the next structure, <code>IsLinear</code>, bundles together the two components of linearity. EXAMPLES: -/ -- QUOTE: structure IsLinear (f : \u211d \u2192 \u211d) where   is_additive : \u2200 x y, f (x + y) = f x + f y   preserves_mul : \u2200 x c, f (c * x) = c * f x</p> <p>section variable (f : \u211d \u2192 \u211d) (linf : IsLinear f)</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-linfis_additive","title":"check linf.is_additive","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-linfpreserves_mul","title":"check linf.preserves_mul","text":"<p>end -- QUOTE.</p> <p>/- TEXT: It is worth pointing out that structures are not the only way to bundle together data. The <code>Point</code> data structure can be defined using the generic type product, and <code>IsLinear</code> can be defined with a simple <code>and</code>. EXAMPLES: -/ -- QUOTE: def Point'' :=   \u211d \u00d7 \u211d \u00d7 \u211d</p> <p>def IsLinear' (f : \u211d \u2192 \u211d) :=   (\u2200 x y, f (x + y) = f x + f y) \u2227 \u2200 x c, f (c * x) = c * f x -- QUOTE.</p> <p>/- TEXT: Generic type constructions can even be used in place of structures with dependencies between their components. For example, the subtype construction combines a piece of data with a property. You can think of the type <code>PReal</code> in the next example as being the type of positive real numbers. Any <code>x : PReal</code> has two components: the value, and the property of being positive. You can access these components as <code>x.val</code>, which has type <code>\u211d</code>, and <code>x.property</code>, which represents the fact <code>0 &lt; x.val</code>. EXAMPLES: -/ -- QUOTE: def PReal :=   { y : \u211d // 0 &lt; y }</p> <p>section variable (x : PReal)</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-xval","title":"check x.val","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-xproperty","title":"check x.property","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-x1","title":"check x.1","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-x2","title":"check x.2","text":"<p>end -- QUOTE.</p> <p>/- TEXT: We could have used subtypes to define the standard 2-simplex, as well as the standard :math:<code>n</code>-simplex for an arbitrary :math:<code>n</code>. EXAMPLES: -/ -- QUOTE: def StandardTwoSimplex' :=   { p : \u211d \u00d7 \u211d \u00d7 \u211d // 0 \u2264 p.1 \u2227 0 \u2264 p.2.1 \u2227 0 \u2264 p.2.2 \u2227 p.1 + p.2.1 + p.2.2 = 1 }</p> <p>def StandardSimplex' (n : \u2115) :=   { v : Fin n \u2192 \u211d // (\u2200 i : Fin n, 0 \u2264 v i) \u2227 (\u2211 i, v i) = 1 } -- QUOTE.</p> <p>/- TEXT: Similarly, Sigma types are generalizations of ordered pairs, whereby the type of the second component depends on the type of the first. EXAMPLES: -/ -- QUOTE: def StdSimplex := \u03a3 n : \u2115, StandardSimplex n</p> <p>section variable (s : StdSimplex)</p>"},{"location":"MIL/C06_Structures/S01_Structures/#check-sfst","title":"check s.fst","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-ssnd","title":"check s.snd","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-s1","title":"check s.1","text":""},{"location":"MIL/C06_Structures/S01_Structures/#check-s2","title":"check s.2","text":"<p>end -- QUOTE.</p> <p>/- TEXT: Given <code>s : StdSimplex</code>, the first component <code>s.fst</code> is a natural number, and the second component is an element of the corresponding simplex <code>StandardSimplex s.fst</code>. The difference between a Sigma type and a subtype is that the second component of a Sigma type is data rather than a proposition.</p> <p>But even though we can use products, subtypes, and Sigma types instead of structures, using structures has a number of advantages. Defining a structure abstracts away the underlying representation and provides custom names for the functions that access the components. This makes proofs more robust: proofs that rely only on the interface to a structure will generally continue to work when we change the definition, as long as we redefine the old accessors in terms of the new definition. Moreover, as we are about to see, Lean provides support for weaving structures together into a rich, interconnected hierarchy, and for managing the interactions between them. TEXT. -/ /- OMIT: (TODO) Comments from Patrick: We could make this paragraph much less abstract by showing how to access the components of a point with the definition def point'' := \u211d \u00d7 \u211d \u00d7 \u211d. However if we do that it would probably be honest to also mention the possibility of using fin 3 \u2192 \u211d as the definition. This interesting anyhow, because I think very few mathematician realize that defining \u211d^n as an iterated cartesian product is a polite lie that would be a nightmare if taken seriously.</p> <p>By the way, should be include some comment about similarities and differences with object-oriented programming? All the examples from that page would clearly fit very well with classes in python say. And we'll have to face the name-clash between classes in Lean and classes in C++ or python sooner or later. Life would be so much simpler if classes in Lean could use another name... OMIT. -/</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/","title":"S02 Algebraic Structures","text":"<p>import MIL.Common import Mathlib.Data.Real.Basic</p> <p>namespace C06S02</p> <p>/- TEXT: .. _section_algebraic_structures:</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#algebraic-structures","title":"Algebraic Structures","text":"<p>To clarify what we mean by the phrase algebraic structure, it will help to consider some examples.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#a-partially-ordered-set-consists-of-a-set-mathp-and","title":". A partially ordered set consists of a set :math:<code>P</code> and","text":"<p>a binary relation :math:<code>\\le</code> on :math:<code>P</code> that is transitive    and reflexive.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#a-group-consists-of-a-set-mathg-with-an-associative","title":". A group consists of a set :math:<code>G</code> with an associative","text":"<p>binary operation, an identity element    :math:<code>1</code>, and a function :math:<code>g \\mapsto g^{-1}</code> that returns    an inverse for each :math:<code>g</code> in :math:<code>G</code>.    A group is abelian or commutative if the operation is commutative.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#a-lattice-is-a-partially-ordered-set-with-meets-and-joins","title":". A lattice is a partially ordered set with meets and joins.","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#a-ring-consists-of-an-additively-written-abelian-group","title":". A ring consists of an (additively written) abelian group","text":"<p>:math:<code>(R, +, 0, x \\mapsto -x)</code>    together with an associative multiplication operation    :math:<code>\\cdot</code> and an identity :math:<code>1</code>,    such that multiplication distributes over addition.    A ring is commutative if the multiplication is commutative.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#an-ordered-ring-mathr-0-cdot-1-le-consists-of-a-ring","title":". An ordered ring :math:<code>(R, +, 0, -, \\cdot, 1, \\le)</code> consists of a ring","text":"<p>together with a partial order on its elements, such that :math:<code>a \\le b</code> implies    :math:<code>a + c \\le b + c</code> for every :math:<code>a</code>, :math:<code>b</code>, and :math:<code>c</code> in :math:<code>R</code>,    and :math:<code>0 \\le a</code> and :math:<code>0 \\le b</code> implies :math:<code>0 \\le a b</code> for    every :math:<code>a</code> and :math:<code>b</code> in :math:<code>R</code>.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#a-metric-space-consists-of-a-set-mathx-and-a-function","title":". A metric space consists of a set :math:<code>X</code> and a function","text":"<p>:math:<code>d : X \\times X \\to \\mathbb{R}</code> such that the following hold:</p> <ul> <li>:math:<code>d(x, y) \\ge 0</code> for every :math:<code>x</code> and :math:<code>y</code> in :math:<code>X</code>.</li> <li>:math:<code>d(x, y) = 0</code> if and only if :math:<code>x = y</code>.</li> <li>:math:<code>d(x, y) = d(y, x)</code> for every :math:<code>x</code> and :math:<code>y</code> in :math:<code>X</code>.</li> <li>:math:<code>d(x, z) \\le d(x, y) + d(y, z)</code> for every :math:<code>x</code>, :math:<code>y</code>, and      :math:<code>z</code> in :math:<code>X</code>.</li> </ul>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#a-topological-space-consists-of-a-set-mathx-and-a-collection-mathmathcal-t","title":". A topological space consists of a set :math:<code>X</code> and a collection :math:<code>\\mathcal T</code>","text":"<p>of subsets of :math:<code>X</code>, called the open subsets of :math:<code>X</code>, such that    the following hold:</p> <ul> <li>The empty set and :math:<code>X</code> are open.</li> <li>The intersection of two open sets is open.</li> <li>An arbitrary union of open sets is open.</li> </ul> <p>In each of these examples, the elements of the structure belong to a set, the carrier set, that sometimes stands proxy for the entire structure. For example, when we say \"let :math:<code>G</code> be a group\" and then \"let :math:<code>g \\in G</code>,\" we are using :math:<code>G</code> to stand for both the structure and its carrier. Not every algebraic structure is associated with a single carrier set in this way. For example, a bipartite graph involves a relation between two sets, as does a Galois connection, A category also involves two sets of interest, commonly called the objects and the morphisms.</p> <p>The examples indicate some of the things that a proof assistant has to do in order to support algebraic reasoning. First, it needs to recognize concrete instances of structures. The number systems :math:<code>\\mathbb{Z}</code>, :math:<code>\\mathbb{Q}</code>, and :math:<code>\\mathbb{R}</code> are all ordered rings, and we should be able to apply a generic theorem about ordered rings in any of these instances. Sometimes a concrete set may be an instance of a structure in more than one way. For example, in addition to the usual topology on :math:<code>\\mathbb{R}</code>, which forms the basis for real analysis, we can also consider the discrete topology on :math:<code>\\mathbb{R}</code>, in which every set is open.</p> <p>Second, a proof assistant needs to support generic notation on structures. In Lean, the notation <code>*</code> is used for multiplication in all the usual number systems, as well as for multiplication in generic groups and rings. When we use an expression like <code>f x * y</code>, Lean has to use information about the types of <code>f</code>, <code>x</code>, and <code>y</code> to determine which multiplication we have in mind.</p> <p>Third, it needs to deal with the fact that structures can inherit definitions, theorems, and notation from other structures in various ways. Some structures extend others by adding more axioms. A commutative ring is still a ring, so any definition that makes sense in a ring also makes sense in a commutative ring, and any theorem that holds in a ring also holds in a commutative ring. Some structures extend others by adding more data. For example, the additive part of any ring is an additive group. The ring structure adds a multiplication and an identity, as well as axioms that govern them and relate them to the additive part. Sometimes we can define one structure in terms of another. Any metric space has a canonical topology associated with it, the metric space topology, and there are various topologies that can be associated with any linear ordering.</p> <p>Finally, it is important to keep in mind that mathematics allows us to use functions and operations to define structures in the same way we use functions and operations to define numbers. Products and powers of groups are again groups. For every :math:<code>n</code>, the integers modulo :math:<code>n</code> form a ring, and for every :math:<code>k &gt; 0</code>, the :math:<code>k \\times k</code> matrices of polynomials with coefficients in that ring again form a ring. Thus we can calculate with structures just as easily as we can calculate with their elements. This means that algebraic structures lead dual lives in mathematics, as containers for collections of objects and as objects in their own right. A proof assistant has to accommodate this dual role.</p> <p>When dealing with elements of a type that has an algebraic structure associated with it, a proof assistant needs to recognize the structure and find the relevant definitions, theorems, and notation. All this should sound like a lot of work, and it is. But Lean uses a small collection of fundamental mechanisms to carry out these tasks. The goal of this section is to explain these mechanisms and show you how to use them.</p> <p>The first ingredient is almost too obvious to mention: formally speaking, algebraic structures are structures in the sense of :numref:<code>section_structures</code>. An algebraic structure is a specification of a bundle of data satisfying some axiomatic hypotheses, and we saw in :numref:<code>section_structures</code> that this is exactly what the <code>structure</code> command is designed to accommodate. It's a marriage made in heaven!</p> <p>Given a data type <code>\u03b1</code>, we can define the group structure on <code>\u03b1</code> as follows. EXAMPLES: -/ -- QUOTE: structure Group\u2081 (\u03b1 : Type*) where   mul : \u03b1 \u2192 \u03b1 \u2192 \u03b1   one : \u03b1   inv : \u03b1 \u2192 \u03b1   mul_assoc : \u2200 x y z : \u03b1, mul (mul x y) z = mul x (mul y z)   mul_one : \u2200 x : \u03b1, mul x one = x   one_mul : \u2200 x : \u03b1, mul one x = x   mul_left_inv : \u2200 x : \u03b1, mul (inv x) x = one -- QUOTE.</p> <p>-- OMIT: TODO: explain the extends command later, and also redundant inheritance /- TEXT: Notice that the type <code>\u03b1</code> is a parameter in the definition of <code>group\u2081</code>. So you should think of an object <code>struc : Group\u2081 \u03b1</code> as being a group structure on <code>\u03b1</code>. We saw in :numref:<code>proving_identities_in_algebraic_structures</code> that the counterpart <code>mul_right_inv</code> to <code>mul_left_inv</code> follows from the other group axioms, so there is no need to add it to the definition.</p> <p>This definition of a group is similar to the definition of <code>Group</code> in Mathlib, and we have chosen the name <code>Group\u2081</code> to distinguish our version. If you write <code>#check Group</code> and ctrl-click on the definition, you will see that the Mathlib version of <code>Group</code> is defined to extend another structure; we will explain how to do that later. If you type <code>#print Group</code> you will also see that the Mathlib version of <code>Group</code> has a number of extra fields. For reasons we will explain later, sometimes it is useful to add redundant information to a structure, so that there are additional fields for objects and functions that can be defined from the core data. Don't worry about that for now. Rest assured that our simplified version <code>Group\u2081</code> is morally the same as the definition of a group that Mathlib uses.</p> <p>It is sometimes useful to bundle the type together with the structure, and Mathlib also contains a definition of a <code>GroupCat</code> structure that is equivalent to the following: EXAMPLES: -/ -- QUOTE: structure Group\u2081Cat where   \u03b1 : Type*   str : Group\u2081 \u03b1 -- QUOTE.</p> <p>/- TEXT: The Mathlib version is found in <code>Mathlib.Algebra.Category.GroupCat.Basic</code>, and you can <code>#check</code> it if you add this to the imports at the beginning of the examples file.</p> <p>For reasons that will become clearer below, it is more often useful to keep the type <code>\u03b1</code> separate from the structure <code>Group \u03b1</code>. We refer to the two objects together as a partially bundled structure, since the representation combines most, but not all, of the components into one structure. It is common in Mathlib to use capital roman letters like <code>G</code> for a type when it is used as the carrier type for a group.</p> <p>Let's construct a group, which is to say, an element of the <code>Group\u2081</code> type. For any pair of types <code>\u03b1</code> and <code>\u03b2</code>, Mathlib defines the type <code>Equiv \u03b1 \u03b2</code> of equivalences between <code>\u03b1</code> and <code>\u03b2</code>. Mathlib also defines the suggestive notation <code>\u03b1 \u2243 \u03b2</code> for this type. An element <code>f : \u03b1 \u2243 \u03b2</code> is a bijection between <code>\u03b1</code> and <code>\u03b2</code> represented by four components: a function <code>f.toFun</code> from <code>\u03b1</code> to <code>\u03b2</code>, the inverse function <code>f.invFun</code> from <code>\u03b2</code> to <code>\u03b1</code>, and two properties that specify these functions are indeed inverse to one another. EXAMPLES: -/ section -- QUOTE: variable (\u03b1 \u03b2 \u03b3 : Type*) variable (f : \u03b1 \u2243 \u03b2) (g : \u03b2 \u2243 \u03b3)</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-equiv","title":"check Equiv \u03b1 \u03b2","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-ftofun","title":"check (f.toFun : \u03b1 \u2192 \u03b2)","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-finvfun","title":"check (f.invFun : \u03b2 \u2192 \u03b1)","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-fright_inv-x-f-finvfun-x-x","title":"check (f.right_inv : \u2200 x : \u03b2, f (f.invFun x) = x)","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-fleft_inv-x-finvfun-f-x-x","title":"check (f.left_inv : \u2200 x : \u03b1, f.invFun (f x) = x)","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-equivrefl","title":"check (Equiv.refl \u03b1 : \u03b1 \u2243 \u03b1)","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-fsymm","title":"check (f.symm : \u03b2 \u2243 \u03b1)","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-ftrans-g","title":"check (f.trans g : \u03b1 \u2243 \u03b3)","text":"<p>-- QUOTE.</p> <p>/- TEXT: Notice the creative naming of the last three constructions. We think of the identity function <code>Equiv.refl</code>, the inverse operation <code>Equiv.symm</code>, and the composition operation <code>Equiv.trans</code> as explicit evidence that the property of being in bijective correspondence is an equivalence relation.</p> <p>Notice also that <code>f.trans g</code> requires composing the forward functions in reverse order. Mathlib has declared a coercion from <code>Equiv \u03b1 \u03b2</code> to the function type <code>\u03b1 \u2192 \u03b2</code>, so we can omit writing <code>.toFun</code> and have Lean insert it for us. EXAMPLES: -/ -- QUOTE: example (x : \u03b1) : (f.trans g).toFun x = g.toFun (f.toFun x) :=   rfl</p> <p>example (x : \u03b1) : (f.trans g) x = g (f x) :=   rfl</p> <p>example : (f.trans g : \u03b1 \u2192 \u03b3) = g \u2218 f :=   rfl -- QUOTE.</p> <p>end</p> <p>/- TEXT: Mathlib also defines the type <code>perm \u03b1</code> of equivalences between <code>\u03b1</code> and itself. EXAMPLES: -/ -- QUOTE: example (\u03b1 : Type*) : Equiv.Perm \u03b1 = (\u03b1 \u2243 \u03b1) :=   rfl -- QUOTE.</p> <p>/- TEXT: It should be clear that <code>Equiv.Perm \u03b1</code> forms a group under composition of equivalences. We orient things so that <code>mul f g</code> is equal to <code>g.trans f</code>, whose forward function is <code>f \u2218 g</code>. In other words, multiplication is what we ordinarily think of as composition of the bijections. Here we define this group: EXAMPLES: -/ -- QUOTE: def permGroup {\u03b1 : Type*} : Group\u2081 (Equiv.Perm \u03b1)     where   mul f g := Equiv.trans g f   one := Equiv.refl \u03b1   inv := Equiv.symm   mul_assoc f g h := (Equiv.trans_assoc _ _ _).symm   one_mul := Equiv.trans_refl   mul_one := Equiv.refl_trans   mul_left_inv := Equiv.self_trans_symm -- QUOTE.</p> <p>/- TEXT: In fact, Mathlib defines exactly this <code>Group</code> structure on <code>Equiv.Perm \u03b1</code> in the file <code>GroupTheory.Perm.Basic</code>. As always, you can hover over the theorems used in the definition of <code>permGroup</code> to see their statements, and you can jump to their definitions in the original file to learn more about how they are implemented.</p> <p>In ordinary mathematics, we generally think of notation as independent of structure. For example, we can consider groups :math:<code>(G_1, \\cdot, 1, \\cdot^{-1})</code>, :math:<code>(G_2, \\circ, e, i(\\cdot))</code>, and :math:<code>(G_3, +, 0, -)</code>. In the first case, we write the binary operation as :math:<code>\\cdot</code>, the identity at :math:<code>1</code>, and the inverse function as :math:<code>x \\mapsto x^{-1}</code>. In the second and third cases, we use the notational alternatives shown. When we formalize the notion of a group in Lean, however, the notation is more tightly linked to the structure. In Lean, the components of any <code>Group</code> are named <code>mul</code>, <code>one</code>, and <code>inv</code>, and in a moment we will see how multiplicative notation is set up to refer to them. If we want to use additive notation, we instead use an isomorphic structure <code>AddGroup</code> (the structure underlying additive groups). Its components are named <code>add</code>, <code>zero</code>, and <code>neg</code>, and the associated notation is what you would expect it to be.</p> <p>Recall the type <code>Point</code> that we defined in :numref:<code>section_structures</code>, and the addition function that we defined there. These definitions are reproduced in the examples file that accompanies this section. As an exercise, define an <code>AddGroup\u2081</code> structure that is similar to the <code>Group\u2081</code> structure we defined above, except that it uses the additive naming scheme just described. Define negation and a zero on the <code>Point</code> data type, and define the <code>AddGroup\u2081</code> structure on <code>Point</code>. BOTH: -/ -- QUOTE: structure AddGroup\u2081 (\u03b1 : Type*) where /- EXAMPLES:   (add : \u03b1 \u2192 \u03b1 \u2192 \u03b1)   -- fill in the rest SOLUTIONS: -/   add : \u03b1 \u2192 \u03b1 \u2192 \u03b1   zero : \u03b1   neg : \u03b1 \u2192 \u03b1   add_assoc : \u2200 x y z : \u03b1, add (add x y) z = add x (add y z)   add_zero : \u2200 x : \u03b1, add x zero = x   zero_add : \u2200 x : \u03b1, add x zero = x   add_left_neg : \u2200 x : \u03b1, add (neg x) x = zero</p> <p>-- BOTH: @[ext] structure Point where   x : \u211d   y : \u211d   z : \u211d</p> <p>namespace Point</p> <p>def add (a b : Point) : Point :=   \u27e8a.x + b.x, a.y + b.y, a.z + b.z\u27e9</p> <p>/- EXAMPLES: def neg (a : Point) : Point := sorry</p> <p>def zero : Point := sorry</p> <p>def addGroupPoint : AddGroup\u2081 Point := sorry</p> <p>SOLUTIONS: -/ def neg (a : Point) : Point :=   \u27e8-a.x, -a.y, -a.z\u27e9</p> <p>def zero : Point :=   \u27e80, 0, 0\u27e9</p> <p>def addGroupPoint : AddGroup\u2081 Point where   add := Point.add   zero := Point.zero   neg := Point.neg   add_assoc := by simp [Point.add, add_assoc]   add_zero := by simp [Point.add, Point.zero]   zero_add := by simp [Point.add, Point.zero]   add_left_neg := by simp [Point.add, Point.neg, Point.zero]</p> <p>-- BOTH: end Point -- QUOTE.</p> <p>/- TEXT: We are making progress. Now we know how to define algebraic structures in Lean, and we know how to define instances of those structures. But we also want to associate notation with structures so that we can use it with each instance. Moreover, we want to arrange it so that we can define an operation on a structure and use it with any particular instance, and we want to arrange it so that we can prove a theorem about a structure and use it with any instance.</p> <p>In fact, Mathlib is already set up to use generic group notation, definitions, and theorems for <code>Equiv.Perm \u03b1</code>. EXAMPLES: -/ section -- QUOTE: variable {\u03b1 : Type*} (f g : Equiv.Perm \u03b1) (n : \u2115)</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-f-g","title":"check f * g","text":""},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-mul_assoc-f-g-g1","title":"check mul_assoc f g g\u207b\u00b9","text":"<p>-- group power, defined for any group</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-g-n","title":"check g ^ n","text":"<p>example : f * g * g\u207b\u00b9 = f := by rw [mul_assoc, mul_right_inv, mul_one]</p> <p>example : f * g * g\u207b\u00b9 = f :=   mul_inv_cancel_right f g</p> <p>example {\u03b1 : Type*} (f g : Equiv.Perm \u03b1) : g.symm.trans (g.trans f) = f :=   mul_inv_cancel_right f g -- QUOTE.</p> <p>end</p> <p>/- TEXT: You can check that this is not the case for the additive group structure on <code>Point</code> that we asked you to define above. Our task now is to understand that magic that goes on under the hood in order to make the examples for <code>Equiv.Perm \u03b1</code> work the way they do.</p> <p>The issue is that Lean needs to be able to find the relevant notation and the implicit group structure, using the information that is found in the expressions that we type. Similarly, when we write <code>x + y</code> with expressions <code>x</code> and <code>y</code> that have type <code>\u211d</code>, Lean needs to interpret the <code>+</code> symbol as the relevant addition function on the reals. It also has to recognize the type <code>\u211d</code> as an instance of a commutative ring, so that all the definitions and theorems for a commutative ring are available. For another example, continuity is defined in Lean relative to any two topological spaces. When we have <code>f : \u211d \u2192 \u2102</code> and we write <code>Continuous f</code>, Lean has to find the relevant topologies on <code>\u211d</code> and <code>\u2102</code>.</p> <p>The magic is achieved with a combination of three things.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#logic-a-definition-that-should-be-interpreted-in-any-group-takes-as","title":". Logic. A definition that should be interpreted in any group takes, as","text":"<p>arguments, the type of the group and the group structure as arguments.    Similarly, a theorem about the elements of an arbitrary group    begins with universal quantifiers over    the type of the group and the group structure.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#implicit-arguments-the-arguments-for-the-type-and-the-structure","title":". Implicit arguments. The arguments for the type and the structure","text":"<p>are generally left implicit, so that we do not have to write them    or see them in the Lean information window. Lean fills the    information in for us silently.</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#type-class-inference-also-known-as-class-inference","title":". Type class inference. Also known as class inference,","text":"<p>this is a simple but powerful mechanism    that enables us to register information for Lean to use later on.    When Lean is called on to fill in implicit arguments to a    definition, theorem, or piece of notation,    it can make use of information that has been registered.</p> <p>Whereas an annotation <code>(grp : Group G)</code> tells Lean that it should expect to be given that argument explicitly and the annotation <code>{grp : Group G}</code> tells Lean that it should try to figure it out from contextual cues in the expression, the annotation <code>[grp : Group G]</code> tells Lean that the corresponding argument should be synthesized using type class inference. Since the whole point to the use of such arguments is that we generally do not need to refer to them explicitly, Lean allows us to write <code>[Group G]</code> and leave the name anonymous. You have probably already noticed that Lean chooses names like <code>_inst_1</code> automatically. When we use the anonymous square-bracket annotation with the <code>variables</code> command, then as long as the variables are still in scope, Lean automatically adds the argument <code>[Group G]</code> to any definition or theorem that mentions <code>G</code>.</p> <p>How do we register the information that Lean needs to use to carry out the search? Returning to our group example, we need only make two changes. First, instead of using the <code>structure</code> command to define the group structure, we use the keyword <code>class</code> to indicate that it is a candidate for class inference. Second, instead of defining particular instances with <code>def</code>, we use the keyword <code>instance</code> to register the particular instance with Lean. As with the names of class variables, we are allowed to leave the name of an instance definition anonymous, since in general we intend Lean to find it and put it to use without troubling us with the details. EXAMPLES: -/ -- QUOTE: class Group\u2082 (\u03b1 : Type*) where   mul : \u03b1 \u2192 \u03b1 \u2192 \u03b1   one : \u03b1   inv : \u03b1 \u2192 \u03b1   mul_assoc : \u2200 x y z : \u03b1, mul (mul x y) z = mul x (mul y z)   mul_one : \u2200 x : \u03b1, mul x one = x   one_mul : \u2200 x : \u03b1, mul one x = x   mul_left_inv : \u2200 x : \u03b1, mul (inv x) x = one</p> <p>instance {\u03b1 : Type*} : Group\u2082 (Equiv.Perm \u03b1) where   mul f g := Equiv.trans g f   one := Equiv.refl \u03b1   inv := Equiv.symm   mul_assoc f g h := (Equiv.trans_assoc _ _ _).symm   one_mul := Equiv.trans_refl   mul_one := Equiv.refl_trans   mul_left_inv := Equiv.self_trans_symm -- QUOTE.</p> <p>/- TEXT: The following illustrates their use. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-group2mul","title":"check Group\u2082.mul","text":"<p>def mySquare {\u03b1 : Type*} [Group\u2082 \u03b1] (x : \u03b1) :=   Group\u2082.mul x x</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-mysquare","title":"check mySquare","text":"<p>section variable {\u03b2 : Type*} (f g : Equiv.Perm \u03b2)</p> <p>example : Group\u2082.mul f g = g.trans f :=   rfl</p> <p>example : mySquare f = f.trans f :=   rfl</p> <p>end -- QUOTE.</p> <p>/- TEXT: The <code>#check</code> command shows that <code>Group\u2082.mul</code> has an implicit argument <code>[Group\u2082 \u03b1]</code> that we expect to be found by class inference, where <code>\u03b1</code> is the type of the arguments to <code>Group\u2082.mul</code>. In other words, <code>{\u03b1 : Type*}</code> is the implicit argument for the type of the group elements and <code>[Group\u2082 \u03b1]</code> is the implicit argument for the group structure on <code>\u03b1</code>. Similarly, when we define a generic squaring function <code>my_square</code> for <code>Group\u2082</code>, we use an implicit argument <code>{\u03b1 : Type*}</code> for the type of the elements and an implicit argument <code>[Group\u2082 \u03b1]</code> for the <code>Group\u2082</code> structure.</p> <p>In the first example, when we write <code>Group\u2082.mul f g</code>, the type of <code>f</code> and <code>g</code> tells Lean that in the argument <code>\u03b1</code> to <code>Group\u2082.mul</code> has to be instantiated to <code>Equiv.Perm \u03b2</code>. That means that Lean has to find an element of <code>Group\u2082 (Equiv.Perm \u03b2)</code>. The previous <code>instance</code> declaration tells Lean exactly how to do that. Problem solved!</p> <p>This simple mechanism for registering information so that Lean can find it when it needs it is remarkably useful. Here is one way it comes up. In Lean's foundation, a data type <code>\u03b1</code> may be empty. In a number of applications, however, it is useful to know that a type has at least one element. For example, the function <code>List.headI</code>, which returns the first element of a list, can return the default value when the list is empty. To make that work, the Lean library defines a class <code>Inhabited \u03b1</code>, which does nothing more than store a default value. We can show that the <code>Point</code> type is an instance: EXAMPLES: -/ -- QUOTE: instance : Inhabited Point where default := \u27e80, 0, 0\u27e9</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-default-point","title":"check (default : Point)","text":"<p>example : ([] : List Point).headI = default :=   rfl -- QUOTE.</p> <p>/- TEXT: The class inference mechanism is also used for generic notation. The expression <code>x + y</code> is an abbreviation for <code>Add.add x y</code> where---you guessed it---<code>Add \u03b1</code> is a class that stores a binary function on <code>\u03b1</code>. Writing <code>x + y</code> tells Lean to find a registered instance of <code>[Add.add \u03b1]</code> and use the corresponding function. Below, we register the addition function for <code>Point</code>. EXAMPLES: -/ -- QUOTE: instance : Add Point where add := Point.add</p> <p>section variable (x y : Point)</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-x-y","title":"check x + y","text":"<p>example : x + y = Point.add x y :=   rfl</p> <p>end -- QUOTE.</p> <p>/- TEXT: In this way, we can assign the notation <code>+</code> to binary operations on other types as well.</p> <p>But we can do even better. We have seen that <code>*</code> can be used in any group, <code>+</code> can be used in any additive group, and both can be used in any ring. When we define a new instance of a ring in Lean, we don't have to define <code>+</code> and <code>*</code> for that instance, because Lean knows that these are defined for every ring. We can use this method to specify notation for our <code>Group\u2082</code> class: EXAMPLES: -/ -- QUOTE: instance hasMulGroup\u2082 {\u03b1 : Type*} [Group\u2082 \u03b1] : Mul \u03b1 :=   \u27e8Group\u2082.mul\u27e9</p> <p>instance hasOneGroup\u2082 {\u03b1 : Type*} [Group\u2082 \u03b1] : One \u03b1 :=   \u27e8Group\u2082.one\u27e9</p> <p>instance hasInvGroup\u2082 {\u03b1 : Type*} [Group\u2082 \u03b1] : Inv \u03b1 :=   \u27e8Group\u2082.inv\u27e9</p> <p>section variable {\u03b1 : Type*} (f g : Equiv.Perm \u03b1)</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-f-1-g1","title":"check f * 1 * g\u207b\u00b9","text":"<p>def foo : f * 1 * g\u207b\u00b9 = g.symm.trans ((Equiv.refl \u03b1).trans f) :=   rfl</p> <p>end -- QUOTE.</p> <p>/- TEXT: In this case, we have to supply names for the instances, because Lean has a hard time coming up with good defaults. What makes this approach work is that Lean carries out a recursive search. According to the instances we have declared, Lean can find an instance of <code>Mul (Equiv.Perm \u03b1)</code> by finding an instance of <code>Group\u2082 (Equiv.Perm \u03b1)</code>, and it can find an instance of <code>Group\u2082 (Equiv.Perm \u03b1)</code> because we have provided one. Lean is capable of finding these two facts and chaining them together.</p> <p>The example we have just given is dangerous, because Lean's library also has an instance of <code>Group (Equiv.Perm \u03b1)</code>, and multiplication is defined on any group. So it is ambiguous as to which instance is found. In fact, Lean favors more recent declarations unless you explicitly specify a different priority. Also, there is another way to tell Lean that one structure is an instance of another, using the <code>extends</code> keyword. This is how Mathlib specifies that, for example, every commutative ring is a ring. You can find more information in :numref:<code>hierarchies</code> and in a <code>section on class inference &lt;https://leanprover.github.io/theorem_proving_in_lean4/type_classes.html#managing-type-class-inference&gt;</code>_ in Theorem Proving in Lean.</p> <p>In general, it is a bad idea to specify a value of <code>*</code> for an instance of an algebraic structure that already has the notation defined. Redefining the notion of <code>Group</code> in Lean is an artificial example. In this case, however, both interpretations of the group notation unfold to <code>Equiv.trans</code>, <code>Equiv.refl</code>, and <code>Equiv.symm</code>, in the same way.</p> <p>As a similarly artificial exercise, define a class <code>AddGroup\u2082</code> in analogy to <code>Group\u2082</code>. Define the usual notation for addition, negation, and zero on any <code>AddGroup\u2082</code> using the classes <code>Add</code>, <code>Neg</code>, and <code>Zero</code>. Then show <code>Point</code> is an instance of <code>AddGroup\u2082</code>. Try it out and make sure that the additive group notation works for elements of <code>Point</code>. BOTH: -/ -- QUOTE: class AddGroup\u2082 (\u03b1 : Type*) where /- EXAMPLES:   add : \u03b1 \u2192 \u03b1 \u2192 \u03b1   -- fill in the rest -- QUOTE. SOLUTIONS: -/   add : \u03b1 \u2192 \u03b1 \u2192 \u03b1   zero : \u03b1   neg : \u03b1 \u2192 \u03b1   add_assoc : \u2200 x y z : \u03b1, add (add x y) z = add x (add y z)   add_zero : \u2200 x : \u03b1, add x zero = x   zero_add : \u2200 x : \u03b1, add x zero = x   add_left_neg : \u2200 x : \u03b1, add (neg x) x = zero</p> <p>instance hasAddAddGroup\u2082 {\u03b1 : Type*} [AddGroup\u2082 \u03b1] : Add \u03b1 :=   \u27e8AddGroup\u2082.add\u27e9</p> <p>instance hasZeroAddGroup\u2082 {\u03b1 : Type*} [AddGroup\u2082 \u03b1] : Zero \u03b1 :=   \u27e8AddGroup\u2082.zero\u27e9</p> <p>instance hasNegAddGroup\u2082 {\u03b1 : Type*} [AddGroup\u2082 \u03b1] : Neg \u03b1 :=   \u27e8AddGroup\u2082.neg\u27e9</p> <p>instance : AddGroup\u2082 Point where   add := Point.add   zero := Point.zero   neg := Point.neg   add_assoc := by simp [Point.add, add_assoc]   add_zero := by simp [Point.add, Point.zero]   zero_add := by simp [Point.add, Point.zero]   add_left_neg := by simp [Point.add, Point.neg, Point.zero]</p> <p>section variable (x y : Point)</p>"},{"location":"MIL/C06_Structures/S02_Algebraic_Structures/#check-x-y-0","title":"check x + -y + 0","text":"<p>end</p> <p>/- TEXT: It is not a big problem that we have already declared instances <code>Add</code>, <code>Neg</code>, and <code>Zero</code> for <code>Point</code> above. Once again, the two ways of synthesizing the notation should come up with the same answer.</p> <p>Class inference is subtle, and you have to be careful when using it, because it configures automation that invisibly governs the interpretation of the expressions we type. When used wisely, however, class inference is a powerful tool. It is what makes algebraic reasoning possible in Lean. TEXT. -/</p>"},{"location":"MIL/C06_Structures/S03_Building_the_Gaussian_Integers/","title":"S03 Building the Gaussian Integers","text":"<p>import Mathlib.Algebra.EuclideanDomain.Basic import Mathlib.RingTheory.PrincipalIdealDomain import MIL.Common</p> <p>/- TEXT: .. _section_building_the_gaussian_integers:</p>"},{"location":"MIL/C06_Structures/S03_Building_the_Gaussian_Integers/#building-the-gaussian-integers","title":"Building the Gaussian Integers","text":"<p>We will now illustrate the use of the algebraic hierarchy in Lean by building an important mathematical object, the Gaussian integers, and showing that it is a Euclidean domain. In other words, according to the terminology we have been using, we will define the Gaussian integers and show that they are an instance of the Euclidean domain structure.</p> <p>In ordinary mathematical terms, the set of Gaussian integers :math:<code>\\Bbb{Z}[i]</code> is the set of complex numbers :math:<code>\\{ a + b i \\mid a, b \\in \\Bbb{Z}\\}</code>. But rather than define them as a subset of the complex numbers, our goal here is to define them as a data type in their own right. We do this by representing a Gaussian integer as a pair of integers, which we think of as the real and imaginary parts. BOTH: -/ -- QUOTE: @[ext] structure GaussInt where   re : \u2124   im : \u2124 -- QUOTE.</p> <p>/- TEXT: We first show that the Gaussian integers have the structure of a ring, with <code>0</code> defined to be <code>\u27e80, 0\u27e9</code>, <code>1</code> defined to be <code>\u27e81, 0\u27e9</code>, and addition defined pointwise. To work out the definition of multiplication, remember that we want the element :math:<code>i</code>, represented by <code>\u27e80, 1\u27e9</code>, to be a square root of :math:<code>-1</code>. Thus we want</p> <p>.. math::</p> <p>(a + bi) (c + di) &amp; = ac + bci + adi + bd i^2 \\      &amp; = (ac - bd) + (bc + ad)i.</p> <p>This explains the definition of <code>Mul</code> below. BOTH: -/ namespace GaussInt</p> <p>-- QUOTE: instance : Zero GaussInt :=   \u27e8\u27e80, 0\u27e9\u27e9</p> <p>instance : One GaussInt :=   \u27e8\u27e81, 0\u27e9\u27e9</p> <p>instance : Add GaussInt :=   \u27e8fun x y \u21a6 \u27e8x.re + y.re, x.im + y.im\u27e9\u27e9</p> <p>instance : Neg GaussInt :=   \u27e8fun x \u21a6 \u27e8-x.re, -x.im\u27e9\u27e9</p> <p>instance : Mul GaussInt :=   \u27e8fun x y \u21a6 \u27e8x.re * y.re - x.im * y.im, x.re * y.im + x.im * y.re\u27e9\u27e9 -- QUOTE.</p> <p>/- TEXT: As noted in :numref:<code>section_structures</code>, it is a good idea to put all the definitions related to a data type in a namespace with the same name. Thus in the Lean files associated with this chapter, these definitions are made in the <code>GaussInt</code> namespace.</p> <p>Notice that here we are defining the interpretations of the notation <code>0</code>, <code>1</code>, <code>+</code>, <code>-</code>, and <code>*</code> directly, rather than naming them <code>GaussInt.zero</code> and the like and assigning the notation to those. It is often useful to have an explicit name for the definitions, for example, to use with <code>simp</code> and <code>rewrite</code>. BOTH: -/ -- QUOTE: theorem zero_def : (0 : GaussInt) = \u27e80, 0\u27e9 :=   rfl</p> <p>theorem one_def : (1 : GaussInt) = \u27e81, 0\u27e9 :=   rfl</p> <p>theorem add_def (x y : GaussInt) : x + y = \u27e8x.re + y.re, x.im + y.im\u27e9 :=   rfl</p> <p>theorem neg_def (x : GaussInt) : -x = \u27e8-x.re, -x.im\u27e9 :=   rfl</p> <p>theorem mul_def (x y : GaussInt) :     x * y = \u27e8x.re * y.re - x.im * y.im, x.re * y.im + x.im * y.re\u27e9 :=   rfl -- QUOTE.</p> <p>/- TEXT: It is also useful to name the rules that compute the real and imaginary parts, and to declare them to the simplifier. BOTH: -/ -- QUOTE: @[simp] theorem zero_re : (0 : GaussInt).re = 0 :=   rfl</p> <p>@[simp] theorem zero_im : (0 : GaussInt).im = 0 :=   rfl</p> <p>@[simp] theorem one_re : (1 : GaussInt).re = 1 :=   rfl</p> <p>@[simp] theorem one_im : (1 : GaussInt).im = 0 :=   rfl</p> <p>@[simp] theorem add_re (x y : GaussInt) : (x + y).re = x.re + y.re :=   rfl</p> <p>@[simp] theorem add_im (x y : GaussInt) : (x + y).im = x.im + y.im :=   rfl</p> <p>@[simp] theorem neg_re (x : GaussInt) : (-x).re = -x.re :=   rfl</p> <p>@[simp] theorem neg_im (x : GaussInt) : (-x).im = -x.im :=   rfl</p> <p>@[simp] theorem mul_re (x y : GaussInt) : (x * y).re = x.re * y.re - x.im * y.im :=   rfl</p> <p>@[simp] theorem mul_im (x y : GaussInt) : (x * y).im = x.re * y.im + x.im * y.re :=   rfl -- QUOTE.</p> <p>/- TEXT: It is now surprisingly easy to show that the Gaussian integers are an instance of a commutative ring. We are putting the structure concept to good use. Each particular Gaussian integer is an instance of the <code>GaussInt</code> structure, whereas the type <code>GaussInt</code> itself, together with the relevant operations, is an instance of the <code>CommRing</code> structure. The <code>CommRing</code> structure, in turn, extends the notational structures <code>Zero</code>, <code>One</code>, <code>Add</code>, <code>Neg</code>, and <code>Mul</code>.</p> <p>If you type <code>instance : CommRing GaussInt := _</code>, click on the light bulb that appears in VS Code, and then ask Lean to fill in a skeleton for the structure definition, you will see a scary number of entries. Jumping to the definition of the structure, however, shows that many of the fields have default definitions that Lean will fill in for you automatically. The essential ones appear in the definition below. A special case are <code>nsmul</code> and <code>zsmul</code> which should be ignored for now and will be explained in the next chapter. In each case, the relevant identity is proved by unfolding definitions, using the <code>ext</code> tactic to reduce the identities to their real and imaginary components, simplifying, and, if necessary, carrying out the relevant ring calculation in the integers. Note that we could easily avoid repeating all this code, but this is not the topic of the current discussion. BOTH: -/ -- QUOTE: instance instCommRing : CommRing GaussInt where   zero := 0   one := 1   add := (\u00b7 + \u00b7)   neg x := -x   mul := (\u00b7 * \u00b7)   nsmul := nsmulRec   zsmul := zsmulRec   add_assoc := by     intros     ext &lt;;&gt; simp &lt;;&gt; ring   zero_add := by     intro     ext &lt;;&gt; simp   add_zero := by     intro     ext &lt;;&gt; simp   add_left_neg := by     intro     ext &lt;;&gt; simp   add_comm := by     intros     ext &lt;;&gt; simp &lt;;&gt; ring   mul_assoc := by     intros     ext &lt;;&gt; simp &lt;;&gt; ring   one_mul := by     intro     ext &lt;;&gt; simp   mul_one := by     intro     ext &lt;;&gt; simp   left_distrib := by     intros     ext &lt;;&gt; simp &lt;;&gt; ring   right_distrib := by     intros     ext &lt;;&gt; simp &lt;;&gt; ring   mul_comm := by     intros     ext &lt;;&gt; simp &lt;;&gt; ring   zero_mul := by     intros     ext &lt;;&gt; simp   mul_zero := by     intros     ext &lt;;&gt; simp -- QUOTE.</p> <p>@[simp] theorem sub_re (x y : GaussInt) : (x - y).re = x.re - y.re :=   rfl</p> <p>@[simp] theorem sub_im (x y : GaussInt) : (x - y).im = x.im - y.im :=   rfl</p> <p>/- TEXT: Lean's library defines the class of nontrivial types to be types with at least two distinct elements. In the context of a ring, this is equivalent to saying that the zero is not equal to the one. Since some common theorems depend on that fact, we may as well establish it now. BOTH: -/ -- QUOTE: instance : Nontrivial GaussInt := by   use 0, 1   rw [Ne, GaussInt.ext_iff]   simp -- QUOTE.</p> <p>end GaussInt</p> <p>/- TEXT: We will now show that the Gaussian integers have an important additional property. A Euclidean domain is a ring :math:<code>R</code> equipped with a norm function :math:<code>N : R \\to \\mathbb{N}</code> with the following two properties:</p> <ul> <li>For every :math:<code>a</code> and :math:<code>b \\ne 0</code> in :math:<code>R</code>, there are   :math:<code>q</code> and :math:<code>r</code> in :math:<code>R</code> such that :math:<code>a = bq + r</code> and   either :math:<code>r = 0</code> or <code>N(r) &lt; N(b)</code>.</li> <li>For every :math:<code>a</code> and :math:<code>b \\ne 0</code>, :math:<code>N(a) \\le N(ab)</code>.</li> </ul> <p>The ring of integers :math:<code>\\Bbb{Z}</code> with :math:<code>N(a) = |a|</code> is an archetypal example of a Euclidean domain. In that case, we can take :math:<code>q</code> to be the result of integer division of :math:<code>a</code> by :math:<code>b</code> and :math:<code>r</code> to be the remainder. These functions are defined in Lean so that the satisfy the following: EXAMPLES: -/ -- QUOTE: example (a b : \u2124) : a = b * (a / b) + a % b :=   Eq.symm (Int.ediv_add_emod a b)</p> <p>example (a b : \u2124) : b \u2260 0 \u2192 0 \u2264 a % b :=   Int.emod_nonneg a</p> <p>example (a b : \u2124) : b \u2260 0 \u2192 a % b &lt; |b| :=   Int.emod_lt a -- QUOTE.</p> <p>/- TEXT: In an arbitrary ring, an element :math:<code>a</code> is said to be a unit if it divides :math:<code>1</code>. A nonzero element :math:<code>a</code> is said to be irreducible if it cannot be written in the form :math:<code>a = bc</code> where neither :math:<code>b</code> nor :math:<code>c</code> is a unit. In the integers, every irreducible element :math:<code>a</code> is prime, which is to say, whenever :math:<code>a</code> divides a product :math:<code>bc</code>, it divides either :math:<code>b</code> or :math:<code>c</code>. But in other rings this property can fail. In the ring :math:<code>\\Bbb{Z}[\\sqrt{-5}]</code>, we have</p> <p>.. math::</p> <p>6 = 2 \\cdot 3 = (1 + \\sqrt{-5})(1 - \\sqrt{-5}),</p> <p>and the elements :math:<code>2</code>, :math:<code>3</code>, :math:<code>1 + \\sqrt{-5}</code>, and :math:<code>1 - \\sqrt{-5}</code> are all irreducible, but they are not prime. For example, :math:<code>2</code> divides the product :math:<code>(1 + \\sqrt{-5})(1 - \\sqrt{-5})</code>, but it does not divide either factor. In particular, we no longer have unique factorization: the number :math:<code>6</code> can be factored into irreducible elements in more than one way.</p> <p>In contrast, every Euclidean domain is a unique factorization domain, which implies that every irreducible element is prime. The axioms for a Euclidean domain imply that one can write any nonzero element as a finite product of irreducible elements. They also imply that one can use the Euclidean algorithm to find a greatest common divisor of any two nonzero elements <code>a</code> and <code>b</code>, i.e.~an element that is divisible by any other common divisor. This, in turn, implies that factorization into irreducible elements is unique up to multiplication by units.</p> <p>We now show that the Gaussian integers are a Euclidean domain with the norm defined by :math:<code>N(a + bi) = (a + bi)(a - bi) = a^2 + b^2</code>. The Gaussian integer :math:<code>a - bi</code> is called the conjugate of :math:<code>a + bi</code>. It is not hard to check that for any complex numbers :math:<code>x</code> and :math:<code>y</code>, we have :math:<code>N(xy) = N(x)N(y)</code>.</p> <p>To see that this definition of the norm makes the Gaussian integers a Euclidean domain, only the first property is challenging. Suppose we want to write :math:<code>a + bi = (c + di) q + r</code> for suitable :math:<code>q</code> and :math:<code>r</code>. Treating :math:<code>a + bi</code> and :math:<code>c + di</code> are complex numbers, carry out the division</p> <p>.. math::</p> <p>\\frac{a + bi}{c + di} = \\frac{(a + bi)(c - di)}{(c + di)(c-di)} =     \\frac{ac + bd}{c^2 + d^2} + \\frac{bc -ad}{c<sup>2+d</sup>2} i.</p> <p>The real and imaginary parts might not be integers, but we can round them to the nearest integers :math:<code>u</code> and :math:<code>v</code>. We can then express the right-hand side as :math:<code>(u + vi) + (u' + v'i)</code>, where :math:<code>u' + v'i</code> is the part left over. Note that we have :math:<code>|u'| \\le 1/2</code> and :math:<code>|v'| \\le 1/2</code>, and hence</p> <p>.. math::</p> <p>N(u' + v' i) = (u')^2 + (v')^2 \\le \u00bc + \u00bc \\le \u00bd.</p> <p>Multiplying through by :math:<code>c + di</code>, we have</p> <p>.. math::</p> <p>a + bi = (c + di) (u + vi) + (c + di) (u' + v'i).</p> <p>Setting :math:<code>q = u + vi</code> and :math:<code>r = (c + di) (u' + v'i)</code>, we have :math:<code>a + bi = (c + di) q + r</code>, and we only need to bound :math:<code>N(r)</code>:</p> <p>.. math::</p> <p>N\u00ae = N(c + di)N(u' + v'i) \\le N(c + di) \\cdot \u00bd &lt; N(c + di).</p> <p>The argument we just carried out requires viewing the Gaussian integers as a subset of the complex numbers. One option for formalizing it in Lean is therefore to embed the Gaussian integers in the complex numbers, embed the integers in the Gaussian integers, define the rounding function from the real numbers to the integers, and take great care to pass back and forth between these number systems appropriately. In fact, this is exactly the approach that is followed in Mathlib, where the Gaussian integers themselves are constructed as a special case of a ring of quadratic integers. See the file <code>GaussianInt.lean &lt;https://github.com/leanprover-community/mathlib4/blob/master/Mathlib/NumberTheory/Zsqrtd/GaussianInt.lean&gt;</code>_.</p> <p>Here we will instead carry out an argument that stays in the integers. This illustrates an choice one commonly faces when formalizing mathematics. Given an argument that requires concepts or machinery that is not already in the library, one has two choices: either formalizes the concepts or machinery needed, or adapt the argument to make use of concepts and machinery you already have. The first choice is generally a good investment of time when the results can be used in other contexts. Pragmatically speaking, however, sometimes seeking a more elementary proof is more efficient.</p> <p>The usual quotient-remainder theorem for the integers says that for every :math:<code>a</code> and nonzero :math:<code>b</code>, there are :math:<code>q</code> and :math:<code>r</code> such that :math:<code>a = b q + r</code> and :math:<code>0 \\le r &lt; b</code>. Here we will make use of the following variation, which says that there are :math:<code>q'</code> and :math:<code>r'</code> such that :math:<code>a = b q' + r'</code> and :math:<code>|r'| \\le b/2</code>. You can check that if the value of :math:<code>r</code> in the first statement satisfies :math:<code>r \\le b/2</code>, we can take :math:<code>q' = q</code> and :math:<code>r' = r</code>, and otherwise we can take :math:<code>q' = q + 1</code> and :math:<code>r' = r - b</code>. We are grateful to Heather Macbeth for suggesting the following more elegant approach, which avoids definition by cases. We simply add <code>b / 2</code> to <code>a</code> before dividing and then subtract it from the remainder. BOTH: -/ namespace Int</p> <p>-- QUOTE: def div' (a b : \u2124) :=   (a + b / 2) / b</p> <p>def mod' (a b : \u2124) :=   (a + b / 2) % b - b / 2</p> <p>theorem div'_add_mod' (a b : \u2124) : b * div' a b + mod' a b = a := by   rw [div', mod']   linarith [Int.ediv_add_emod (a + b / 2) b]</p> <p>theorem abs_mod'_le (a b : \u2124) (h : 0 &lt; b) : |mod' a b| \u2264 b / 2 := by   rw [mod', abs_le]   constructor   \u00b7 linarith [Int.emod_nonneg (a + b / 2) h.ne']   have := Int.emod_lt_of_pos (a + b / 2) h   have := Int.ediv_add_emod b 2   have := Int.emod_lt_of_pos b zero_lt_two   revert this; intro this -- FIXME, this should not be needed   linarith -- QUOTE.</p> <p>/- TEXT: Note the use of our old friend, <code>linarith</code>. We will also need to express <code>mod'</code> in terms of <code>div'</code>. BOTH: -/ -- QUOTE: theorem mod'_eq (a b : \u2124) : mod' a b = a - b * div' a b := by linarith [div'_add_mod' a b] -- QUOTE.</p> <p>end Int</p> <p>/- TEXT: We will use the fact that :math:<code>x^2 + y^2</code> is equal to zero if and only if :math:<code>x</code> and :math:<code>y</code> are both zero. As an exercise, we ask you to prove that this holds in any ordered ring. SOLUTIONS: -/ private theorem aux {\u03b1 : Type*} [LinearOrderedRing \u03b1] {x y : \u03b1} (h : x ^ 2 + y ^ 2 = 0) : x = 0 :=   haveI h' : x ^ 2 = 0 := by     apply le_antisymm _ (sq_nonneg x)     rw [\u2190 h]     apply le_add_of_nonneg_right (sq_nonneg y)   pow_eq_zero h'</p> <p>-- QUOTE: -- BOTH: theorem sq_add_sq_eq_zero {\u03b1 : Type*} [LinearOrderedRing \u03b1] (x y : \u03b1) :     x ^ 2 + y ^ 2 = 0 \u2194 x = 0 \u2227 y = 0 := by /- EXAMPLES:   sorry SOLUTIONS: -/   constructor   \u00b7 intro h     constructor     \u00b7 exact aux h     rw [add_comm] at h     exact aux h   rintro \u27e8rfl, rfl\u27e9   norm_num -- QUOTE.</p> <p>-- BOTH: /- TEXT: We will put all the remaining definitions and theorems in this section in the <code>GaussInt</code> namespace. First, we define the <code>norm</code> function and ask you to establish some of its properties. The proofs are all short. BOTH: -/ namespace GaussInt</p> <p>-- QUOTE: def norm (x : GaussInt) :=   x.re ^ 2 + x.im ^ 2</p> <p>@[simp] theorem norm_nonneg (x : GaussInt) : 0 \u2264 norm x := by /- EXAMPLES:   sorry SOLUTIONS: -/   apply add_nonneg &lt;;&gt;   apply sq_nonneg</p> <p>-- BOTH: theorem norm_eq_zero (x : GaussInt) : norm x = 0 \u2194 x = 0 := by /- EXAMPLES:   sorry SOLUTIONS: -/   rw [norm, sq_add_sq_eq_zero, GaussInt.ext_iff]   rfl</p> <p>-- BOTH: theorem norm_pos (x : GaussInt) : 0 &lt; norm x \u2194 x \u2260 0 := by /- EXAMPLES:   sorry SOLUTIONS: -/   rw [lt_iff_le_and_ne, ne_comm, Ne, norm_eq_zero]   simp [norm_nonneg]</p> <p>-- BOTH: theorem norm_mul (x y : GaussInt) : norm (x * y) = norm x * norm y := by /- EXAMPLES:   sorry SOLUTIONS: -/   simp [norm]   ring</p> <p>-- BOTH: -- QUOTE. /- TEXT: Next we define the conjugate function: BOTH: -/ -- QUOTE: def conj (x : GaussInt) : GaussInt :=   \u27e8x.re, -x.im\u27e9</p> <p>@[simp] theorem conj_re (x : GaussInt) : (conj x).re = x.re :=   rfl</p> <p>@[simp] theorem conj_im (x : GaussInt) : (conj x).im = -x.im :=   rfl</p> <p>theorem norm_conj (x : GaussInt) : norm (conj x) = norm x := by simp [norm] -- QUOTE.</p> <p>/- TEXT: Finally, we define division for the Gaussian integers with the notation <code>x / y</code>, that rounds the complex quotient to the nearest Gaussian integer. We use our bespoke <code>Int.div'</code> for that purpose. As we calculated above, if <code>x</code> is :math:<code>a + bi</code> and <code>y</code> is :math:<code>c + di</code>, then the real and imaginary parts of <code>x / y</code> are the nearest integers to</p> <p>.. math::</p> <p>\\frac{ac + bd}{c^2 + d^2} \\quad \\text{and} \\quad \\frac{bc -ad}{c<sup>2+d</sup>2},</p> <p>respectively. Here the numerators are the real and imaginary parts of :math:<code>(a + bi) (c - di)</code>, and the denominators are both equal to the norm of :math:<code>c + di</code>. BOTH: -/ -- QUOTE: instance : Div GaussInt :=   \u27e8fun x y \u21a6 \u27e8Int.div' (x * conj y).re (norm y), Int.div' (x * conj y).im (norm y)\u27e9\u27e9 -- QUOTE.</p> <p>/- TEXT: Having defined <code>x / y</code>, We define <code>x % y</code> to be the remainder, <code>x - (x / y) * y</code>. As above, we record the definitions in the theorems <code>div_def</code> and <code>mod_def</code> so that we can use them with <code>simp</code> and <code>rewrite</code>. BOTH: -/ -- QUOTE: instance : Mod GaussInt :=   \u27e8fun x y \u21a6 x - y * (x / y)\u27e9</p> <p>theorem div_def (x y : GaussInt) :     x / y = \u27e8Int.div' (x * conj y).re (norm y), Int.div' (x * conj y).im (norm y)\u27e9 :=   rfl</p> <p>theorem mod_def (x y : GaussInt) : x % y = x - y * (x / y) :=   rfl -- QUOTE.</p> <p>/- TEXT: These definitions immediately yield <code>x = y * (x / y) + x % y</code> for every <code>x</code> and <code>y</code>, so all we need to do is show that the norm of <code>x % y</code> is less than the norm of <code>y</code> when <code>y</code> is not zero.</p> <p>We just defined the real and imaginary parts of <code>x / y</code> to be <code>div' (x * conj y).re (norm y)</code> and <code>div' (x * conj y).im (norm y)</code>, respectively. Calculating, we have</p> <p><code>(x % y) * conj y = (x - x / y * y) * conj y = x * conj y - x / y * (y * conj y)</code></p> <p>The real and imaginary parts of the right-hand side are exactly <code>mod' (x * conj y).re (norm y)</code> and <code>mod' (x * conj y).im (norm y)</code>. By the properties of <code>div'</code> and <code>mod'</code>, these are guaranteed to be less than or equal to <code>norm y / 2</code>. So we have</p> <p><code>norm ((x % y) * conj y) \u2264 (norm y / 2)^2 + (norm y / 2)^2 \u2264 (norm y / 2) * norm y</code>.</p> <p>On the other hand, we have</p> <p><code>norm ((x % y) * conj y) = norm (x % y) * norm (conj y) = norm (x % y) * norm y</code>.</p> <p>Dividing through by <code>norm y</code> we have <code>norm (x % y) \u2264 (norm y) / 2 &lt; norm y</code>, as required.</p> <p>This messy calculation is carried out in the next proof. We encourage you to step through the details and see if you can find a nicer argument. BOTH: -/ -- QUOTE: theorem norm_mod_lt (x : GaussInt) {y : GaussInt} (hy : y \u2260 0) :     (x % y).norm &lt; y.norm := by   have norm_y_pos : 0 &lt; norm y := by rwa [norm_pos]   have H1 : x % y * conj y = \u27e8Int.mod' (x * conj y).re (norm y), Int.mod' (x * conj y).im (norm y)\u27e9   \u00b7 ext &lt;;&gt; simp [Int.mod'_eq, mod_def, div_def, norm] &lt;;&gt; ring   have H2 : norm (x % y) * norm y \u2264 norm y / 2 * norm y   \u00b7 calc       norm (x % y) * norm y = norm (x % y * conj y) := by simp only [norm_mul, norm_conj]       _ = |Int.mod' (x.re * y.re + x.im * y.im) (norm y)| ^ 2           + |Int.mod' (-(x.re * y.im) + x.im * y.re) (norm y)| ^ 2 := by simp [H1, norm, sq_abs]       _ \u2264 (y.norm / 2) ^ 2 + (y.norm / 2) ^ 2 := by gcongr &lt;;&gt; apply Int.abs_mod'_le _ _ norm_y_pos       _ = norm y / 2 * (norm y / 2 * 2) := by ring       _ \u2264 norm y / 2 * norm y := by gcongr; apply Int.ediv_mul_le; norm_num   calc norm (x % y) \u2264 norm y / 2 := le_of_mul_le_mul_right H2 norm_y_pos     _ &lt; norm y := by         apply Int.ediv_lt_of_lt_mul         \u00b7 norm_num         \u00b7 linarith -- QUOTE.</p> <p>/- TEXT: We are in the home stretch. Our <code>norm</code> function maps Gaussian integers to nonnegative integers. We need a function that maps Gaussian integers to natural numbers, and we obtain that by composing <code>norm</code> with the function <code>Int.natAbs</code>, which maps integers to the natural numbers. The first of the next two lemmas establishes that mapping the norm to the natural numbers and back to the integers does not change the value. The second one re-expresses the fact that the norm is decreasing. BOTH: -/ -- QUOTE: theorem coe_natAbs_norm (x : GaussInt) : (x.norm.natAbs : \u2124) = x.norm :=   Int.natAbs_of_nonneg (norm_nonneg _)</p> <p>theorem natAbs_norm_mod_lt (x y : GaussInt) (hy : y \u2260 0) :     (x % y).norm.natAbs &lt; y.norm.natAbs := by   apply Int.ofNat_lt.1   simp only [Int.coe_natAbs, abs_of_nonneg, norm_nonneg]   apply norm_mod_lt x hy -- QUOTE.</p> <p>/- TEXT: We also need to establish the second key property of the norm function on a Euclidean domain. BOTH: -/ -- QUOTE: theorem not_norm_mul_left_lt_norm (x : GaussInt) {y : GaussInt} (hy : y \u2260 0) :     \u00ac(norm (x * y)).natAbs &lt; (norm x).natAbs := by   apply not_lt_of_ge   rw [norm_mul, Int.natAbs_mul]   apply le_mul_of_one_le_right (Nat.zero_le _)   apply Int.ofNat_le.1   rw [coe_natAbs_norm]   exact Int.add_one_le_of_lt ((norm_pos _).mpr hy) -- QUOTE.</p> <p>/- TEXT: We can now put it together to show that the Gaussian integers are an instance of a Euclidean domain. We use the quotient and remainder function we have defined. The Mathlib definition of a Euclidean domain is more general than the one above in that it allows us to show that remainder decreases with respect to any well-founded measure. Comparing the values of a norm function that returns natural numbers is just one instance of such a measure, and in that case, the required properties are the theorems <code>natAbs_norm_mod_lt</code> and <code>not_norm_mul_left_lt_norm</code>. BOTH: -/ -- QUOTE: instance : EuclideanDomain GaussInt :=   { GaussInt.instCommRing with     quotient := (\u00b7 / \u00b7)     remainder := (\u00b7 % \u00b7)     quotient_mul_add_remainder_eq :=       fun x y \u21a6 by simp only; rw [mod_def, add_comm] ; ring     quotient_zero := fun x \u21a6 by       simp [div_def, norm, Int.div']       rfl     r := (measure (Int.natAbs \u2218 norm)).1     r_wellFounded := (measure (Int.natAbs \u2218 norm)).2     remainder_lt := natAbs_norm_mod_lt     mul_left_not_lt := not_norm_mul_left_lt_norm } -- QUOTE.</p> <p>/- TEXT: An immediate payoff is that we now know that, in the Gaussian integers, the notions of being prime and being irreducible coincide. BOTH: -/ -- QUOTE: example (x : GaussInt) : Irreducible x \u2194 Prime x :=   PrincipalIdealRing.irreducible_iff_prime -- QUOTE.</p> <p>end GaussInt</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/","title":"S01 Basics","text":"<p>import MIL.Common import Mathlib.Algebra.BigOperators.Ring import Mathlib.Data.Real.Basic</p> <p>set_option autoImplicit true</p> <p>/- TEXT: .. _section_hierarchies_basics:</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#basics","title":"Basics","text":"<p>At the very bottom of all hierarchies in Lean, we find data-carrying classes. The following class records that the given type <code>\u03b1</code> is endowed with a distinguished element called <code>one</code>. At this stage, it has no property at all. BOTH: -/</p> <p>-- QUOTE: class One\u2081 (\u03b1 : Type) where   /-- The element one -/   one : \u03b1 -- QUOTE.</p> <p>/- TEXT: Since we'll make a much heavier use of classes in this chapter, we need to understand some more details about what the <code>class</code> command is doing. First, the <code>class</code> command above defines a structure <code>One\u2081</code> with parameter <code>\u03b1 : Type</code> and a single field <code>one</code>. It also mark this structure as a class so that arguments of type <code>One\u2081 \u03b1</code> for some type <code>\u03b1</code> will be inferrable using the instance resolution procedure, as long as they are marked as instance-implicit, ie appear between square brackets. Those two effects could also have been achieved using the <code>structure</code> command with <code>class</code> attribute, ie writing <code>@[class] structure</code> instance of <code>class</code>. But the class command also ensures that <code>One\u2081 \u03b1</code> appears as an instance-implicit argument in its own fields. Compare: BOTH: -/</p> <p>-- QUOTE:</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-one1one-one1one-type-self-one1","title":"check One\u2081.one -- One\u2081.one {\u03b1 : Type} [self : One\u2081 \u03b1] : \u03b1","text":"<p>@[class] structure One\u2082 (\u03b1 : Type) where   /-- The element one -/   one : \u03b1</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-one2one","title":"check One\u2082.one","text":"<p>-- QUOTE.</p> <p>/- TEXT: In the second check, we can see that <code>self : One\u2082 \u03b1</code> is an explicit argument. Let us make sure the first version is indeed usable without any explicit argument. BOTH: -/</p> <p>-- QUOTE: example (\u03b1 : Type) [One\u2081 \u03b1] : \u03b1 := One\u2081.one -- QUOTE.</p> <p>/- TEXT: Remark: in the above example, the argument <code>One\u2081 \u03b1</code> is marked as instance-implicit, which is a bit silly since this affects only uses of the declaration and declaration created by the <code>example</code> command cannot be used. However it allows us to avoid giving a name to that argument and, more importantly, it starts installing the good habit of marking <code>One\u2081 \u03b1</code> arguments as instance-implicit.</p> <p>Another remark is that all this will work only when Lean knows what is <code>\u03b1</code>. In the above example, leaving out the type ascription <code>: \u03b1</code> would generate an error message like: <code>typeclass instance problem is stuck, it is often due to metavariables One\u2081 (?m.263 \u03b1)</code> where <code>?m.263 \u03b1</code> means \"some type depending on <code>\u03b1</code>\" (and 263 is simply an auto-generated index that would be useful to distinguish between several unknown things). Another way to avoid this issue would be to use a type annotation, as in: BOTH: -/ -- QUOTE: example (\u03b1 : Type) [One\u2081 \u03b1] := (One\u2081.one : \u03b1) -- QUOTE.</p> <p>/- TEXT: You may have already encountered that issue when playing with limits of sequences in :numref:<code>sequences_and_convergence</code> if you tried to state for instance that <code>0 &lt; 1</code> without telling Lean whether you meant this inequality to be about natural numbers or real numbers.</p> <p>Our next task is to assign a notation to <code>One\u2081.one</code>. Since we don't want collisions with the builtin notation for <code>1</code>, we will use <code>\ud835\udfd9</code>. This is achieved by the following command where the first line tells Lean to use the documentation of <code>One\u2081.one</code> as documentation for the symbol <code>\ud835\udfd9</code>. BOTH: -/ -- QUOTE: @[inherit_doc] notation \"\ud835\udfd9\" =&gt; One\u2081.one</p> <p>example {\u03b1 : Type} [One\u2081 \u03b1] : \u03b1 := \ud835\udfd9</p> <p>example {\u03b1 : Type} [One\u2081 \u03b1] : (\ud835\udfd9 : \u03b1) = \ud835\udfd9 := rfl -- QUOTE.</p> <p>/- TEXT: We now want a data-carrying class recording a binary operation. We don't want to choose between addition and multiplication for now so we'll use diamond. BOTH: -/</p> <p>-- QUOTE: class Dia\u2081 (\u03b1 : Type) where   dia : \u03b1 \u2192 \u03b1 \u2192 \u03b1</p> <p>infixl:70 \" \u22c4 \"   =&gt; Dia\u2081.dia -- QUOTE.</p> <p>/- TEXT: As in the <code>One\u2081</code> example, the operation has no property at all at this stage. Let us now define the class of semigroup structures where the operation is denoted by <code>\u22c4</code>. For now, we define it by hand as a structure with two fields, a <code>Dia\u2081</code> instance and some <code>Prop</code>-valued field <code>dia_assoc</code> asserting associativity of <code>\u22c4</code>. BOTH: -/</p> <p>-- QUOTE: class Semigroup\u2081 (\u03b1 : Type) where   toDia\u2081 : Dia\u2081 \u03b1   /-- Diamond is associative -/   dia_assoc : \u2200 a b c : \u03b1, a \u22c4 b \u22c4 c = a \u22c4 (b \u22c4 c) -- QUOTE.</p> <p>/- TEXT: Note that while stating <code>dia_assoc</code>, the previously defined field <code>toDia\u2081</code> is in the local context hence can be used when Lean searches for an instance of <code>Dia\u2081 \u03b1</code> to make sense of <code>a \u22c4 b</code>. However this <code>toDia\u2081</code> field does not become part of the type class instances database. Hence doing <code>example {\u03b1 : Type} [Semigroup\u2081 \u03b1] (a b : \u03b1) : \u03b1 := a \u22c4 b</code> would fail with error message <code>failed to synthesize instance Dia\u2081 \u03b1</code>.</p> <p>We can fix this by adding the <code>instance</code> attribute later. BOTH: -/</p> <p>-- QUOTE: attribute [instance] Semigroup\u2081.toDia\u2081</p> <p>example {\u03b1 : Type} [Semigroup\u2081 \u03b1] (a b : \u03b1) : \u03b1 := a \u22c4 b -- QUOTE.</p> <p>/- TEXT: Before building up, we need a more convenient way to extend structures than explicitly writing fields like <code>toDia\u2081</code> and adding the instance attribute by hand. The <code>class</code> supports this using the <code>extends</code> syntax as in: BOTH: -/</p> <p>-- QUOTE: class Semigroup\u2082 (\u03b1 : Type) extends Dia\u2081 \u03b1 where   /-- Diamond is associative -/   dia_assoc : \u2200 a b c : \u03b1, a \u22c4 b \u22c4 c = a \u22c4 (b \u22c4 c)</p> <p>example {\u03b1 : Type} [Semigroup\u2082 \u03b1] (a b : \u03b1) : \u03b1 := a \u22c4 b -- QUOTE.</p> <p>/- TEXT: Note this syntax is also available in the <code>structure</code> command, although it that case it fixes only the hurdle of writing fields such as <code>toDia\u2081</code> since there is no instance to define in that case.</p> <p>Let us now try to combine a diamond operation and a distinguished one with axioms saying this element is neutral on both sides. BOTH: -/ -- QUOTE: class DiaOneClass\u2081 (\u03b1 : Type) extends One\u2081 \u03b1, Dia\u2081 \u03b1 where   /-- One is a left neutral element for diamond. -/   one_dia : \u2200 a : \u03b1, \ud835\udfd9 \u22c4 a = a   /-- One is a right neutral element for diamond -/   dia_one : \u2200 a : \u03b1, a \u22c4 \ud835\udfd9 = a</p> <p>-- QUOTE.</p> <p>/- TEXT: In the next example, we tell Lean that <code>\u03b1</code> has a <code>DiaOneClass\u2081</code> structure and state a property that uses both a <code>Dia\u2081</code> instance and a <code>One\u2081</code> instance. In order to see how Lean finds those instances we set a tracing option whose result can be seen in the Infoview. This result is rather terse by default but it can be expanded by clicking on lines ending with black arrows. It includes failed attempts where Lean tried to find instances before having enough type information to succeed. The successful attempts do involve the instances generated by the <code>extends</code> syntax. BOTH: -/</p> <p>-- QUOTE: set_option trace.Meta.synthInstance true in example {\u03b1 : Type} [DiaOneClass\u2081 \u03b1] (a b : \u03b1) : Prop := a \u22c4 b = \ud835\udfd9 -- QUOTE.</p> <p>/- TEXT: Note that we don't need to include extra fields where combining existing classes. Hence we can define monoids as: BOTH: -/</p> <p>-- QUOTE: class Monoid\u2081 (\u03b1 : Type) extends Semigroup\u2081 \u03b1, DiaOneClass\u2081 \u03b1 -- QUOTE.</p> <p>/- TEXT: While the above definition seems straightforward, it hides an important subtlety. Both <code>Semigroup\u2081 \u03b1</code> and <code>DiaOneClass\u2081 \u03b1</code> extend <code>Dia\u2081 \u03b1</code>, so one could fear that having a <code>Monoid\u2081 \u03b1</code> instance gives two unrelated diamond operations on <code>\u03b1</code>, one coming from a field <code>Monoid\u2081.toSemigroup\u2081</code> and one coming from a field <code>Monoid\u2081.toDiaOneClass\u2081</code>.</p> <p>Indeed if we try to build a monoid class by hand using: BOTH: -/</p> <p>-- QUOTE: class Monoid\u2082 (\u03b1 : Type) where   toSemigroup\u2081 : Semigroup\u2081 \u03b1   toDiaOneClass\u2081 : DiaOneClass\u2081 \u03b1 -- QUOTE.</p> <p>/- TEXT: then we get two completely unrelated diamond operations <code>Monoid\u2082.toSemigroup\u2081.toDia\u2081.dia</code> and <code>Monoid\u2082.toDiaOneClass\u2081.toDia\u2081.dia</code>.</p> <p>The version generated using the <code>extends</code> syntax does not have this defect. BOTH: -/</p> <p>-- QUOTE: example {\u03b1 : Type} [Monoid\u2081 \u03b1] :   (Monoid\u2081.toSemigroup\u2081.toDia\u2081.dia : \u03b1 \u2192 \u03b1 \u2192 \u03b1) = Monoid\u2081.toDiaOneClass\u2081.toDia\u2081.dia := rfl -- QUOTE.</p> <p>/- TEXT: So the <code>class</code> command did some magic for us (and the <code>structure</code> command would have done it too). An easy way to see what are the fields of our classes is to check their constructor. Compare: BOTH: -/</p> <p>-- QUOTE: /- Monoid\u2082.mk {\u03b1 : Type} (toSemigroup\u2081 : Semigroup\u2081 \u03b1) (toDiaOneClass\u2081 : DiaOneClass\u2081 \u03b1) : Monoid\u2082 \u03b1 -/</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-monoid2mk","title":"check Monoid\u2082.mk","text":"<p>/- Monoid\u2081.mk {\u03b1 : Type} [toSemigroup\u2081 : Semigroup\u2081 \u03b1] [toOne\u2081 : One\u2081 \u03b1] (one_dia : \u2200 (a : \u03b1), \ud835\udfd9 \u22c4 a = a) (dia_one : \u2200 (a : \u03b1), a \u22c4 \ud835\udfd9 = a) : Monoid\u2081 \u03b1 -/</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-monoid1mk","title":"check Monoid\u2081.mk","text":"<p>-- QUOTE.</p> <p>/- TEXT: So we see that <code>Monoid\u2081</code> takes <code>Semigroup\u2081 \u03b1</code> argument as expected but then it won't take a would-be overlapping <code>DiaOneClass\u2081 \u03b1</code> argument but instead tears it apart and includes only the non-overlapping parts. And it also auto-generated an instance <code>Monoid\u2081.toDiaOneClass\u2081</code> which is not a field but has the expected signature which, from the end-user point of view, restores the symmetry between the two extended classes <code>Semigroup\u2081</code> and <code>DiaOneClass\u2081</code>. BOTH: -/</p> <p>-- QUOTE:</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-monoid1tosemigroup1","title":"check Monoid\u2081.toSemigroup\u2081","text":""},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-monoid1todiaoneclass1","title":"check Monoid\u2081.toDiaOneClass\u2081","text":"<p>-- QUOTE.</p> <p>/- TEXT: We are now very close to defining groups. We could add to the monoid structure a field asserting the existence of an inverse for every element. But then we would need to work to access these inverses. In practice it is more convenient to add it as data. To optimize reusability, we define a new data-carrying class, and then give it some notation. BOTH: -/</p> <p>-- QUOTE: class Inv\u2081 (\u03b1 : Type) where   /-- The inversion function -/   inv : \u03b1 \u2192 \u03b1</p> <p>@[inherit_doc] postfix:max \"\u207b\u00b9\" =&gt; Inv\u2081.inv</p> <p>class Group\u2081 (G : Type) extends Monoid\u2081 G, Inv\u2081 G where   inv_dia : \u2200 a : G, a\u207b\u00b9 \u22c4 a = \ud835\udfd9 -- QUOTE.</p> <p>/- TEXT: The above definition may seem too weak, we only ask that <code>a\u207b\u00b9</code> is a left-inverse of <code>a</code>. But the other side is automatic. In order to prove that, we need a preliminary lemma. BOTH: -/</p> <p>-- QUOTE: lemma left_inv_eq_right_inv\u2081 {M : Type} [Monoid\u2081 M] {a b c : M} (hba : b \u22c4 a = \ud835\udfd9) (hac : a \u22c4 c = \ud835\udfd9) : b = c := by   rw [\u2190 DiaOneClass\u2081.one_dia c, \u2190 hba, Semigroup\u2081.dia_assoc, hac, DiaOneClass\u2081.dia_one b] -- QUOTE.</p> <p>/- TEXT: In this lemma, it is pretty annoying to give full names, especially since it requires knowing which part of the hierarchy provides those facts. One way to fix this is to use the <code>export</code> command to copy those facts as lemmas in the root name space. BOTH: -/</p> <p>-- QUOTE: export DiaOneClass\u2081 (one_dia dia_one) export Semigroup\u2081 (dia_assoc) export Group\u2081 (inv_dia) -- QUOTE.</p> <p>/- TEXT: We can then rewrite the above proof as: BOTH: -/</p> <p>-- QUOTE: example {M : Type} [Monoid\u2081 M] {a b c : M} (hba : b \u22c4 a = \ud835\udfd9) (hac : a \u22c4 c = \ud835\udfd9) : b = c := by   rw [\u2190 one_dia c, \u2190 hba, dia_assoc, hac, dia_one b] -- QUOTE.</p> <p>/- TEXT: It is now your turn to prove things about our algebraic structures. BOTH: -/</p> <p>-- QUOTE: lemma inv_eq_of_dia [Group\u2081 G] {a b : G} (h : a \u22c4 b = \ud835\udfd9) : a\u207b\u00b9 = b := /- EXAMPLES:   sorry SOLUTIONS: -/   left_inv_eq_right_inv\u2081 (inv_dia a) h -- BOTH:</p> <p>lemma dia_inv [Group\u2081 G] (a : G) : a \u22c4 a\u207b\u00b9 = \ud835\udfd9 := /- EXAMPLES:   sorry SOLUTIONS: -/   by rw [\u2190 inv_dia a\u207b\u00b9, inv_eq_of_dia (inv_dia a)] -- QUOTE.</p> <p>/- TEXT: At this stage we would like to move on to define rings, but there is a serious issue. A ring structure on a type contains both an additive group structure and a multiplicative monoid structure, and some properties about their interaction. But so far we hard-coded a notation <code>\u22c4</code> for all our operations. More fundamentally, the type class system assumes every type has only one instance of each type class. There are various ways to solve this issue. Surprisingly Mathlib uses the naive idea to duplicate everything for additive and multiplicative theories with the help of some code-generating attribute. Structures and classes are defined in both additive and multiplicative notation with an attribute <code>to_additive</code> linking them. In case of multiple inheritance like for semi-groups, the auto-generated \"symmetry-restoring\" instances need also to be marked. This is a bit technical; you don't need to understand details. The important point is that lemmas are then only stated in multiplicative notation and marked with the attribute <code>to_additive</code> to generate the additive version as <code>left_inv_eq_right_inv'</code> with its auto-generated additive version <code>left_neg_eq_right_neg'</code>. In order to check the name of this additive version we used the <code>whatsnew in</code> command on top of <code>left_inv_eq_right_inv'</code>. BOTH: -/</p> <p>-- QUOTE:</p> <p>class AddSemigroup\u2083 (\u03b1 : Type) extends Add \u03b1 where /-- Addition is associative -/   add_assoc\u2083 : \u2200 a b c : \u03b1, a + b + c = a + (b + c)</p> <p>@[to_additive AddSemigroup\u2083] class Semigroup\u2083 (\u03b1 : Type) extends Mul \u03b1 where /-- Multiplication is associative -/   mul_assoc\u2083 : \u2200 a b c : \u03b1, a * b * c = a * (b * c)</p> <p>class AddMonoid\u2083 (\u03b1 : Type) extends AddSemigroup\u2083 \u03b1, AddZeroClass \u03b1</p> <p>@[to_additive AddMonoid\u2083] class Monoid\u2083 (\u03b1 : Type) extends Semigroup\u2083 \u03b1, MulOneClass \u03b1</p> <p>attribute [to_additive existing] Monoid\u2083.toMulOneClass</p> <p>export Semigroup\u2083 (mul_assoc\u2083) export AddSemigroup\u2083 (add_assoc\u2083)</p> <p>whatsnew in @[to_additive] lemma left_inv_eq_right_inv' {M : Type} [Monoid\u2083 M] {a b c : M} (hba : b * a = 1) (hac : a * c = 1) : b = c := by   rw [\u2190 one_mul c, \u2190 hba, mul_assoc\u2083, hac, mul_one b]</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#check-left_neg_eq_right_neg","title":"check left_neg_eq_right_neg'","text":"<p>-- QUOTE.</p> <p>/- TEXT: Equipped with this technology, we can easily define also commutative semigroups, monoids and groups, and then define rings.</p> <p>BOTH: -/ -- QUOTE: class AddCommSemigroup\u2083 (\u03b1 : Type) extends AddSemigroup\u2083 \u03b1 where   add_comm : \u2200 a b : \u03b1, a + b = b + a</p> <p>@[to_additive AddCommSemigroup\u2083] class CommSemigroup\u2083 (\u03b1 : Type) extends Semigroup\u2083 \u03b1 where   mul_comm : \u2200 a b : \u03b1, a * b = b * a</p> <p>class AddCommMonoid\u2083 (\u03b1 : Type) extends AddMonoid\u2083 \u03b1, AddCommSemigroup\u2083 \u03b1</p> <p>@[to_additive AddCommMonoid\u2083] class CommMonoid\u2083 (\u03b1 : Type) extends Monoid\u2083 \u03b1, CommSemigroup\u2083 \u03b1</p> <p>class AddGroup\u2083 (G : Type) extends AddMonoid\u2083 G, Neg G where   neg_add : \u2200 a : G, -a + a = 0</p> <p>@[to_additive AddGroup\u2083] class Group\u2083 (G : Type) extends Monoid\u2083 G, Inv G where   inv_mul : \u2200 a : G, a\u207b\u00b9 * a = 1 -- QUOTE.</p> <p>/- TEXT: We should remember to tag lemmas with <code>simp</code> when appropriate. BOTH: -/</p> <p>-- QUOTE: attribute [simp] Group\u2083.inv_mul AddGroup\u2083.neg_add</p> <p>-- QUOTE.</p> <p>/- TEXT: Then we need to repeat ourselves a bit since we switch to standard notations, but at least <code>to_additive</code> does the work of translating from the multiplicative notation to the additive one. BOTH: -/</p> <p>-- QUOTE: @[to_additive] lemma inv_eq_of_mul [Group\u2083 G] {a b : G} (h : a * b = 1) : a\u207b\u00b9 = b := /- EXAMPLES:   sorry SOLUTIONS: -/   left_inv_eq_right_inv' (Group\u2083.inv_mul a) h -- BOTH: -- QUOTE.</p> <p>/- TEXT: Note that <code>to_additive</code> can be asked to tag a lemma with <code>simp</code> and propagate that attribute to the additive version as follows. BOTH: -/</p> <p>-- QUOTE: @[to_additive (attr := simp)] lemma Group\u2083.mul_inv {G : Type} [Group\u2083 G] {a : G} : a * a\u207b\u00b9 = 1 := by /- EXAMPLES:   sorry SOLUTIONS: -/   rw [\u2190 inv_mul a\u207b\u00b9, inv_eq_of_mul (inv_mul a)] -- BOTH:</p> <p>@[to_additive] lemma mul_left_cancel\u2083 {G : Type} [Group\u2083 G] {a b c : G} (h : a * b = a * c) : b = c := by /- EXAMPLES:   sorry SOLUTIONS: -/   simpa [\u2190 mul_assoc\u2083] using congr_arg (a\u207b\u00b9 * \u00b7) h -- BOTH:</p> <p>@[to_additive] lemma mul_right_cancel\u2083 {G : Type} [Group\u2083 G] {a b c : G} (h : b*a = c*a) : b = c := by /- EXAMPLES:   sorry SOLUTIONS: -/   simpa [mul_assoc\u2083] using congr_arg (\u00b7 * a\u207b\u00b9) h -- BOTH:</p> <p>class AddCommGroup\u2083 (G : Type) extends AddGroup\u2083 G, AddCommMonoid\u2083 G</p> <p>@[to_additive AddCommGroup\u2083] class CommGroup\u2083 (G : Type) extends Group\u2083 G, CommMonoid\u2083 G</p> <p>-- QUOTE.</p> <p>/- TEXT: We are now ready for rings. For demonstration purposes we won't assume that addition is commutative, and then immediately provide an instance of <code>AddCommGroup\u2083</code>. Mathlib does not play this game, first because in practice this does not make any ring instance easier and also because Mathlib's algebraic hierarchy goes through semirings which are like rings but without opposites so that the proof below does not work for them. What we gain here, besides a nice exercise if you have never seen it, is an example of building an instance using the syntax that allows to provide a parent structure and some extra fields. BOTH: -/</p> <p>-- QUOTE: class Ring\u2083 (R : Type) extends AddGroup\u2083 R, Monoid\u2083 R, MulZeroClass R where   /-- Multiplication is left distributive over addition -/   left_distrib : \u2200 a b c : R, a * (b + c) = a * b + a * c   /-- Multiplication is right distributive over addition -/   right_distrib : \u2200 a b c : R, (a + b) * c = a * c + b * c</p> <p>instance {R : Type} [Ring\u2083 R] : AddCommGroup\u2083 R := { Ring\u2083.toAddGroup\u2083 with   add_comm := by /- EXAMPLES:     sorry } SOLUTIONS: -/     intro a b     have : a + (a + b + b) = a + (b + a + b) := calc       a + (a + b + b) = (a + a) + (b + b) := by simp [add_assoc\u2083, add_assoc\u2083]       _ = (1 * a + 1 * a) + (1 * b + 1 * b) := by simp       _ = (1 + 1) * a + (1 + 1) * b := by simp [Ring\u2083.right_distrib]       _ = (1 + 1) * (a + b) := by simp [Ring\u2083.left_distrib]       _ = 1 * (a + b) + 1 * (a + b) := by simp [Ring\u2083.right_distrib]       _ = (a + b) + (a + b) := by simp       _ = a + (b + a + b) := by simp [add_assoc\u2083]     exact add_right_cancel\u2083 (add_left_cancel\u2083 this) } -- QUOTE. /- TEXT: Of course we can also build concrete instances, such as a ring structure on integers (of course the instance below uses that all the work is already done in Mathlib). BOTH: -/</p> <p>-- QUOTE: instance : Ring\u2083 \u2124 where   add := (\u00b7 + \u00b7)   add_assoc\u2083 := add_assoc   zero := 0   zero_add := by simp   add_zero := by simp   neg := (- \u00b7)   neg_add := by simp   mul := (\u00b7 * \u00b7)   mul_assoc\u2083 := mul_assoc   one := 1   one_mul := by simp   mul_one := by simp   zero_mul := by simp   mul_zero := by simp   left_distrib := Int.mul_add   right_distrib := Int.add_mul -- QUOTE. /- TEXT: As an exercise you can now set up a simple hierarchy for order relations, including a class for ordered commutative monoids, which have both a partial order and a commutative monoid structure such that <code>\u2200 a b : \u03b1, a \u2264 b \u2192 \u2200 c : \u03b1, c * a \u2264 c * b</code>. Of course you need to add fields and maybe <code>extends</code> clauses to the following classes. BOTH: -/ -- QUOTE:</p> <p>class LE\u2081 (\u03b1 : Type) where   /-- The Less-or-Equal relation. -/   le : \u03b1 \u2192 \u03b1 \u2192 Prop</p> <p>@[inherit_doc] infix:50 \" \u2264\u2081 \" =&gt; LE\u2081.le</p> <p>class Preorder\u2081 (\u03b1 : Type) -- SOLUTIONS:   extends LE\u2081 \u03b1 where   le_refl : \u2200 a : \u03b1, a \u2264\u2081 a   le_trans : \u2200 a b c : \u03b1, a \u2264\u2081 b \u2192 b \u2264\u2081 c \u2192 a \u2264\u2081 c -- BOTH:</p> <p>class PartialOrder\u2081 (\u03b1 : Type) -- SOLUTIONS:   extends Preorder\u2081 \u03b1 where   le_antisymm : \u2200 a b : \u03b1, a \u2264\u2081 b \u2192 b \u2264\u2081 a \u2192 a = b -- BOTH:</p> <p>class OrderedCommMonoid\u2081 (\u03b1 : Type) -- SOLUTIONS:   extends PartialOrder\u2081 \u03b1, CommMonoid\u2083 \u03b1 where   mul_of_le : \u2200 a b : \u03b1, a \u2264\u2081 b \u2192 \u2200 c : \u03b1, c * a \u2264\u2081 c * b -- BOTH:</p> <p>instance : OrderedCommMonoid\u2081 \u2115 where -- SOLUTIONS:   le := (\u00b7 \u2264 \u00b7)   le_refl := fun _ \u21a6 le_rfl   le_trans := fun _ _ _ \u21a6 le_trans   le_antisymm := fun _ _ \u21a6 le_antisymm   mul := (\u00b7 * \u00b7)   mul_assoc\u2083 := mul_assoc   one := 1   one_mul := one_mul   mul_one := mul_one   mul_comm := mul_comm   mul_of_le := fun _ _ h c \u21a6 Nat.mul_le_mul_left c h -- QUOTE. /- TEXT:</p> <p>We now want to discuss algebraic structures involving several types. The prime example is modules over rings. If you don't know what is a module, you can pretend it means vector space and think that all our rings are fields. Those structures are commutative additive groups equipped with a scalar multiplication by elements of some ring.</p> <p>We first define the data-carrying type class of scalar multiplication by some type <code>\u03b1</code> on some type <code>\u03b2</code>, and give it a right associative notation. BOTH: -/</p> <p>-- QUOTE: class SMul\u2083 (\u03b1 : Type) (\u03b2 : Type) where   /-- Scalar multiplication -/   smul : \u03b1 \u2192 \u03b2 \u2192 \u03b2</p> <p>infixr:73 \" \u2022 \" =&gt; SMul\u2083.smul -- QUOTE.</p> <p>/- TEXT: Then we can define modules (again think about vector spaces if you don't know what is a module). BOTH: -/</p> <p>-- QUOTE: class Module\u2081 (R : Type) [Ring\u2083 R] (M : Type) [AddCommGroup\u2083 M] extends SMul\u2083 R M where   zero_smul : \u2200 m : M, (0 : R) \u2022 m = 0   one_smul : \u2200 m : M, (1 : R) \u2022 m = m   mul_smul : \u2200 (a b : R) (m : M), (a * b) \u2022 m = a \u2022 b \u2022 m   add_smul : \u2200 (a b : R) (m : M), (a + b) \u2022 m = a \u2022 m + b \u2022 m   smul_add : \u2200 (a : R) (m n : M), a \u2022 (m + n) = a \u2022 m + a \u2022 n -- QUOTE.</p> <p>/- TEXT: There is something interesting going on here. While it isn't too surprising that the ring structure on <code>R</code> is a parameter in this definition, you probably expected <code>AddCommGroup\u2083 M</code> to be part of the <code>extends</code> clause just as <code>SMul\u2083 R M</code> is.  Trying to do that would lead to a mysterious sounding error message: <code>cannot find synthesization order for instance Module\u2081.toAddCommGroup\u2083 with type (R : Type) \u2192 [inst : Ring\u2083 R] \u2192 {M : Type} \u2192 [self : Module\u2081 R M] \u2192 AddCommGroup\u2083 M all remaining arguments have metavariables: Ring\u2083 ?R @Module\u2081 ?R ?inst\u271d M</code>. In order to understand this message, you need to remember that such an <code>extends</code> clause would lead to a field <code>Module\u2083.toAddCommGroup\u2083</code> marked as an instance. This instance would have the signature appearing in the error message: <code>(R : Type) \u2192 [inst : Ring\u2083 R] \u2192 {M : Type} \u2192 [self : Module\u2081 R M] \u2192 AddCommGroup\u2083 M</code>. With such an instance in the type class database, each time Lean would look for a <code>AddCommGroup\u2083 M</code> instance for some <code>M</code>, it would need to go hunting for a completely unspecified type <code>R</code> and a <code>Ring\u2083 R</code> instance before embarking on the main quest of finding a <code>Module\u2081 R M</code> instance. Those two side-quests are represented by the meta-variables mentioned in the error message and denoted by <code>?R</code> and <code>?inst\u271d</code> there. Such a <code>Module\u2083.toAddCommGroup\u2083</code> instance would then be a huge trap for the instance resolution procedure and then <code>class</code> command refuses to set it up.</p> <p>What about <code>extends SMul\u2083 R M</code> then? That one creates a field <code>Module\u2081.toSMul\u2083 : {R : Type} \u2192  [inst : Ring\u2083 R] \u2192 {M : Type} \u2192 [inst_1 : AddCommGroup\u2083 M] \u2192 [self : Module\u2081 R M] \u2192 SMul\u2083 R M</code> whose end result <code>SMul\u2083 R M</code> mentions both <code>R</code> and <code>M</code> so this field can safely be used as an instance. The rule is easy to remember: each class appearing in the <code>extends</code> clause should mention every type appearing in the parameters.</p> <p>Let us create our first module instance: a ring is a module over itself using its multiplication as a scalar multiplication. BOTH: -/ -- QUOTE: instance selfModule (R : Type) [Ring\u2083 R] : Module\u2081 R R where   smul := fun r s \u21a6 r*s   zero_smul := zero_mul   one_smul := one_mul   mul_smul := mul_assoc\u2083   add_smul := Ring\u2083.right_distrib   smul_add := Ring\u2083.left_distrib -- QUOTE. /- TEXT: As a second example, every abelian group is a module over <code>\u2124</code> (this is one of the reason to generalize the theory of vector spaces by allowing non-invertible scalars). First one can define scalar multiplication by a natural number for any type equipped with a zero and an addition: <code>n \u2022 a</code> is defined as <code>a + \u22ef + a</code> where <code>a</code> appears <code>n</code> times. Then this is extended to scalar multiplication by an integer by ensuring <code>(-1) \u2022 a = -a</code>. BOTH: -/ -- QUOTE:</p> <p>def nsmul\u2081 [Zero M] [Add M] : \u2115 \u2192 M \u2192 M   | 0, _ =&gt; 0   | n + 1, a =&gt; a + nsmul\u2081 n a</p> <p>def zsmul\u2081 {M : Type*} [Zero M] [Add M] [Neg M] : \u2124 \u2192 M \u2192 M   | Int.ofNat n, a =&gt; nsmul\u2081 n a   | Int.negSucc n, a =&gt; -nsmul\u2081 n.succ a -- QUOTE. /- TEXT: Proving this gives rise to a module structure is a bit tedious and not interesting for the current discussion, so we will sorry all axioms. You are not asked to replace those sorries with proofs. If you insist on doing it then you will probably want to state and prove several intermediate lemmas about <code>nsmul\u2081</code> and <code>zsmul\u2081</code>. BOTH: -/ -- QUOTE:</p> <p>instance abGrpModule (A : Type) [AddCommGroup\u2083 A] : Module\u2081 \u2124 A where   smul := zsmul\u2081   zero_smul := sorry   one_smul := sorry   mul_smul := sorry   add_smul := sorry   smul_add := sorry -- QUOTE. /- TEXT: A much more important issue is that we now have two module structures over the ring <code>\u2124</code> for <code>\u2124</code> itself: <code>abGrpModule \u2124</code> since <code>\u2124</code> is a abelian group, and <code>selfModule \u2124</code> since <code>\u2124</code> is a ring. Those two module structure correspond to the same abelian group structure, but it is not obvious that they have the same scalar multiplication. They actually do, but this isn't true by definition, it requires a proof. This is very bad news for the type class instance resolution procedure and will lead to very frustrating failures for users of this hierarchy. When directly asked to find an instance, Lean will pick one, and we can see which one using: BOTH: -/ -- QUOTE:</p>"},{"location":"MIL/C07_Hierarchies/S01_Basics/#synth-module1-z-z-abgrpmodule-z","title":"synth Module\u2081 \u2124 \u2124 -- abGrpModule \u2124","text":"<p>-- QUOTE. /- TEXT: But in a more indirect context it can happen that Lean infers the one and then gets confused. This situation is known as a bad diamond. This has nothing to do with the diamond operation we used above, it refers to the way one can draw the paths from <code>\u2124</code> to its <code>Module\u2081 \u2124</code> going through either <code>AddCommGroup\u2083 \u2124</code> or <code>Ring\u2083 \u2124</code>.</p> <p>It is important to understand that not all diamonds are bad. In fact there are diamonds everywhere in Mathlib, and also in this chapter. Already at the very beginning we saw one can go from <code>Monoid\u2081 \u03b1</code> to <code>Dia\u2081 \u03b1</code> through either <code>Semigroup\u2081 \u03b1</code> or <code>DiaOneClass\u2081 \u03b1</code> and thanks to the work done by the <code>class</code> command, the resulting two <code>Dia\u2081 \u03b1</code> instances are definitionally equal. In particular a diamond having a <code>Prop</code>-valued class at the bottom cannot be bad since any too proofs of the same statement are definitionally equal.</p> <p>But the diamond we created with modules is definitely bad. The offending piece is the <code>smul</code> field which is data, not a proof, and we have two constructions that are not definitionally equal. The robust way of fixing this issue is to make sure that going from a rich structure to a poor structure is always done by forgetting data, not by defining data. This well-known pattern as been named \"forgetful inheritance\" and extensively discussed in https://inria.hal.science/hal-02463336.</p> <p>In our concrete case, we can modify the definition of <code>AddMonoid\u2083</code> to include a <code>nsmul</code> data field and some <code>Prop</code>-valued fields ensuring this operation is provably the one we constructed above. Those fields are given default values using <code>:=</code> after their type in the definition below. Thanks to these default values, most instances would be constructed exactly as with our previous definitions. But in the special case of <code>\u2124</code> we will be able to provide specific values. BOTH: -/ -- QUOTE:</p> <p>class AddMonoid\u2084 (M : Type) extends AddSemigroup\u2083 M, AddZeroClass M where   /-- Multiplication by a natural number. -/   nsmul : \u2115 \u2192 M \u2192 M := nsmul\u2081   /-- Multiplication by <code>(0 : \u2115)</code> gives <code>0</code>. -/   nsmul_zero : \u2200 x, nsmul 0 x = 0 := by intros; rfl   /-- Multiplication by <code>(n + 1 : \u2115)</code> behaves as expected. -/   nsmul_succ : \u2200 (n : \u2115) (x), nsmul (n + 1) x = x + nsmul n x := by intros; rfl</p> <p>instance mySMul {M : Type} [AddMonoid\u2084 M] : SMul \u2115 M := \u27e8AddMonoid\u2084.nsmul\u27e9 -- QUOTE. /- TEXT:</p> <p>Let us check we can still construct a product monoid instance without providing the <code>nsmul</code> related fields. BOTH: -/ -- QUOTE:</p> <p>instance (M N : Type) [AddMonoid\u2084 M] [AddMonoid\u2084 N] : AddMonoid\u2084 (M \u00d7 N) where   add := fun p q \u21a6 (p.1 + q.1, p.2 + q.2)   add_assoc\u2083 := fun a b c \u21a6 by ext &lt;;&gt; apply add_assoc\u2083   zero := (0, 0)   zero_add := fun a \u21a6 by ext &lt;;&gt; apply zero_add   add_zero := fun a \u21a6 by ext &lt;;&gt; apply add_zero -- QUOTE. /- TEXT: And now let us handle the special case of <code>\u2124</code> where we want to build <code>nsmul</code> using the coercion of <code>\u2115</code> to <code>\u2124</code> and the multiplication on <code>\u2124</code>. Note in particular how the proof fields contain more work than in the default value above. BOTH: -/ -- QUOTE:</p> <p>instance : AddMonoid\u2084 \u2124 where   add := (\u00b7 + \u00b7)   add_assoc\u2083 := Int.add_assoc   zero := 0   zero_add := Int.zero_add   add_zero := Int.add_zero   nsmul := fun n m \u21a6 (n : \u2124) * m   nsmul_zero := Int.zero_mul   nsmul_succ := fun n m \u21a6 show (n + 1 : \u2124) * m = m + n * m     by rw [Int.add_mul, Int.add_comm, Int.one_mul] -- QUOTE. /- TEXT: Let us check we solved our issue. Because Lean already has a definition of scalar multiplication of a natural number and an integer, and we want to make sure our instance is used, we won't use the <code>\u2022</code> notation but call <code>SMul.mul</code> and explicitly provide our instance defined above. BOTH: -/ -- QUOTE:</p> <p>example (n : \u2115) (m : \u2124) : SMul.smul (self := mySMul) n m = n * m := rfl -- QUOTE. /- TEXT: This story then continues with incorporating a <code>zsmul</code> field into the definition of groups and similar tricks. You are now ready to read the definition of monoids, groups, rings and modules in Mathlib. There are more complicated than what we have seen here, because they are part of a huge hierarchy, but all principles have been explained above.</p> <p>As an exercise, you can come back to the order relation hierarchy you built above and try to incorporate a type class <code>LT\u2081</code> carrying the Less-Than notation <code>&lt;\u2081</code> and make sure that every preorder comes with a <code>&lt;\u2081</code> which has a default value built from <code>\u2264\u2081</code> and a <code>Prop</code>-valued field asserting the natural relation between those two comparison operators. -/</p> <p>-- SOLUTIONS:</p> <p>class LT\u2081 (\u03b1 : Type) where   /-- The Less-Than relation -/   lt : \u03b1 \u2192 \u03b1 \u2192 Prop</p> <p>@[inherit_doc] infix:50 \" &lt;\u2081 \" =&gt; LT\u2081.lt</p> <p>class PreOrder\u2082 (\u03b1 : Type) extends LE\u2081 \u03b1, LT\u2081 \u03b1 where   le_refl : \u2200 a : \u03b1, a \u2264\u2081 a   le_trans : \u2200 a b c : \u03b1, a \u2264\u2081 b \u2192 b \u2264\u2081 c \u2192 a \u2264\u2081 c   lt := fun a b \u21a6 a \u2264\u2081 b \u2227 \u00acb \u2264\u2081 a   lt_iff_le_not_le : \u2200 a b : \u03b1, a &lt;\u2081 b \u2194 a \u2264\u2081 b \u2227 \u00acb \u2264\u2081 a := by intros; rfl</p>"},{"location":"MIL/C07_Hierarchies/S02_Morphisms/","title":"S02 Morphisms","text":"<p>import MIL.Common import Mathlib.Topology.Instances.Real</p> <p>set_option autoImplicit true</p> <p>/- TEXT: .. _section_hierarchies_morphisms:</p>"},{"location":"MIL/C07_Hierarchies/S02_Morphisms/#morphisms","title":"Morphisms","text":"<p>So far in this chapter, we discussed how to create a hierarchy of mathematical structures. But defining structures is not really completed until we have morphisms. There are two main approaches here. The most obvious one is to define a predicate on functions. BOTH: -/</p> <p>-- QUOTE: def isMonoidHom\u2081 [Monoid G] [Monoid H] (f : G \u2192 H) : Prop :=   f 1 = 1 \u2227 \u2200 g g', f (g * g') = f g * f g' -- QUOTE. /- TEXT: In this definition, it is a bit unpleasant to use a conjunction. In particular users will need to remember the ordering we chose when they want to access the two conditions. So we could use a structure instead.</p> <p>BOTH: -/ -- QUOTE: structure isMonoidHom\u2082 [Monoid G] [Monoid H] (f : G \u2192 H) : Prop where   map_one : f 1 = 1   map_mul : \u2200 g g', f (g * g') = f g * f g' -- QUOTE. /- TEXT: Once we are here, it is even tempting to make it a class and use the type class instance resolution procedure to automatically infer <code>isMonoidHom\u2082</code> for complicated functions out of instances for simpler functions. For instance a composition of monoid morphisms is a monoid morphism and this seems like a useful instance. However such an instance would be very tricky for the resolution procedure since it would need to hunt down <code>g \u2218 f</code> everywhere. Seeing it failing in <code>g (f x)</code> would be very frustrating. More generally one must always keep in mind that recognizing which function is applied in a given expression is a very difficult problem, called the \"higher-order unification problem\". So Mathlib does not use this class approach.</p> <p>A more fundamental question is whether we use predicates as above (using either a <code>def</code> or a <code>structure</code>) or use structures bundling a function and predicates. This is partly a psychological issue. It is extremely rare to consider a function between monoids that is not a morphism. It really feels like \"monoid morphism\" is not an adjective you can assign to a bare function, it is a noun. On the other hand one can argue that a continuous function between topological spaces is really a function that happens to be continuous. This is one reason why Mathlib has a <code>Continuous</code> predicate. For instance you can write:</p> <p>BOTH: -/ -- QUOTE: example : Continuous (id : \u211d \u2192 \u211d) := continuous_id -- QUOTE. /- TEXT: We still have bundles continuous functions, which are convenient for instance to put a topology on a space of continuous functions, but they are not the primary tool to work with continuity.</p> <p>By contrast, morphisms between monoids (or other algebraic structures) are bundled as in:</p> <p>BOTH: -/ -- QUOTE: @[ext] structure MonoidHom\u2081 (G H : Type) [Monoid G] [Monoid H]  where   toFun : G \u2192 H   map_one : toFun 1 = 1   map_mul : \u2200 g g', toFun (g * g') = toFun g * toFun g'</p> <p>-- QUOTE. /- TEXT: Of course we don't want to type <code>toFun</code> everywhere so we register a coercion using the <code>CoeFun</code> type class. Its first argument is the type we want to coerce to a function. The second argument describes the target function type. In our case it is always <code>G \u2192 H</code> for every <code>f : MonoidHom\u2081 G H</code>. We also tag <code>MonoidHom\u2081.toFun</code> with the <code>coe</code> attribute to make sure it is displayed almost invisibly in the tactic state, simply by a <code>\u2191</code> prefix.</p> <p>BOTH: -/ -- QUOTE: instance [Monoid G] [Monoid H] : CoeFun (MonoidHom\u2081 G H) (fun _ \u21a6 G \u2192 H) where   coe := MonoidHom\u2081.toFun</p> <p>attribute [coe] MonoidHom\u2081.toFun -- QUOTE.</p> <p>/- TEXT: Let us check we can indeed apply a bundled monoid morphism to an element.</p> <p>BOTH: -/</p> <p>-- QUOTE: example [Monoid G] [Monoid H] (f : MonoidHom\u2081 G H) : f 1 = 1 :=  f.map_one -- QUOTE. /- TEXT: We can do the same with other kind of morphisms until we reach ring morphisms.</p> <p>BOTH: -/</p> <p>-- QUOTE: @[ext] structure AddMonoidHom\u2081 (G H : Type) [AddMonoid G] [AddMonoid H]  where   toFun : G \u2192 H   map_zero : toFun 0 = 0   map_add : \u2200 g g', toFun (g + g') = toFun g + toFun g'</p> <p>instance [AddMonoid G] [AddMonoid H] : CoeFun (AddMonoidHom\u2081 G H) (fun _ \u21a6 G \u2192 H) where   coe := AddMonoidHom\u2081.toFun</p> <p>attribute [coe] AddMonoidHom\u2081.toFun</p> <p>@[ext] structure RingHom\u2081 (R S : Type) [Ring R] [Ring S] extends MonoidHom\u2081 R S, AddMonoidHom\u2081 R S</p> <p>-- QUOTE.</p> <p>/- TEXT: There are a couple of issues about this approach. A minor one is we don't quite know where to put the <code>coe</code> attribute since the <code>RingHom\u2081.toFun</code> does not exist, the relevant function is <code>MonoidHom\u2081.toFun \u2218 RingHom\u2081.toMonoidHom\u2081</code> which is not a declaration that can be tagged with an attribute (but we could still define a <code>CoeFun  (RingHom\u2081 R S) (fun _ \u21a6 R \u2192 S)</code> instance). A much more important one is that lemmas about monoid morphisms won't directly apply to ring morphisms. This leaves the alternative of either juggling with <code>RingHom\u2081.toMonoidHom\u2081</code> each time we want to apply a monoid morphism lemma or restate every such lemmas for ring morphisms. Neither option is appealing so Mathlib uses a new hierarchy trick here. The idea is to define a type class for objects that are at least monoid morphisms, instantiate that class with both monoid morphisms and ring morphisms and use it to state every lemma. In the definition below, <code>F</code> could be <code>MonoidHom\u2081 M N</code>, or <code>RingHom\u2081 M N</code> if <code>M</code> and <code>N</code> have a ring structure.</p> <p>BOTH: -/</p> <p>-- QUOTE: class MonoidHomClass\u2081 (F : Type) (M N : Type) [Monoid M] [Monoid N] where   toFun : F \u2192 M \u2192 N   map_one : \u2200 f : F, toFun f 1 = 1   map_mul : \u2200 f g g', toFun f (g * g') = toFun f g * toFun f g' -- QUOTE.</p> <p>/- TEXT: However there is a problem with the above implementation. We haven't registered a coercion to function instance yet. Let us try to do it now.</p> <p>BOTH: -/</p> <p>-- QUOTE: def badInst [Monoid M] [Monoid N] [MonoidHomClass\u2081 F M N] : CoeFun F (fun _ \u21a6 M \u2192 N) where   coe := MonoidHomClass\u2081.toFun -- QUOTE.</p> <p>/- TEXT: Making this an instance would be bad. When faced with something like <code>f x</code> where the type of <code>f</code> is not a function type, Lean will try to find a <code>CoeFun</code> instance to coerce <code>f</code> into a function. The above function has type: <code>{M N F : Type} \u2192 [Monoid M] \u2192 [Monoid N] \u2192 [MonoidHomClass\u2081 F M N] \u2192 CoeFun F (fun x \u21a6 M \u2192 N)</code> so, when it trying to apply it, it wouldn't be a priori clear to Lean in which order the unknown types <code>M</code>, <code>N</code> and <code>F</code> should be inferred. This is a kind of bad instance that is slightly different from the one we saw already, but it boils down to the same issue: without knowing <code>M</code>, Lean would have to search for a monoid instance on an unknown type, hence hopelessly try every monoid instance in the database. If you are curious to see the effect of such an instance you can type <code>set_option synthInstance.checkSynthOrder false in</code> on top of the above declaration, replace <code>def badInst</code> with <code>instance</code>, and look for random failures in this file.</p> <p>Here the solution is easy, we need to tell Lean to first search what is <code>F</code> and then deduce <code>M</code> and <code>N</code>. This is done using the <code>outParam</code> function. This function is defined as the identity function, but is still recognized by the type class machinery and triggers the desired behavior. Hence we can retry defining our class, paying attention to the <code>outParam</code> function: BOTH: -/</p> <p>-- QUOTE: class MonoidHomClass\u2082 (F : Type) (M N : outParam Type) [Monoid M] [Monoid N] where   toFun : F \u2192 M \u2192 N   map_one : \u2200 f : F, toFun f 1 = 1   map_mul : \u2200 f g g', toFun f (g * g') = toFun f g * toFun f g'</p> <p>instance [Monoid M] [Monoid N] [MonoidHomClass\u2082 F M N] : CoeFun F (fun _ \u21a6 M \u2192 N) where   coe := MonoidHomClass\u2082.toFun</p> <p>attribute [coe] MonoidHomClass\u2082.toFun -- QUOTE.</p> <p>/- TEXT: Now we can proceed with our plan to instantiate this class.</p> <p>BOTH: -/</p> <p>-- QUOTE: instance (M N : Type) [Monoid M] [Monoid N] : MonoidHomClass\u2082 (MonoidHom\u2081 M N) M N where   toFun := MonoidHom\u2081.toFun   map_one := fun f \u21a6 f.map_one   map_mul := fun f \u21a6 f.map_mul</p> <p>instance (R S : Type) [Ring R] [Ring S] : MonoidHomClass\u2082 (RingHom\u2081 R S) R S where   toFun := fun f \u21a6 f.toMonoidHom\u2081.toFun   map_one := fun f \u21a6 f.toMonoidHom\u2081.map_one   map_mul := fun f \u21a6 f.toMonoidHom\u2081.map_mul -- QUOTE.</p> <p>/- TEXT: As promised every lemma we prove about <code>f : F</code> assuming an instance of <code>MonoidHomClass\u2081 F</code> will apply both to monoid morphisms and ring morphisms. Let us see an example lemma and check it applies to both situations. BOTH: -/</p> <p>-- QUOTE: lemma map_inv_of_inv [Monoid M] [Monoid N] [MonoidHomClass\u2082 F M N] (f : F) {m m' : M} (h : m*m' = 1) :     f m * f m' = 1 := by   rw [\u2190 MonoidHomClass\u2082.map_mul, h, MonoidHomClass\u2082.map_one]</p> <p>example [Monoid M] [Monoid N] (f : MonoidHom\u2081 M N) {m m' : M} (h : m*m' = 1) : f m * f m' = 1 := map_inv_of_inv f h</p> <p>example [Ring R] [Ring S] (f : RingHom\u2081 R S) {r r' : R} (h : r*r' = 1) : f r * f r' = 1 := map_inv_of_inv f h</p> <p>-- QUOTE.</p> <p>/- TEXT: At first sight, it may look like we got back to our old bad idea of making <code>MonoidHom\u2081</code> a class. But we haven't. Everything is shifted one level of abstraction up. The type class resolution procedure won't be looking for functions, it will be looking for either <code>MonoidHom\u2081</code> or <code>RingHom\u2081</code>.</p> <p>One remaining issue with our approach is the presence of repetitive code around the <code>toFun</code> field and the corresponding <code>CoeFun</code> instance and <code>coe</code> attribute. It would also be better to record that this pattern is used only for functions with extra properties, meaning that the coercion to functions should be injective. So Mathlib adds one more layer of abstraction with the base class <code>DFunLike</code> (where \u201cDFun\u201d stands for dependent function). Let us redefine our <code>MonoidHomClass</code> on top of this base layer.</p> <p>BOTH: -/</p> <p>-- QUOTE: class MonoidHomClass\u2083 (F : Type) (M N : outParam Type) [Monoid M] [Monoid N] extends     DFunLike F M (fun _ \u21a6 N) where   map_one : \u2200 f : F, f 1 = 1   map_mul : \u2200 (f : F) g g', f (g * g') = f g * f g'</p> <p>instance (M N : Type) [Monoid M] [Monoid N] : MonoidHomClass\u2083 (MonoidHom\u2081 M N) M N where   coe := MonoidHom\u2081.toFun   coe_injective' := MonoidHom\u2081.ext   map_one := MonoidHom\u2081.map_one   map_mul := MonoidHom\u2081.map_mul -- QUOTE.</p> <p>/- TEXT: Of course the hierarchy of morphisms does not stop here. We could go on and define a class <code>RingHomClass\u2083</code> extending <code>MonoidHomClass\u2083</code> and instantiate it on <code>RingHom</code> and then later on <code>AlgebraHom</code> (algebras are rings with some extra structure). But we've covered the main formalization ideas used in Mathlib for morphisms and you should be ready to understand how morphisms are defined in Mathlib.</p> <p>As an exercise, you should try to define your class of bundled order-preserving function between ordered types, and then order preserving monoid morphisms. This is for training purposes only. Like continuous functions, order preserving functions are primarily unbundled in Mathlib where they are defined by the <code>Monotone</code> predicate. Of course you need to complete the class definitions below. BOTH: -/</p> <p>-- QUOTE: @[ext] structure OrderPresHom (\u03b1 \u03b2 : Type) [LE \u03b1] [LE \u03b2] where   toFun : \u03b1 \u2192 \u03b2   le_of_le : \u2200 a a', a \u2264 a' \u2192 toFun a \u2264 toFun a'</p> <p>@[ext] structure OrderPresMonoidHom (M N : Type) [Monoid M] [LE M] [Monoid N] [LE N] extends MonoidHom\u2081 M N, OrderPresHom M N</p> <p>class OrderPresHomClass (F : Type) (\u03b1 \u03b2 : outParam Type) [LE \u03b1] [LE \u03b2] -- SOLUTIONS: extends DFunLike F \u03b1 (fun _ \u21a6 \u03b2) where   le_of_le : \u2200 (f : F) a a', a \u2264 a' \u2192 f a \u2264 f a' -- BOTH:</p> <p>instance (\u03b1 \u03b2 : Type) [LE \u03b1] [LE \u03b2] : OrderPresHomClass (OrderPresHom \u03b1 \u03b2) \u03b1 \u03b2 where -- SOLUTIONS:   coe := OrderPresHom.toFun   coe_injective' := OrderPresHom.ext   le_of_le := OrderPresHom.le_of_le -- BOTH:</p> <p>instance (\u03b1 \u03b2 : Type) [LE \u03b1] [Monoid \u03b1] [LE \u03b2] [Monoid \u03b2] :     OrderPresHomClass (OrderPresMonoidHom \u03b1 \u03b2) \u03b1 \u03b2 where -- SOLUTIONS:   coe := fun f \u21a6 f.toOrderPresHom.toFun   coe_injective' := OrderPresMonoidHom.ext   le_of_le := fun f \u21a6 f.toOrderPresHom.le_of_le -- BOTH:</p> <p>instance (\u03b1 \u03b2 : Type) [LE \u03b1] [Monoid \u03b1] [LE \u03b2] [Monoid \u03b2] :     MonoidHomClass\u2083 (OrderPresMonoidHom \u03b1 \u03b2) \u03b1 \u03b2 /- EXAMPLES:   := sorry SOLUTIONS: -/ where   coe := fun f \u21a6 f.toOrderPresHom.toFun   coe_injective' := OrderPresMonoidHom.ext   map_one := fun f \u21a6 f.toMonoidHom\u2081.map_one   map_mul := fun f \u21a6 f.toMonoidHom\u2081.map_mul -- QUOTE.</p>"},{"location":"MIL/C07_Hierarchies/S03_Subobjects/","title":"S03 Subobjects","text":"<p>import MIL.Common import Mathlib.GroupTheory.QuotientGroup</p> <p>set_option autoImplicit true</p> <p>/- TEXT: .. _section_hierarchies_subobjects:</p>"},{"location":"MIL/C07_Hierarchies/S03_Subobjects/#sub-objects","title":"Sub-objects","text":"<p>After defining some algebraic structure and its morphisms, the next step is to consider sets that inherit this algebraic structure, for instance subgroups or subrings. This largely overlaps with our previous topic. Indeed a set in <code>X</code> is implemented as a function from <code>X</code> to <code>Prop</code> so sub-objects are function satisfying a certain predicate. Hence we can reuse of lot of the ideas that led to the <code>DFunLike</code> class and its descendants. We won't reuse <code>DFunLike</code> itself because this would break the abstraction barrier from <code>Set X</code> to <code>X \u2192 Prop</code>. Instead there is a <code>SetLike</code> class. Instead of wrapping an injection into a function type, that class wraps an injection into a <code>Set</code> type and defines the corresponding coercion and <code>Membership</code> instance.</p> <p>BOTH: -/</p> <p>-- QUOTE: @[ext] structure Submonoid\u2081 (M : Type) [Monoid M] where   /-- The carrier of a submonoid. -/   carrier : Set M   /-- The product of two elements of a submonoid belongs to the submonoid. -/   mul_mem {a b} : a \u2208 carrier \u2192 b \u2208 carrier \u2192 a * b \u2208 carrier   /-- The unit element belongs to the submonoid. -/   one_mem : 1 \u2208 carrier</p> <p>/-- Submonoids in <code>M</code> can be seen as sets in <code>M</code>. -/ instance [Monoid M] : SetLike (Submonoid\u2081 M) M where   coe := Submonoid\u2081.carrier   coe_injective' := Submonoid\u2081.ext</p> <p>-- QUOTE.</p> <p>/- TEXT: Equipped with the above <code>SetLike</code> instance, we can already state naturally that a submonoid <code>N</code> contains <code>1</code> without using <code>N.carrier</code>. We can also silently treat <code>N</code> as a set in <code>M</code> as take its direct image under a map. BOTH: -/</p> <p>-- QUOTE: example [Monoid M] (N : Submonoid\u2081 M) : 1 \u2208 N := N.one_mem</p> <p>example [Monoid M] (N : Submonoid\u2081 M) (\u03b1 : Type) (f : M \u2192 \u03b1) := f '' N -- QUOTE.</p> <p>/- TEXT: We also have a coercion to <code>Type</code> which uses <code>Subtype</code> so, given a submonoid <code>N</code> we can write a parameter <code>(x : N)</code> which can be coerced to an element of <code>M</code> belonging to <code>N</code>.</p> <p>BOTH: -/</p> <p>-- QUOTE: example [Monoid M] (N : Submonoid\u2081 M) (x : N) : (x : M) \u2208 N := x.property -- QUOTE.</p> <p>/- TEXT: Using this coercion to <code>Type</code> we can also tackle the task of equipping a submonoid with a monoid structure. We will use the coercion from the type associated to <code>N</code> as above, and the lemma <code>SetCoe.ext</code> asserting this coercion is injective. Both are provided by the <code>SetLike</code> instance.</p> <p>BOTH: -/</p> <p>-- QUOTE: instance SubMonoid\u2081Monoid [Monoid M] (N : Submonoid\u2081 M) : Monoid N where   mul := fun x y \u21a6 \u27e8x*y, N.mul_mem x.property y.property\u27e9   mul_assoc := fun x y z \u21a6 SetCoe.ext (mul_assoc (x : M) y z)   one := \u27e81, N.one_mem\u27e9   one_mul := fun x \u21a6 SetCoe.ext (one_mul (x : M))   mul_one := fun x \u21a6 SetCoe.ext (mul_one (x : M)) -- QUOTE.</p> <p>/- TEXT: Note that, in the above instance, instead of using the coercion to <code>M</code> and calling the <code>property</code> field, we could have used destructuring binders as follows.</p> <p>BOTH: -/</p> <p>-- QUOTE: example [Monoid M] (N : Submonoid\u2081 M) : Monoid N where   mul := fun \u27e8x, hx\u27e9 \u27e8y, hy\u27e9 \u21a6 \u27e8x*y, N.mul_mem hx hy\u27e9   mul_assoc := fun \u27e8x, _\u27e9 \u27e8y, _\u27e9 \u27e8z, _\u27e9 \u21a6 SetCoe.ext (mul_assoc x y z)   one := \u27e81, N.one_mem\u27e9   one_mul := fun \u27e8x, _\u27e9 \u21a6 SetCoe.ext (one_mul x)   mul_one := fun \u27e8x, _\u27e9 \u21a6 SetCoe.ext (mul_one x) -- QUOTE.</p> <p>/- TEXT:</p> <p>In order to apply lemmas about submonoids to subgroups or subrings, we need a class, just like for morphisms. Note this class take a <code>SetLike</code> instance as a parameter so it does not need a carrier field and can use the membership notation in its fields. BOTH: -/</p> <p>-- QUOTE: class SubmonoidClass\u2081 (S : Type) (M : Type) [Monoid M] [SetLike S M] : Prop where   mul_mem : \u2200 (s : S) {a b : M}, a \u2208 s \u2192 b \u2208 s \u2192 a * b \u2208 s   one_mem : \u2200 s : S, 1 \u2208 s</p> <p>instance [Monoid M] : SubmonoidClass\u2081 (Submonoid\u2081 M) M where   mul_mem := Submonoid\u2081.mul_mem   one_mem := Submonoid\u2081.one_mem -- QUOTE.</p> <p>/- TEXT:</p> <p>As an exercise you should define a <code>Subgroup\u2081</code> structure, endow it with a <code>SetLike</code> instance and a <code>SubmonoidClass\u2081</code> instance, put a <code>Group</code> instance on the subtype associated to a <code>Subgroup\u2081</code> and define a <code>SubgroupClass\u2081</code> class.</p> <p>SOLUTIONS: -/ @[ext] structure Subgroup\u2081 (G : Type) [Group G] extends Submonoid\u2081 G where   /-- The inverse of an element of a subgroup belongs to the subgroup. -/   inv_mem {a} : a \u2208 carrier \u2192 a\u207b\u00b9 \u2208 carrier</p> <p>/-- Subgroups in <code>M</code> can be seen as sets in <code>M</code>. -/ instance [Group G] : SetLike (Subgroup\u2081 G) G where   coe := fun H \u21a6 H.toSubmonoid\u2081.carrier   coe_injective' := Subgroup\u2081.ext</p> <p>instance [Group G] (H : Subgroup\u2081 G) : Group H := { SubMonoid\u2081Monoid H.toSubmonoid\u2081 with   inv := fun x \u21a6 \u27e8x\u207b\u00b9, H.inv_mem x.property\u27e9   mul_left_inv := fun x \u21a6 SetCoe.ext (mul_left_inv (x : G)) }</p> <p>class SubgroupClass\u2081 (S : Type) (G : Type) [Group G] [SetLike S G]     extends SubmonoidClass\u2081 S G  : Prop where   inv_mem : \u2200 (s : S) {a : G}, a \u2208 s \u2192 a\u207b\u00b9 \u2208 s</p> <p>instance [Group G] : SubmonoidClass\u2081 (Subgroup\u2081 G) G where   mul_mem := fun H \u21a6 H.toSubmonoid\u2081.mul_mem   one_mem := fun H \u21a6 H.toSubmonoid\u2081.one_mem</p> <p>instance [Group G] : SubgroupClass\u2081 (Subgroup\u2081 G) G := { (inferInstance : SubmonoidClass\u2081 (Subgroup\u2081 G) G) with   inv_mem := Subgroup\u2081.inv_mem }</p> <p>/- TEXT: Another very important thing to know about subobjects of a given algebraic object in Mathlib always form a complete lattice, and this structure is used a lot. For instance you may look for the lemma saying that an intersection of submonoids is a submonoid. But this won't be a lemma, this will be an infimum construction. Let us do the case of two submonoids.</p> <p>BOTH: -/</p> <p>-- QUOTE: instance [Monoid M] : Inf (Submonoid\u2081 M) :=   \u27e8fun S\u2081 S\u2082 \u21a6     { carrier := S\u2081 \u2229 S\u2082       one_mem := \u27e8S\u2081.one_mem, S\u2082.one_mem\u27e9       mul_mem := fun \u27e8hx, hx'\u27e9 \u27e8hy, hy'\u27e9 \u21a6 \u27e8S\u2081.mul_mem hx hy, S\u2082.mul_mem hx' hy'\u27e9 }\u27e9 -- QUOTE.</p> <p>/- TEXT: This allows to get the intersections of two submonoids as a submonoid.</p> <p>BOTH: -/</p> <p>-- QUOTE: example [Monoid M] (N P : Submonoid\u2081 M) : Submonoid\u2081 M := N \u2293 P -- QUOTE.</p> <p>/- TEXT: You may think it's a shame that we had to use the inf symbol <code>\u2293</code> in the above example instead of the intersection symbol <code>\u2229</code>. But think about the supremum. The union of two submonoids is not a submonoid. However submonoids still form a lattice (even a complete one). Actually <code>N \u2294 P</code> is the submonoid generated by the union of <code>N</code> and <code>P</code> and of course it would be very confusing to denote it by <code>N \u222a P</code>. So you can see the use of <code>N \u2293 P</code> as much more consistent. It is also a lot more consistent across various kind of algebraic structures. It may look a bit weird at first to see the sum of two vector subspace <code>E</code> and <code>F</code> denoted by <code>E \u2294 F</code> instead of <code>E + F</code>. But you will get used to it. And soon you will consider the <code>E + F</code> notation as a distraction emphasizing the anecdotal fact that elements of <code>E \u2294 F</code> can be written as a sum of an element of <code>E</code> and an element of <code>F</code> instead of emphasizing the fundamental fact that <code>E \u2294 F</code> is the smallest vector subspace containing both <code>E</code> and <code>F</code>.</p> <p>Our last topic for this chapter is that of quotients. Again we want to explain how convenient notation are built and code duplication is avoided in Mathlib. Here the main device is the <code>HasQuotient</code> class which allows notations like <code>M \u29f8 N</code>. Beware the quotient symbol <code>\u29f8</code> is a special unicode character, not a regular ASCII division symbol.</p> <p>As an example, we will build the quotient of a commutative monoid by a submonoid, leave proofs to you. In the last example, you can use <code>Setoid.refl</code> but it won't automatically pick up the relevant <code>Setoid</code> structure. You can fix this issue by providing all arguments using the <code>@</code> syntax, as in <code>@Setoid.refl M N.Setoid</code>.</p> <p>BOTH: -/</p> <p>-- QUOTE: def Submonoid.Setoid [CommMonoid M] (N : Submonoid M) : Setoid M  where   r := fun x y \u21a6 \u2203 w \u2208 N, \u2203 z \u2208 N, x*w = y*z   iseqv := {     refl := fun x \u21a6 \u27e81, N.one_mem, 1, N.one_mem, rfl\u27e9     symm := fun \u27e8w, hw, z, hz, h\u27e9 \u21a6 \u27e8z, hz, w, hw, h.symm\u27e9     trans := by /- EXAMPLES:       sorry SOLUTIONS: -/       rintro a b c \u27e8w, hw, z, hz, h\u27e9 \u27e8w', hw', z', hz', h'\u27e9       refine \u27e8w*w', N.mul_mem hw hw', z*z', N.mul_mem hz hz', ?_\u27e9       rw [\u2190 mul_assoc, h, mul_comm b, mul_assoc, h', \u2190 mul_assoc, mul_comm z, mul_assoc] -- BOTH:   }</p> <p>instance [CommMonoid M] : HasQuotient M (Submonoid M) where   quotient' := fun N \u21a6 Quotient N.Setoid</p> <p>def QuotientMonoid.mk [CommMonoid M] (N : Submonoid M) : M \u2192 M \u29f8 N := Quotient.mk N.Setoid</p> <p>instance [CommMonoid M] (N : Submonoid M) : Monoid (M \u29f8 N) where   mul := Quotient.map\u2082' (\u00b7 * \u00b7) (by /- EXAMPLES:       sorry SOLUTIONS: -/     rintro a\u2081 b\u2081 \u27e8w, hw, z, hz, ha\u27e9 a\u2082 b\u2082 \u27e8w', hw', z', hz', hb\u27e9     refine \u27e8w*w', N.mul_mem hw hw', z*z', N.mul_mem hz hz', ?_\u27e9     rw [mul_comm w, \u2190 mul_assoc, mul_assoc a\u2081, hb, mul_comm, \u2190 mul_assoc, mul_comm w, ha,         mul_assoc, mul_comm z, mul_assoc b\u2082, mul_comm z', mul_assoc] -- BOTH:         )   mul_assoc := by /- EXAMPLES:       sorry SOLUTIONS: -/     rintro \u27e8a\u27e9 \u27e8b\u27e9 \u27e8c\u27e9     apply Quotient.sound     dsimp only     rw [mul_assoc]     apply @Setoid.refl M N.Setoid -- BOTH:   one := QuotientMonoid.mk N 1   one_mul := by /- EXAMPLES:       sorry SOLUTIONS: -/     rintro \u27e8a\u27e9 ; apply Quotient.sound ; dsimp only ; rw [one_mul] ; apply @Setoid.refl M N.Setoid -- BOTH:   mul_one := by /- EXAMPLES:       sorry SOLUTIONS: -/     rintro \u27e8a\u27e9 ; apply Quotient.sound ; dsimp only ; rw [mul_one] ; apply @Setoid.refl M N.Setoid -- QUOTE.</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/","title":"S01 Groups","text":"<p>-- BOTH: import Mathlib.GroupTheory.Sylow import Mathlib.GroupTheory.Perm.Cycle.Concrete import Mathlib.GroupTheory.Perm.Subgroup import Mathlib.GroupTheory.PresentedGroup</p> <p>import MIL.Common</p> <p>/- TEXT: .. _groups:</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#monoids-and-groups","title":"Monoids and Groups","text":"<p>.. index:: monoid .. index:: group (algebraic structure)</p> <p>Monoids and their morphisms <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>Courses in abstract algebra often start with groups and then progress to rings, fields, and vector spaces. This involves some contortions when discussing multiplication on rings since the multiplication operation does not come from a group structure but many of the proofs carry over verbatim from group theory to this new setting. The most common fix, when doing mathematics with pen and paper, is to leave those proofs as exercises. A less efficient but safer and more formalization-friendly way of proceeding is to use monoids. A monoid structure on a type <code>M</code> is an internal composition law that is associative and has a neutral element. Monoids are used primarily to accommodate both groups and the multiplicative structure of rings. But there are also a number of natural examples; for instance, the set of natural numbers equipped with addition forms a monoid.</p> <p>From a practical point of view, you can mostly ignore monoids when using Mathlib. But you need to know they exist when you are looking for a lemma by browsing Mathlib files. Otherwise, you might end up looking for a statement in the group theory files when it is actually in the found with monoids because it does not require elements to be invertible.</p> <p>The type of monoid structures on a type <code>M</code> is written <code>Monoid M</code>. The function <code>Monoid</code> is a type class so it will almost always appear as an instance implicit argument (in other words, in square brackets). By default, <code>Monoid</code> uses multiplicative notation for the operation; for additive notation use <code>AddMonoid</code> instead. The commutative versions of these structures add the prefix <code>Comm</code> before <code>Monoid</code>. EXAMPLES: -/ -- QUOTE: example {M : Type*} [Monoid M] (x : M) : x * 1 = x := mul_one x</p> <p>example {M : Type*} [AddCommMonoid M] (x y : M) : x + y = y + x := add_comm x y -- QUOTE.</p> <p>/- TEXT: Note that although <code>AddMonoid</code> is found in the library, it is generally confusing to use additive notation with a non-commutative operation.</p> <p>The type of morphisms between monoids <code>M</code> and <code>N</code> is called <code>MonoidHom M N</code> and written <code>M \u2192* N</code>. Lean will automatically see such a morphism as a function from <code>M</code> to <code>N</code> when we apply it to elements of <code>M</code>. The additive version is called <code>AddMonoidHom</code> and written <code>M \u2192+ N</code>. EXAMPLES: -/ -- QUOTE: example {M N : Type*} [Monoid M] [Monoid N] (x y : M) (f : M \u2192* N) : f (x * y) = f x * f y :=   f.map_mul x y</p> <p>example {M N : Type*} [AddMonoid M] [AddMonoid N] (f : M \u2192+ N) : f 0 = 0 :=   f.map_zero -- QUOTE.</p> <p>/- TEXT: These morphisms are bundled maps, i.e. they package together a map and some of its properties. Remember that :numref:<code>section_hierarchies_morphisms</code> explains bundled maps; here we simply note the slightly unfortunate consequence that we cannot use ordinary function composition to compose maps. Instead, we need to use <code>MonoidHom.comp</code> and <code>AddMonoidHom.comp</code>. EXAMPLES: -/ -- QUOTE: example {M N P : Type*} [AddMonoid M] [AddMonoid N] [AddMonoid P]     (f : M \u2192+ N) (g : N \u2192+ P) : M \u2192+ P := g.comp f -- QUOTE.</p> <p>/- TEXT: Groups and their morphisms <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>We will have much more to say about groups, which are monoids with the extra property that every element has an inverse. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (x : G) : x * x\u207b\u00b9 = 1 := mul_inv_self x -- QUOTE.</p> <p>/- TEXT:</p> <p>.. index:: group (tactic), tactics ; group</p> <p>Similar to the <code>ring</code> tactic that we saw earlier, there is a <code>group</code> tactic that proves any identity that holds in any group. (Equivalently, it proves the identities that hold in free groups.)</p> <p>EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (x y z : G) : x * (y * z) * (x * z)\u207b\u00b9 * (x * y * x\u207b\u00b9)\u207b\u00b9 = 1 := by   group -- QUOTE.</p> <p>/- TEXT: .. index:: abel, tactics ; abel</p> <p>There is also a tactic for identities in commutative additive groups called <code>abel</code>.</p> <p>EXAMPLES: -/ -- QUOTE: example {G : Type*} [AddCommGroup G] (x y z : G) : z + x + (y - z - x) = y := by   abel -- QUOTE.</p> <p>/- TEXT: Interestingly, a group morphism is nothing more than a monoid morphism between groups. So we can copy and paste one of our earlier examples, replacing <code>Monoid</code> with <code>Group</code>. EXAMPLES: -/ -- QUOTE: example {G H : Type*} [Group G] [Group H] (x y : G) (f : G \u2192* H) : f (x * y) = f x * f y :=   f.map_mul x y -- QUOTE.</p> <p>/- TEXT: Of course we do get some new properties, such as this one: EXAMPLES: -/ -- QUOTE: example {G H : Type*} [Group G] [Group H] (x : G) (f : G \u2192* H) : f (x\u207b\u00b9) = (f x)\u207b\u00b9 :=   f.map_inv x -- QUOTE.</p> <p>/- TEXT: You may be worried that constructing group morphisms will require us to do unnecessary work since the definition of monoid morphism enforces that neutral elements are sent to neutral elements while this is automatic in the case of group morphisms. In practice the extra work is not hard, but, to avoid it, there is a function building a group morphism from a function between groups that is compatible with the composition laws. EXAMPLES: -/ -- QUOTE: example {G H : Type*} [Group G] [Group H] (f : G \u2192 H) (h : \u2200 x y, f (x * y) = f x * f y) :     G \u2192* H :=   MonoidHom.mk' f h -- QUOTE.</p> <p>/- TEXT: There is also a type <code>MulEquiv</code> of group (or monoid) isomorphisms denoted by <code>\u2243*</code> (and <code>AddEquiv</code> denoted by <code>\u2243+</code> in additive notation). The inverse of <code>f : G \u2243* H</code> is <code>MulEquiv.symm f : H \u2243* G</code>, composition of <code>f</code> and <code>g</code> is <code>MulEquiv.trans f g</code>, and the identity isomorphism of <code>G</code> is <code>M\u0300ulEquiv.refl G</code>. Using anonymous projector notation, the first two can be written <code>f.symm</code> and <code>f.trans g</code> respectively. Elements of this type are automatically coerced to morphisms and functions when necessary. EXAMPLES: -/ -- QUOTE: example {G H : Type*} [Group G] [Group H] (f : G \u2243* H) :     f.trans f.symm = MulEquiv.refl G :=   f.self_trans_symm -- QUOTE.</p> <p>/- TEXT: One can use <code>MulEquiv.ofBijective</code> to build an isomorphism from a bijective morphism. Doing so makes the inverse function noncomputable. EXAMPLES: -/ -- QUOTE: noncomputable example {G H : Type*} [Group G] [Group H]     (f : G \u2192* H) (h : Function.Bijective f) :     G \u2243* H :=   MulEquiv.ofBijective f h -- QUOTE.</p> <p>/- TEXT: Subgroups <sup>^</sup><sup>^</sup><sup>^</sup></p> <p>Just as group morphisms are bundled, a subgroup of <code>G</code> is also a bundled structure consisting of a set in <code>G</code> with the relevant closure properties. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (H : Subgroup G) {x y : G} (hx : x \u2208 H) (hy : y \u2208 H) :     x * y \u2208 H :=   H.mul_mem hx hy</p> <p>example {G : Type*} [Group G] (H : Subgroup G) {x : G} (hx : x \u2208 H) :     x\u207b\u00b9 \u2208 H :=   H.inv_mem hx -- QUOTE.</p> <p>/- TEXT: In the example above, it is important to understand that <code>Subgroup G</code> is the type of subgroups of <code>G</code>, rather than a predicate <code>IsSubgroup H</code> where <code>H</code> is an element of <code>Set G</code>. <code>Subgroup G</code> is endowed with a coercion to <code>Set G</code> and a membership predicate on <code>G</code>. See :numref:<code>section_hierarchies_subobjects</code> for an explanation of how and why this is done.</p> <p>Of course, two subgroups are the same if and only if they have the same elements. This fact is registered for use with the <code>ext</code> tactic, which can be used to prove two subgroups are equal in the same way it is used to prove that two sets are equal.</p> <p>To state and prove, for example, that <code>\u2124</code> is an additive subgroup of <code>\u211a</code>, what we really want is to construct a term of type <code>AddSubgroup \u211a</code> whose projection to <code>Set \u211a</code> is <code>\u2124</code>, or, more precisely, the image of <code>\u2124</code> in <code>\u211a</code>. EXAMPLES: -/ -- QUOTE: example : AddSubgroup \u211a where   carrier := Set.range ((\u2191) : \u2124 \u2192 \u211a)   add_mem' := by     rintro _ _ \u27e8n, rfl\u27e9 \u27e8m, rfl\u27e9     use n + m     simp   zero_mem' := by     use 0     simp   neg_mem' := by     rintro _ \u27e8n, rfl\u27e9     use -n     simp -- QUOTE.</p> <p>/- TEXT: Using type classes, Mathlib knows that a subgroup of a group inherits a group structure. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (H : Subgroup G) : Group H := inferInstance -- QUOTE.</p> <p>/- TEXT: This example is subtle. The object <code>H</code> is not a type, but Lean automatically coerces it to a type by interpreting it as a subtype of <code>G</code>. So the above example can be restated more explicitly as: EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (H : Subgroup G) : Group {x : G // x \u2208 H} := inferInstance -- QUOTE.</p> <p>/- TEXT: An important benefit of having a type <code>Subgroup G</code> instead of a predicate <code>IsSubgroup : Set G \u2192 Prop</code> is that one can easily endow <code>Subgroup G</code> with additional structure. Importantly, it has the structure of a complete lattice structure with respect to inclusion. For instance, instead of having a lemma stating that an intersection of two subgroups of <code>G</code> is again a subgroup, we have used the lattice operation <code>\u2293</code> to construct the intersection. We can then apply arbitrary lemmas about lattices to the construction.</p> <p>Let us check that the set underlying the infimum of two subgroups is indeed, by definition, their intersection. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (H H' : Subgroup G) :     ((H \u2293 H' : Subgroup G) : Set G) = (H : Set G) \u2229 (H' : Set G) := rfl -- QUOTE.</p> <p>/- TEXT: It may look strange to have a different notation for what amounts to the intersection of the underlying sets, but the correspondence does not carry over to the supremum operation and set union, since a union of subgroups is not, in general, a subgroup. Instead one needs to use the subgroup generated by the union, which is done using <code>Subgroup.closure</code>. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (H H' : Subgroup G) :     ((H \u2294 H' : Subgroup G) : Set G) = Subgroup.closure ((H : Set G) \u222a (H' : Set G)) := by   rw [Subgroup.sup_eq_closure] -- QUOTE.</p> <p>/- TEXT: Another subtlety is that <code>G</code> itself does not have type <code>Subgroup G</code>, so we need a way to talk about <code>G</code> seen as a subgroup of <code>G</code>. This is also provided by the lattice structure: the full subgroup is the top element of this lattice. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (x : G) : x \u2208 (\u22a4 : Subgroup G) := trivial -- QUOTE.</p> <p>/- TEXT: Similarly the bottom element of this lattice is the subgroup whose only element is the neutral element. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (x : G) : x \u2208 (\u22a5 : Subgroup G) \u2194 x = 1 := Subgroup.mem_bot -- QUOTE.</p> <p>/- TEXT: As an exercise in manipulating groups and subgroups, you can define the conjugate of a subgroup by an element of the ambient group. BOTH: -/ -- QUOTE: def conjugate {G : Type*} [Group G] (x : G) (H : Subgroup G) : Subgroup G where   carrier := {a : G | \u2203 h, h \u2208 H \u2227 a = x * h * x\u207b\u00b9}   one_mem' := by /- EXAMPLES:     dsimp     sorry SOLUTIONS: -/     dsimp     use 1     constructor     exact H.one_mem     group -- BOTH:   inv_mem' := by /- EXAMPLES:     dsimp     sorry SOLUTIONS: -/     dsimp     rintro - \u27e8h, h_in, rfl\u27e9     use h\u207b\u00b9, H.inv_mem h_in     group -- BOTH:   mul_mem' := by /- EXAMPLES:     dsimp     sorry SOLUTIONS: -/     dsimp     rintro - - \u27e8h, h_in, rfl\u27e9 \u27e8k, k_in, rfl\u27e9     use h*k, H.mul_mem h_in k_in     group -- BOTH: -- QUOTE.</p> <p>/- TEXT: Tying the previous two topics together, one can push forward and pull back subgroups using group morphisms. The naming convention in Mathlib is to call those operations <code>map</code> and <code>comap</code>. These are not the common mathematical terms, but they have the advantage of being shorter than \"pushforward\" and \"direct image.\" EXAMPLES: -/ -- QUOTE: example {G H : Type*} [Group G] [Group H] (G' : Subgroup G) (f : G \u2192* H) : Subgroup H :=   Subgroup.map f G'</p> <p>example {G H : Type*} [Group G] [Group H] (H' : Subgroup H) (f : G \u2192* H) : Subgroup G :=   Subgroup.comap f H'</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-subgroupmem_map","title":"check Subgroup.mem_map","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-subgroupmem_comap","title":"check Subgroup.mem_comap","text":"<p>-- QUOTE.</p> <p>/- TEXT: In particular, the preimage of the bottom subgroup under a morphism <code>f</code> is a subgroup called the kernel of <code>f</code>, and the range of <code>f</code> is also a subgroup. EXAMPLES: -/ -- QUOTE: example {G H : Type*} [Group G] [Group H] (f : G \u2192* H) (g : G) :     g \u2208 MonoidHom.ker f \u2194 f g = 1 :=   f.mem_ker</p> <p>example {G H : Type*} [Group G] [Group H] (f : G \u2192* H) (h : H) :     h \u2208 MonoidHom.range f \u2194 \u2203 g : G, f g = h :=   f.mem_range -- QUOTE.</p> <p>/- TEXT: As exercises in manipulating group morphisms and subgroups, let us prove some elementary properties. They are already proved in Mathlib, so do not use <code>exact?</code> too quickly if you want to benefit from these exercises. BOTH: -/ -- QUOTE: section exercises variable {G H : Type*} [Group G] [Group H]</p> <p>open Subgroup</p> <p>example (\u03c6 : G \u2192* H) (S T : Subgroup H) (hST : S \u2264 T) : comap \u03c6 S \u2264 comap \u03c6 T := by /- EXAMPLES:   sorry SOLUTIONS: -/   intro x hx   rw [mem_comap] at * -- Lean does not need this line   exact hST hx -- BOTH:</p> <p>example (\u03c6 : G \u2192* H) (S T : Subgroup G) (hST : S \u2264 T) : map \u03c6 S \u2264 map \u03c6 T := by /- EXAMPLES:   sorry SOLUTIONS: -/   intro x hx   rw [mem_map] at * -- Lean does not need this line   rcases hx with \u27e8y, hy, rfl\u27e9   use y, hST hy -- BOTH:</p> <p>variable {K : Type*} [Group K]</p> <p>-- Remember you can use the <code>ext</code> tactic to prove an equality of subgroups. example (\u03c6 : G \u2192* H) (\u03c8 : H \u2192* K) (U : Subgroup K) :     comap (\u03c8.comp \u03c6) U = comap \u03c6 (comap \u03c8 U) := by /- EXAMPLES:   sorry SOLUTIONS: -/   -- The whole proof could be <code>rfl</code>, but let's decompose it a bit.   ext x   simp only [mem_comap]   rfl -- BOTH:</p> <p>-- Pushing a subgroup along one homomorphism and then another is equal to -- pushing it forward along the composite of the homomorphisms. example (\u03c6 : G \u2192* H) (\u03c8 : H \u2192* K) (S : Subgroup G) :     map (\u03c8.comp \u03c6) S = map \u03c8 (S.map \u03c6) := by /- EXAMPLES:   sorry SOLUTIONS: -/   ext x   simp only [mem_map]   constructor   \u00b7 rintro \u27e8y, y_in, hy\u27e9     exact \u27e8\u03c6 y, \u27e8y, y_in, rfl\u27e9, hy\u27e9   \u00b7 rintro \u27e8y, \u27e8z, z_in, hz\u27e9, hy\u27e9     use z, z_in     calc \u03c8.comp \u03c6 z = \u03c8 (\u03c6 z) := rfl     _               = \u03c8 y := by congr     _               = x := hy -- BOTH:</p> <p>end exercises -- QUOTE.</p> <p>/- TEXT: Let us finish this introduction to subgroups in Mathlib with two very classical results. Lagrange theorem states the cardinality of a subgroup of a finite group divides the cardinality of the group. Sylow's first theorem is a famous partial converse to Lagrange's theorem.</p> <p>While this corner of Mathlib is partly set up to allow computation, we can tell Lean to use nonconstructive logic anyway using the following <code>open scoped</code> command. BOTH: -/ -- QUOTE: open scoped Classical</p> <p>-- EXAMPLES:</p> <p>example {G : Type*} [Group G] (G' : Subgroup G) : Nat.card G' \u2223 Nat.card G :=   \u27e8G'.index, mul_comm G'.index _ \u25b8 G'.index_mul_card.symm\u27e9</p> <p>-- BOTH: open Subgroup</p> <p>-- EXAMPLES: example {G : Type*} [Group G] [Finite G] (p : \u2115) {n : \u2115} [Fact p.Prime]     (hdvd : p ^ n \u2223 Nat.card G) : \u2203 K : Subgroup G, Nat.card K = p ^ n :=   Sylow.exists_subgroup_card_pow_prime p hdvd -- QUOTE.</p> <p>/- TEXT: The next two exercises derive a corollary of Lagrange's lemma. (This is also already in Mathlib, so do not use <code>exact?</code> too quickly.) BOTH: -/ -- QUOTE: lemma eq_bot_iff_card {G : Type*} [Group G] {H : Subgroup G} :     H = \u22a5 \u2194 Nat.card H = 1 := by   suffices (\u2200 x \u2208 H, x = 1) \u2194 \u2203 x \u2208 H, \u2200 a \u2208 H, a = x by     simpa [eq_bot_iff_forall, Nat.card_eq_one_iff_exists] /- EXAMPLES:   sorry SOLUTIONS: -/   constructor   \u00b7 intro h     use 1, H.one_mem   \u00b7 rintro \u27e8y, -, hy'\u27e9 x hx     calc x = y := hy' x hx     _      = 1 := (hy' 1 H.one_mem).symm -- EXAMPLES:</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-card_dvd_of_le","title":"check card_dvd_of_le","text":"<p>-- BOTH:</p> <p>lemma inf_bot_of_coprime {G : Type*} [Group G] (H K : Subgroup G)     (h : (Nat.card H).Coprime (Nat.card K)) : H \u2293 K = \u22a5 := by /- EXAMPLES:   sorry SOLUTIONS: -/   have D\u2081 : Nat.card (H \u2293 K : Subgroup G) \u2223 Nat.card H := card_dvd_of_le inf_le_left   have D\u2082 : Nat.card (H \u2293 K : Subgroup G) \u2223 Nat.card K := card_dvd_of_le inf_le_right   exact eq_bot_iff_card.2 (Nat.eq_one_of_dvd_coprimes h D\u2081 D\u2082) -- QUOTE.</p> <p>/- TEXT: Concrete groups <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>One can also manipulate concrete groups in Mathlib, although this is typically more complicated than working with the abstract theory. For instance, given any type <code>X</code>, the group of permutations of <code>X</code> is <code>Equiv.Perm X</code>. In particular the symmetric group :math:<code>\\mathfrak{S}_n</code> is <code>Equiv.Perm (Fin n)</code>. One can state abstract results about this group, for instance saying that <code>Equiv.Perm X</code> is generated by cycles if <code>X</code> is finite. EXAMPLES: -/ -- QUOTE: open Equiv</p> <p>example {X : Type*} [Finite X] : Subgroup.closure {\u03c3 : Perm X | Perm.IsCycle \u03c3} = \u22a4 :=   Perm.closure_isCycle -- QUOTE.</p> <p>/- TEXT: One can be fully concrete and compute actual products of cycles. Below we use the <code>#simp</code> command, which calls the <code>simp</code> tactic on a given expression. The notation <code>c[]</code> is used to define a cyclic permutation. In the example, the result is a permutation of <code>\u2115</code>. One could use a type ascription such as <code>(1 : Fin 5)</code> on the first number appearing to make it a computation in <code>Perm (Fin 5)</code>. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#simp-mul_assoc-c1-2-3-c2-3-4","title":"simp [mul_assoc] c[1, 2, 3] * c[2, 3, 4]","text":"<p>-- QUOTE.</p> <p>/- TEXT: Another way to work with concrete groups is to use free groups and group presentations. The free group on a type <code>\u03b1</code> is <code>FreeGroup \u03b1</code> and the inclusion map is <code>FreeGroup.of : \u03b1 \u2192 FreeGroup \u03b1</code>. For instance let us define a type <code>S</code> with three elements denoted by <code>a</code>, <code>b</code> and <code>c</code>, and the element <code>ab\u207b\u00b9</code> of the corresponding free group. EXAMPLES: -/ -- QUOTE: section FreeGroup</p> <p>inductive S | a | b | c</p> <p>open S</p> <p>def myElement : FreeGroup S := (.of a) * (.of b)\u207b\u00b9 -- QUOTE.</p> <p>/- TEXT: Note that we gave the expected type of the definition so that Lean knows that <code>.of</code> means <code>FreeGroup.of</code>.</p> <p>The universal property of free groups is embodied as the equivalence <code>FreeGroup.lift</code>. For example, let us define the group morphism from <code>FreeGroup S</code> to <code>Perm (Fin 5)</code> that sends <code>a</code> to <code>c[1, 2, 3]</code>, <code>b</code> to <code>c[2, 3, 1]</code>, and <code>c</code> to <code>c[2, 3]</code>, EXAMPLES: -/ -- QUOTE: def myMorphism : FreeGroup S \u2192* Perm (Fin 5) :=   FreeGroup.lift fun | .a =&gt; c[1, 2, 3]                      | .b =&gt; c[2, 3, 1]                      | .c =&gt; c[2, 3]</p> <p>-- QUOTE.</p> <p>/- TEXT: As a last concrete example, let us see how to define a group generated by a single element whose cube is one (so that group will be isomorphic to :math:<code>\\mathbb{Z}/3</code>) and build a morphism from that group to <code>Perm (Fin 5)</code>.</p> <p>As a type with exactly one element, we will use <code>Unit</code> whose only element is denoted by <code>()</code>. The function <code>PresentedGroup</code> takes a set of relations, i.e. a set of elements of some free group, and returns a group that is this free group quotiented by a normal subgroup generated by relations. (We will see how to handle more general quotients in :numref:<code>quotient_groups</code>.) Since we somehow hide this behind a definition, we use <code>deriving Group</code> to force creation of a group instance on <code>myGroup</code>. EXAMPLES: -/ -- QUOTE: def myGroup := PresentedGroup {.of () ^ 3} deriving Group -- QUOTE.</p> <p>/- TEXT: The universal property of presented groups ensures that morphisms out of this group can be built from functions that send the relations to the neutral element of the target group. So we need such a function and a proof that the condition holds. Then we can feed this proof to <code>PresentedGroup.toGroup</code> to get the desired group morphism. EXAMPLES: -/ -- QUOTE: def myMap : Unit \u2192 Perm (Fin 5) | () =&gt; c[1, 2, 3]</p> <p>lemma compat_myMap :     \u2200 r \u2208 ({.of () ^ 3} : Set (FreeGroup Unit)), FreeGroup.lift myMap r = 1 := by   rintro _ rfl   simp   decide</p> <p>def myNewMorphism : myGroup \u2192* Perm (Fin 5) := PresentedGroup.toGroup compat_myMap</p> <p>end FreeGroup -- QUOTE.</p> <p>/- TEXT: Group actions <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^</p> <p>One important way that group theory interacts with the rest of mathematics is through the use of group actions. An action of a group <code>G</code> on some type <code>X</code> is nothing more than a morphism from <code>G</code> to <code>Equiv.Perm X</code>. So in a sense group actions are already covered by the previous discussion. But we don't want to carry this morphism around; instead, we want it to be inferred automatically by Lean as much as possible. So we have a type class for this, which is <code>MulAction G X</code>. The downside of this setup is that having multiple actions of the same group on the same type requires some contortions, such as defining type synonyms, each of which carries different type class instances.</p> <p>This allows us in particular to use <code>g \u2022 x</code> to denote the action of a group element <code>g</code> on a point <code>x</code>. BOTH: -/ -- QUOTE: noncomputable section GroupActions</p> <p>-- EXAMPLES: example {G X : Type*} [Group G] [MulAction G X] (g g': G) (x : X) :     g \u2022 (g' \u2022 x) = (g * g') \u2022 x :=   (mul_smul g g' x).symm -- QUOTE.</p> <p>/- TEXT: There is also a version for additive group called <code>AddAction</code>, where the action is denoted by <code>+\u1d65</code>. This is used for instance in the definition of affine spaces. EXAMPLES: -/ -- QUOTE: example {G X : Type*} [AddGroup G] [AddAction G X] (g g' : G) (x : X) :     g +\u1d65 (g' +\u1d65 x) = (g + g') +\u1d65 x :=   (add_vadd g g' x).symm -- QUOTE.</p> <p>/- TEXT: The underlying group morphism is called <code>MulAction.toPermHom</code>. EXAMPLES: -/ -- QUOTE: open MulAction</p> <p>example {G X : Type*} [Group G] [MulAction G X] : G \u2192* Equiv.Perm X :=   toPermHom G X -- QUOTE.</p> <p>/- TEXT: As an illustration let us see how to define the Cayley isomorphism embedding of any group <code>G</code> into a permutation group, namely <code>Perm G</code>. EXAMPLES: -/ -- QUOTE: def CayleyIsoMorphism (G : Type*) [Group G] : G \u2243* (toPermHom G G).range :=   Equiv.Perm.subgroupOfMulAction G G -- QUOTE.</p> <p>/- TEXT: Note that nothing before the above definition required having a group rather than a monoid (or any type endowed with a multiplication operation really).</p> <p>The group condition really enters the picture when we will want to partition <code>X</code> into orbits. The corresponding equivalence relation on <code>X</code> is called <code>MulAction.orbitRel</code>. It is not declared as a global instance. EXAMPLES: -/ /- OMIT: TODO: We need to explain <code>Setoid</code> somewhere. EXAMPLES. -/ -- QUOTE: example {G X : Type*} [Group G] [MulAction G X] : Setoid X := orbitRel G X -- QUOTE.</p> <p>/- TEXT: Using this we can state that <code>X</code> is partitioned into orbits under the action of <code>G</code>. More precisely, we get a bijection between <code>X</code> and the dependent product <code>(\u03c9 : orbitRel.Quotient G X) \u00d7 (orbit G (Quotient.out' \u03c9))</code> where <code>Quotient.out' \u03c9</code> simply chooses an element that projects to <code>\u03c9</code>. Recall that elements of this dependent product are pairs <code>\u27e8\u03c9, x\u27e9</code> where the type <code>orbit G (Quotient.out' \u03c9)</code> of <code>x</code> depends on <code>\u03c9</code>. EXAMPLES: -/ -- QUOTE: example {G X : Type*} [Group G] [MulAction G X] :     X \u2243 (\u03c9 : orbitRel.Quotient G X) \u00d7 (orbit G (Quotient.out' \u03c9)) :=   MulAction.selfEquivSigmaOrbits G X -- QUOTE.</p> <p>/- TEXT: In particular, when X is finite, this can be combined with <code>Fintype.card_congr</code> and <code>Fintype.card_sigma</code> to deduce that the cardinality of <code>X</code> is the sum of the cardinalities of the orbits. Furthermore, the orbits are in bijection with the quotient of <code>G</code> under the action of the stabilizers by left translation. This action of a subgroup by left-translation is used to define quotients of a group by a subgroup with notation <code>/</code> so we can use the following concise statement. EXAMPLES: -/ -- QUOTE: example {G X : Type*} [Group G] [MulAction G X] (x : X) :     orbit G x \u2243 G \u29f8 stabilizer G x :=   MulAction.orbitEquivQuotientStabilizer G x -- QUOTE.</p> <p>/- TEXT: An important special case of combining the above two results is when <code>X</code> is a group <code>G</code> equipped with the action of a subgroup <code>H</code> by translation. In this case all stabilizers are trivial so every orbit is in bijection with <code>H</code> and we get: EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (H : Subgroup G) : G \u2243 (G \u29f8 H) \u00d7 H :=   groupEquivQuotientProdSubgroup -- QUOTE.</p> <p>/- TEXT: This is the conceptual variant of the version of Lagrange theorem that we saw above. Note this version makes no finiteness assumption.</p> <p>As an exercise for this section, let us build the action of a group on its subgroup by conjugation, using our definition of <code>conjugate</code> from a previous exercise. BOTH: -/ -- QUOTE: variable {G : Type*} [Group G]</p> <p>lemma conjugate_one (H : Subgroup G) : conjugate 1 H = H := by /- EXAMPLES:   sorry SOLUTIONS: -/   ext x   simp [conjugate] -- BOTH:</p> <p>instance : MulAction G (Subgroup G) where   smul := conjugate   one_smul := by /- EXAMPLES:     sorry SOLUTIONS: -/     exact conjugate_one -- BOTH:   mul_smul := by /- EXAMPLES:     sorry SOLUTIONS: -/     intro x y H     ext z     constructor     \u00b7 rintro \u27e8h, h_in, rfl\u27e9       use y*h*y\u207b\u00b9       constructor       \u00b7 use h       \u00b7 group     \u00b7 rintro \u27e8-, \u27e8h, h_in, rfl\u27e9, rfl\u27e9       use h, h_in       group -- BOTH:</p> <p>end GroupActions -- QUOTE.</p> <p>/- TEXT: .. _quotient_groups:</p> <p>Quotient groups <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>In the above discussion of subgroups acting on groups, we saw the quotient <code>G \u29f8 H</code> appear. In general this is only a type. It can be endowed with a group structure such that the quotient map is a group morphism if and only if <code>H</code> is a normal subgroup (and this group structure is then unique).</p> <p>The normality assumption is a type class <code>Subgroup.Normal</code> so that type class inference can use it to derive the group structure on the quotient. BOTH: -/ -- QUOTE: noncomputable section QuotientGroup</p> <p>-- EXAMPLES: example {G : Type*} [Group G] (H : Subgroup G) [H.Normal] : Group (G \u29f8 H) := inferInstance</p> <p>example {G : Type*} [Group G] (H : Subgroup G) [H.Normal] : G \u2192* G \u29f8 H :=   QuotientGroup.mk' H -- QUOTE.</p> <p>/- TEXT: The universal property of quotient groups is accessed through <code>QuotientGroup.lift</code>: a group morphism <code>\u03c6</code> descends to <code>G \u29f8 N</code> as soon as its kernel contains <code>N</code>. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] (N : Subgroup G) [N.Normal] {M : Type*}     [Group M] (\u03c6 : G \u2192* M) (h : N \u2264 MonoidHom.ker \u03c6) : G \u29f8 N \u2192* M :=   QuotientGroup.lift N \u03c6 h -- QUOTE.</p> <p>/- TEXT: The fact that the target group is called <code>M</code> is the above snippet is a clue that having a monoid structure on <code>M</code> would be enough.</p> <p>An important special case is when <code>N = ker \u03c6</code>. In that case the descended morphism is injective and we get a group isomorphism onto its image. This result is often called the first isomorphism theorem. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] {M : Type*} [Group M] (\u03c6 : G \u2192* M) :     G \u29f8 MonoidHom.ker \u03c6 \u2192* MonoidHom.range \u03c6 :=   QuotientGroup.quotientKerEquivRange \u03c6 -- QUOTE.</p> <p>/- TEXT: Applying the universal property to a composition of a morphism <code>\u03c6 : G \u2192* G'</code> with a quotient group projection <code>Quotient.mk' N'</code>, we can also aim for a morphism from <code>G \u29f8 N</code> to <code>G' \u29f8 N'</code>. The condition required on <code>\u03c6</code> is usually formulated by saying \"<code>\u03c6</code> should send <code>N</code> inside <code>N'</code>.\" But this is equivalent to asking that <code>\u03c6</code> should pull <code>N'</code> back inside <code>N</code>, and the latter condition is nicer to work with since the definition of pullback does not involve an existential quantifier. EXAMPLES: -/ -- QUOTE: example {G G': Type*} [Group G] [Group G']     {N : Subgroup G} [N.Normal] {N' : Subgroup G'} [N'.Normal]     {\u03c6 : G \u2192* G'} (h : N \u2264 Subgroup.comap \u03c6 N') : G \u29f8 N \u2192* G' \u29f8 N':=   QuotientGroup.map N N' \u03c6 h -- QUOTE.</p> <p>/- TEXT: One subtle point to keep in mind is that the type <code>G \u29f8 N</code> really depends on <code>N</code> (up to definitional equality), so having a proof that two normal subgroups <code>N</code> and <code>M</code> are equal is not enough to make the corresponding quotients equal. However the universal properties does give an isomorphism in this case. EXAMPLES: -/ -- QUOTE: example {G : Type*} [Group G] {M N : Subgroup G} [M.Normal]     [N.Normal] (h : M = N) : G \u29f8 M \u2243* G \u29f8 N := QuotientGroup.quotientMulEquivOfEq h -- QUOTE.</p> <p>/- TEXT: As a final series of exercises for this section, we will prove that if <code>H</code> and <code>K</code> are disjoint normal subgroups of a finite group <code>G</code> such that the product of their cardinalities is equal to the cardinality of <code>G</code> then <code>G</code> is isomorphic to <code>H \u00d7 K</code>. Recall that disjoint in this context means <code>H \u2293 K = \u22a5</code>.</p> <p>We start with playing a bit with Lagrange's lemma, without assuming the subgroups are normal or disjoint. BOTH: -/ -- QUOTE: section variable {G : Type*} [Group G] {H K : Subgroup G}</p> <p>open MonoidHom</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-natcard_pos-the-nonempty-argument-will-be-automatically-inferred-for-subgroups","title":"check Nat.card_pos -- The nonempty argument will be automatically inferred for subgroups","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-subgroupindex_eq_card","title":"check Subgroup.index_eq_card","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-subgroupindex_mul_card","title":"check Subgroup.index_mul_card","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-nateq_of_mul_eq_mul_right","title":"check Nat.eq_of_mul_eq_mul_right","text":"<p>lemma aux_card_eq [Finite G] (h' : Nat.card G = Nat.card H * Nat.card K) :     Nat.card (G \u29f8 H) = Nat.card K := by /- EXAMPLES:   sorry SOLUTIONS: -/   have := calc     Nat.card (G \u29f8 H) * Nat.card H = Nat.card G := by rw [\u2190 H.index_eq_card, H.index_mul_card]     _                             = Nat.card K * Nat.card H := by rw [h', mul_comm]</p> <p>exact Nat.eq_of_mul_eq_mul_right Nat.card_pos this -- QUOTE.</p> <p>/- TEXT: From now on, we assume that our subgroups are normal and disjoint, and we assume the cardinality condition. Now we construct the first building block of the desired isomorphism. BOTH: -/ -- QUOTE: variable H.Normal [Fintype G] (h : Disjoint H K)   (h' : Nat.card G = Nat.card H * Nat.card K)</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-natbijective_iff_injective_and_card","title":"check Nat.bijective_iff_injective_and_card","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-ker_eq_bot_iff","title":"check ker_eq_bot_iff","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-restrict","title":"check restrict","text":""},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-ker_restrict","title":"check ker_restrict","text":"<p>def iso\u2081 [Fintype G] (h : Disjoint H K) (h' : Nat.card G = Nat.card H * Nat.card K) : K \u2243* G \u29f8 H := by /- EXAMPLES:   sorry SOLUTIONS: -/   apply MulEquiv.ofBijective ((QuotientGroup.mk' H).restrict K)   rw [Nat.bijective_iff_injective_and_card]   constructor   \u00b7 rw [\u2190 ker_eq_bot_iff, (QuotientGroup.mk' H).ker_restrict K]     simp [h]   \u00b7 symm     exact aux_card_eq h' -- QUOTE.</p> <p>/- TEXT: Now we can define our second building block. We will need <code>MonoidHom.prod</code>, which builds a morphism from <code>G\u2080</code> to <code>G\u2081 \u00d7 G\u2082</code> out of morphisms from <code>G\u2080</code> to <code>G\u2081</code> and <code>G\u2082</code>. BOTH: -/ -- QUOTE: def iso\u2082 : G \u2243* (G \u29f8 K) \u00d7 (G \u29f8 H) := by /- EXAMPLES:   sorry SOLUTIONS: -/   apply MulEquiv.ofBijective &lt;| (QuotientGroup.mk' K).prod (QuotientGroup.mk' H)   rw [Nat.bijective_iff_injective_and_card]   constructor   \u00b7 rw [\u2190 ker_eq_bot_iff, ker_prod]     simp [h.symm.eq_bot]   \u00b7 rw [Nat.card_prod]     rw [aux_card_eq h', aux_card_eq (mul_comm (Nat.card H) _\u25b8 h'), h'] -- QUOTE.</p> <p>/- TEXT: We are ready to put all pieces together. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C08_Groups_and_Rings/S01_Groups/#check-mulequivprodcongr","title":"check MulEquiv.prodCongr","text":"<p>-- BOTH: def finalIso : G \u2243* H \u00d7 K := /- EXAMPLES:   sorry SOLUTIONS: -/   (iso\u2082 h h').trans ((iso\u2081 h.symm (mul_comm (Nat.card H) _ \u25b8 h')).prodCongr (iso\u2081 h h')).symm</p> <p>end end QuotientGroup -- QUOTE.</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/","title":"S02 Rings","text":"<p>-- BOTH: import Mathlib.RingTheory.Ideal.QuotientOperations import Mathlib.RingTheory.Localization.Basic import Mathlib.RingTheory.DedekindDomain.Ideal import Mathlib.Analysis.Complex.Polynomial import Mathlib.Data.ZMod.Quotient import MIL.Common</p> <p>noncomputable section</p> <p>/- TEXT: .. _rings:</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#rings","title":"Rings","text":"<p>.. index:: ring (algebraic structure)</p> <p>Rings, their units, morphisms and subrings <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>The type of ring structures on a type <code>R</code> is <code>Ring R</code>. The variant where multiplication is assumed to be commutative is <code>CommRing R</code>. We have already seen that the <code>ring</code> tactic will prove any equality that follows from the axioms of a commutative ring. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] (x y : R) : (x + y) ^ 2 = x ^ 2 + y ^ 2 + 2 * x * y := by ring -- QUOTE.</p> <p>/- TEXT: More exotic variants do not require that the addition on <code>R</code> forms a group but only an additive monoid. The corresponding type classes are <code>Semiring R</code> and <code>CommSemiring R</code>. The type of natural numbers is an important instance of <code>CommSemiring R</code>, as is any type of functions taking values in the natural numbers. Another important example is the type of ideals in a ring, which will be discussed below. The name of the <code>ring</code> tactic is doubly misleading, since it assumes commutativity but works in semirings as well. In other words, it applies to any <code>CommSemiring</code>. EXAMPLES: -/ -- QUOTE: example (x y : \u2115) : (x + y) ^ 2 = x ^ 2 + y ^ 2 + 2 * x * y := by ring -- QUOTE.</p> <p>/- TEXT: There are also versions of the ring and semiring classes that do not assume the existence of a multiplicative unit or the associativity of multiplication. We will not discuss those here.</p> <p>Some concepts that are traditionally taught in an introduction to ring theory are actually about the underlying multiplicative monoid. A prominent example is the definition of the units of a ring. Every (multiplicative) monoid <code>M</code> has a predicate <code>IsUnit : M \u2192 Prop</code> asserting existence of a two-sided inverse, a type <code>Units M</code> of units with notation <code>M\u02e3</code>, and a coercion to <code>M</code>. The type <code>Units M</code> bundles an invertible element with its inverse as well as properties than ensure that each is indeed the inverse of the other. This implementation detail is relevant mainly when defining computable functions. In most situations one can use <code>IsUnit.unit {x : M} : IsUnit x \u2192 M\u02e3</code> to build a unit. In the commutative case, one also has <code>Units.mkOfMulEqOne (x y : M) : x * y = 1 \u2192 M\u02e3</code> which builds <code>x</code> seen as unit. EXAMPLES: -/ -- QUOTE: example (x : \u2124\u02e3) : x = 1 \u2228 x = -1 := Int.units_eq_one_or x</p> <p>example {M : Type*} [Monoid M] (x : M\u02e3) : (x : M) * x\u207b\u00b9 = 1 := Units.mul_inv x</p> <p>example {M : Type*} [Monoid M] : Group M\u02e3 := inferInstance -- QUOTE.</p> <p>/- TEXT: The type of ring morphisms between two (semi)-rings <code>R</code> and <code>S</code> is <code>RingHom R S</code>, with notation <code>R \u2192+* S</code>. EXAMPLES: -/ -- QUOTE: example {R S : Type*} [Ring R] [Ring S] (f : R \u2192+* S) (x y : R) :     f (x + y) = f x + f y := f.map_add x y</p> <p>example {R S : Type*} [Ring R] [Ring S] (f : R \u2192+* S) : R\u02e3 \u2192* S\u02e3 :=   Units.map f -- QUOTE.</p> <p>/- TEXT: The isomorphism variant is <code>RingEquiv</code>, with notation <code>\u2243+*</code>.</p> <p>As with submonoids and subgroups, there is a <code>Subring R</code> type for subrings of a ring <code>R</code>, but this type is a lot less useful than the type of subgroups since one cannot quotient a ring by a subring. EXAMPLES: -/ -- QUOTE: example {R : Type*} [Ring R] (S : Subring R) : Ring S := inferInstance -- QUOTE.</p> <p>/- TEXT: Also notice that <code>RingHom.range</code> produces a subring.</p> <p>Ideals and quotients <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>For historical reasons, Mathlib only has a theory of ideals for commutative rings. (The ring library was originally developed to make quick progress toward the foundations of modern algebraic geometry.) So in this section we will work with commutative (semi)rings. Ideals of <code>R</code> are defined as submodules of <code>R</code> seen as <code>R</code>-modules. Modules will be covered later in a chapter on linear algebra, but this implementation detail can mostly be safely ignored since most (but not all) relevant lemmas are restated in the special context of ideals. But anonymous projection notation won't always work as expected. For instance, one cannot replace <code>Ideal.Quotient.mk I</code> by <code>I.Quotient.mk</code> in the snippet below because there are two <code>.</code>s and so it will parse as <code>(Ideal.Quotient I).mk</code>; but <code>Ideal.Quotient</code> by itself doesn't exist. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] (I : Ideal R) : R \u2192+* R \u29f8 I :=   Ideal.Quotient.mk I</p> <p>example {R : Type*} [CommRing R] {a : R} {I : Ideal R} :     Ideal.Quotient.mk I a = 0 \u2194 a \u2208 I :=   Ideal.Quotient.eq_zero_iff_mem -- QUOTE.</p> <p>/- TEXT: The universal property of quotient rings is <code>Ideal.Quotient.lift</code>. EXAMPLES: -/ -- QUOTE: example {R S : Type*} [CommRing R] [CommRing S] (I : Ideal R) (f : R \u2192+* S)     (H : I \u2264 RingHom.ker f) : R \u29f8 I \u2192+* S :=   Ideal.Quotient.lift I f H -- QUOTE.</p> <p>/- TEXT: In particular it leads to the first isomorphism theorem for rings. EXAMPLES: -/ -- QUOTE: example {R S : Type*} [CommRing R] CommRing S :     R \u29f8 RingHom.ker f \u2243+* f.range :=   RingHom.quotientKerEquivRange f -- QUOTE.</p> <p>/- TEXT: Ideals form a complete lattice structure with the inclusion relation, as well as a semiring structure. These two structures interact nicely. EXAMPLES: -/ section -- QUOTE: variable {R : Type*} [CommRing R] {I J : Ideal R}</p> <p>-- EXAMPLES: example : I + J = I \u2294 J := rfl</p> <p>example {x : R} : x \u2208 I + J \u2194 \u2203 a \u2208 I, \u2203 b \u2208 J, a + b = x := by   simp [Submodule.mem_sup]</p> <p>example : I * J \u2264 J := Ideal.mul_le_left</p> <p>example : I * J \u2264 I := Ideal.mul_le_right</p> <p>example : I * J \u2264 I \u2293 J := Ideal.mul_le_inf -- QUOTE.</p> <p>end</p> <p>/- TEXT: One can use ring morphisms to push ideals forward and pull them back using <code>Ideal.map</code> and <code>Ideal.comap</code>, respectively. As usual, the latter is more convenient to use since it does not involve an existential quantifier. This explains why it is used to state the condition that allows us to build morphisms between quotient rings. EXAMPLES: -/ -- QUOTE: example {R S : Type*} [CommRing R] [CommRing S] (I : Ideal R) (J : Ideal S) (f : R \u2192+* S)     (H : I \u2264 Ideal.comap f J) : R \u29f8 I \u2192+* S \u29f8 J :=   Ideal.quotientMap J f H -- QUOTE.</p> <p>/- TEXT: One subtle point is that the type <code>R \u29f8 I</code> really depends on <code>I</code> (up to definitional equality), so having a proof that two ideals <code>I</code> and <code>J</code> are equal is not enough to make the corresponding quotients equal. However, the universal properties do provide an isomorphism in this case. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] {I J : Ideal R} (h : I = J) : R \u29f8 I \u2243+* R \u29f8 J :=   Ideal.quotEquivOfEq h -- QUOTE.</p> <p>/- TEXT: We can now present the Chinese remainder isomorphism as an example. Pay attention to the difference between the indexed infimum symbol <code>\u2a05</code> and the big product of types symbol <code>\u03a0</code>. Depending on your font, those can be pretty hard to distinguish. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] {\u03b9 : Type*} [Fintype \u03b9] (f : \u03b9 \u2192 Ideal R)     (hf : \u2200 i j, i \u2260 j \u2192 IsCoprime (f i) (f j)) : (R \u29f8 \u2a05 i, f i) \u2243+* \u03a0 i, R \u29f8 f i :=   Ideal.quotientInfRingEquivPiQuotient f hf -- QUOTE.</p> <p>/- TEXT: The elementary version of the Chinese remainder theorem, a statement about <code>ZMod</code>, can be easily deduced from the previous one: BOTH: -/ -- QUOTE: open BigOperators PiNotation</p> <p>-- EXAMPLES: example {\u03b9 : Type*} [Fintype \u03b9] (a : \u03b9 \u2192 \u2115) (coprime : \u2200 i j, i \u2260 j \u2192 (a i).Coprime (a j)) :     ZMod (\u220f i, a i) \u2243+* \u03a0 i, ZMod (a i) :=   ZMod.prodEquivPi a coprime -- QUOTE.</p> <p>/- TEXT: As a series of exercises, we will reprove the Chinese remainder theorem in the general case.</p> <p>We first need to define the map appearing in the theorem, as a ring morphism, using the universal property of quotient rings. BOTH: -/ section -- QUOTE: variable {\u03b9 R : Type*} [CommRing R] open Ideal Quotient Function</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-piringhom","title":"check Pi.ringHom","text":""},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-ker_pi_quotient_mk","title":"check ker_Pi_Quotient_mk","text":"<p>/-- The homomorphism from <code>R \u29f8 \u2a05 i, I i</code> to <code>\u03a0 i, R \u29f8 I i</code> featured in the Chinese   Remainder Theorem. -/ def chineseMap (I : \u03b9 \u2192 Ideal R) : (R \u29f8 \u2a05 i, I i) \u2192+* \u03a0 i, R \u29f8 I i := /- EXAMPLES:   sorry SOLUTIONS: -/   Ideal.Quotient.lift (\u2a05 i, I i) (Pi.ringHom fun i : \u03b9 \u21a6 Ideal.Quotient.mk (I i))     (by simp [\u2190 RingHom.mem_ker, ker_Pi_Quotient_mk]) -- QUOTE. -- BOTH:</p> <p>/- TEXT: Make sure the following next two lemmas can be proven by <code>rfl</code>. BOTH: -/ -- QUOTE: lemma chineseMap_mk (I : \u03b9 \u2192 Ideal R) (x : R) :     chineseMap I (Quotient.mk _ x) = fun i : \u03b9 \u21a6 Ideal.Quotient.mk (I i) x := /- EXAMPLES:   sorry SOLUTIONS: -/   rfl -- BOTH:</p> <p>lemma chineseMap_mk' (I : \u03b9 \u2192 Ideal R) (x : R) (i : \u03b9) :     chineseMap I (mk _ x) i = mk (I i) x := /- EXAMPLES:   sorry SOLUTIONS: -/   rfl -- QUOTE. -- BOTH:</p> <p>/- TEXT: The next lemma proves the easy half of the Chinese remainder theorem, without any assumption on the family of ideals. The proof is less than one line long. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-injective_lift_iff","title":"check injective_lift_iff","text":"<p>-- BOTH: lemma chineseMap_inj (I : \u03b9 \u2192 Ideal R) : Injective (chineseMap I) := by /- EXAMPLES:   sorry SOLUTIONS: -/   rw [chineseMap, injective_lift_iff, ker_Pi_Quotient_mk] -- QUOTE. -- BOTH:</p> <p>/- TEXT: We are now ready for the heart of the theorem, which will show the surjectivity of our <code>chineseMap</code>. First we need to know the different ways one can express the coprimality (also called co-maximality assumption). Only the first two will be needed below. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-iscoprime","title":"check IsCoprime","text":""},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-iscoprime_iff_add","title":"check isCoprime_iff_add","text":""},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-iscoprime_iff_exists","title":"check isCoprime_iff_exists","text":""},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-iscoprime_iff_sup_eq","title":"check isCoprime_iff_sup_eq","text":""},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-iscoprime_iff_codisjoint","title":"check isCoprime_iff_codisjoint","text":"<p>-- QUOTE.</p> <p>/- TEXT: We take the opportunity to use induction on <code>Finset</code>. Relevant lemmas on <code>Finset</code> are given below. Remember that the <code>ring</code> tactic works for semirings and that the ideals of a ring form a semiring. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-finsetmem_insert_of_mem","title":"check Finset.mem_insert_of_mem","text":""},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-finsetmem_insert_self","title":"check Finset.mem_insert_self","text":"<p>-- BOTH: theorem isCoprime_Inf {I : Ideal R} {J : \u03b9 \u2192 Ideal R} {s : Finset \u03b9}     (hf : \u2200 j \u2208 s, IsCoprime I (J j)) : IsCoprime I (\u2a05 j \u2208 s, J j) := by   classical   simp_rw [isCoprime_iff_add] at *   induction s using Finset.induction with   | empty =&gt;       simp   | @insert i s _ hs =&gt;       rw [Finset.iInf_insert, inf_comm, one_eq_top, eq_top_iff, \u2190 one_eq_top]       set K := \u2a05 j \u2208 s, J j       calc /- EXAMPLES:         1 = I + K                  := sorry         _ = I + K * (I + J i)      := sorry         _ = (1 + K) * I + K * J i  := sorry         _ \u2264 I + K \u2293 J i            := sorry SOLUTIONS: -/         1 = I + K                  := (hs fun j hj \u21a6 hf j (Finset.mem_insert_of_mem hj)).symm         _ = I + K * (I + J i)      := by rw [hf i (Finset.mem_insert_self i s), mul_one]         _ = (1 + K) * I + K * J i  := by ring         _ \u2264 I + K \u2293 J i            := by gcongr ; apply mul_le_left ; apply mul_le_inf</p> <p>-- QUOTE.</p> <p>/- TEXT: We can now prove surjectivity of the map appearing in the Chinese remainder theorem. BOTH: -/ -- QUOTE: lemma chineseMap_surj [Fintype \u03b9] {I : \u03b9 \u2192 Ideal R}     (hI : \u2200 i j, i \u2260 j \u2192 IsCoprime (I i) (I j)) : Surjective (chineseMap I) := by   classical   intro g   choose f hf using fun i \u21a6 Ideal.Quotient.mk_surjective (g i)   have key : \u2200 i, \u2203 e : R, mk (I i) e = 1 \u2227 \u2200 j, j \u2260 i \u2192 mk (I j) e = 0 := by     intro i     have hI' : \u2200 j \u2208 ({i} : Finset \u03b9)\u1d9c, IsCoprime (I i) (I j) := by /- EXAMPLES:       sorry SOLUTIONS: -/       intros j hj       exact hI _ _ (by simpa [ne_comm, isCoprime_iff_add] using hj) /- EXAMPLES:     sorry SOLUTIONS: -/     rcases isCoprime_iff_exists.mp (isCoprime_Inf hI') with \u27e8u, hu, e, he, hue\u27e9     replace he : \u2200 j, j \u2260 i \u2192 e \u2208 I j := by simpa using he     refine \u27e8e, ?, ?\u27e9     \u00b7 simp [eq_sub_of_add_eq' hue, map_sub, eq_zero_iff_mem.mpr hu]     \u00b7 exact fun j hj \u21a6 eq_zero_iff_mem.mpr (he j hj) -- BOTH:   choose e he using key   use mk _ (\u2211 i, f i * e i) /- EXAMPLES:   sorry SOLUTIONS: -/   ext i   rw [chineseMap_mk', map_sum, Fintype.sum_eq_single i]   \u00b7 simp [(he i).1, hf]   \u00b7 intros j hj     simp [(he j).2 i hj.symm] -- QUOTE. -- BOTH:</p> <p>/- TEXT: Now all the pieces come together in the following: BOTH: -/ -- QUOTE: noncomputable def chineseIso [Fintype \u03b9] (f : \u03b9 \u2192 Ideal R)     (hf : \u2200 i j, i \u2260 j \u2192 IsCoprime (f i) (f j)) : (R \u29f8 \u2a05 i, f i) \u2243+* \u03a0 i, R \u29f8 f i :=   { Equiv.ofBijective _ \u27e8chineseMap_inj f, chineseMap_surj hf\u27e9,     chineseMap f with } -- QUOTE.</p> <p>end</p> <p>/- TEXT: Algebras and polynomials <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>Given a commutative (semi)ring <code>R</code>, an algebra over <code>R</code> is a semiring <code>A</code> equipped with a ring morphism whose image commutes with every element of <code>A</code>. This is encoded as a type class <code>Algebra R A</code>. The morphism from <code>R</code> to <code>A</code> is called the structure map and is denoted <code>algebraMap R A : R \u2192+* A</code> in Lean. Multiplication of <code>a : A</code> by <code>algebraMap R A r</code> for some <code>r : R</code> is called the scalar multiplication of <code>a</code> by <code>r</code> and denoted by <code>r \u2022 a</code>. Note that this notion of algebra is sometimes called an associative unital algebra to emphasize the existence of more general notions of algebra.</p> <p>The fact that <code>algebraMap R A</code> is ring morphism packages together a lot of properties of scalar multiplication, such as the following: EXAMPLES: -/ -- QUOTE: example {R A : Type*} [CommRing R] [Ring A] [Algebra R A] (r r' : R) (a : A) :     (r + r') \u2022 a = r \u2022 a + r' \u2022 a :=   add_smul r r' a</p> <p>example {R A : Type*} [CommRing R] [Ring A] [Algebra R A] (r r' : R) (a : A) :     (r * r') \u2022 a = r \u2022 r' \u2022 a :=   mul_smul r r' a -- QUOTE.</p> <p>/- TEXT: The morphisms between two <code>R</code>-algebras <code>A</code> and <code>B</code> are ring morphisms which commute with scalar multiplication by elements of <code>R</code>. They are bundled morphisms with type <code>AlgHom R A B</code>, which is denoted by <code>A \u2192\u2090[R] B</code>.</p> <p>Important examples of non-commutative algebras include algebras of endomorphisms and algebras of square matrices, both of which will be covered in the chapter on linear algebra. In this chapter we will discuss one of the most important examples of a commutative algebra, namely, polynomial algebras.</p> <p>The algebra of univariate polynomials with coefficients in <code>R</code> is called <code>Polynomial R</code>, which can be written as <code>R[X]</code> as soon as one opens the <code>Polynomial</code> namespace. The algebra structure map from <code>R</code> to <code>R[X]</code> is denoted by <code>C</code>, which stands for \"constant\" since the corresponding polynomial functions are always constant. The indeterminate is denoted by <code>X</code>. EXAMPLES: -/ section Polynomials -- QUOTE: open Polynomial</p> <p>example {R : Type*} [CommRing R] : R[X] := X</p> <p>example {R : Type*} [CommRing R] (r : R) := X - C r -- QUOTE.</p> <p>/- TEXT: In the first example above, it is crucial that we give Lean the expected type since it cannot be determined from the body of the definition. In the second example, the target polynomial algebra can be inferred from our use of <code>C r</code> since the type of <code>r</code> is known.</p> <p>Because <code>C</code> is a ring morphism from <code>R</code> to <code>R[X]</code>, we can use all ring morphisms lemmas such as <code>map_zero</code>, <code>map_one</code>, <code>map_mul</code>, and <code>map_pow</code> before computing in the ring <code>R[X]</code>. For example: EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] (r : R) : (X + C r) * (X - C r) = X ^ 2 - C (r ^ 2) := by   rw [C.map_pow]   ring -- QUOTE.</p> <p>/- TEXT: You can access coefficients using <code>Polynomial.coeff</code> EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] (r:R) : (C r).coeff 0 = r := by simp</p> <p>example {R : Type*} [CommRing R] : (X ^ 2 + 2 * X + C 3 : R[X]).coeff 1 = 2 := by simp -- QUOTE.</p> <p>/- TEXT: Defining the degree of a polynomial is always tricky because of the special case of the zero polynomial. Mathlib has two variants: <code>Polynomial.natDegree : R[X] \u2192 \u2115</code> assigns degree <code>0</code> to the zero polynomial, and <code>Polynomial.degree : R[X] \u2192 WithBot \u2115</code> assigns <code>\u22a5</code>. In the latter, <code>WithBot \u2115</code> can be seen as <code>\u2115 \u222a {-\u221e}</code>, except that <code>-\u221e</code> is denoted <code>\u22a5</code>, the same symbol as the bottom element in a complete lattice. This special value is used as the degree of the zero polynomial, and it is absorbent for addition. (It is almost absorbent for multiplication, except that <code>\u22a5 * 0 = 0</code>.)</p> <p>Morally speaking, the <code>degree</code> version is the correct one. For instance, it allows us to state the expected formula for the degree of a product (assuming the base ring has no zero divisor). EXAMPLES: -/ -- QUOTE: example {R : Type*} [Semiring R] [NoZeroDivisors R] {p q : R[X]} :     degree (p * q) = degree p + degree q :=   Polynomial.degree_mul -- QUOTE.</p> <p>/- TEXT: Whereas the version for <code>natDegree</code> needs to assume non-zero polynomials. EXAMPLES: -/ -- QUOTE: example {R : Type*} [Semiring R] [NoZeroDivisors R] {p q : R[X]} (hp : p \u2260 0) (hq : q \u2260 0) :     natDegree (p * q) = natDegree p + natDegree q :=   Polynomial.natDegree_mul hp hq -- QUOTE.</p> <p>/- TEXT: However, <code>\u2115</code> is much nicer to use than <code>WithBot \u2115</code>, so Mathlib makes both versions available and provides lemmas to convert between them. Also, <code>natDegree</code> is the more convenient definition to use when computing the degree of a composition. Composition of polynomial is <code>Polynomial.comp</code> and we have: EXAMPLES: -/ -- QUOTE: example {R : Type*} [Semiring R] [NoZeroDivisors R] {p q : R[X]} :     natDegree (comp p q) = natDegree p * natDegree q :=   Polynomial.natDegree_comp -- QUOTE.</p> <p>/- TEXT: Polynomials give rise to polynomial functions: any polynomial can be evaluated on <code>R</code> using <code>Polynomial.eval</code>. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] (P: R[X]) (x : R) := P.eval x</p> <p>example {R : Type*} [CommRing R] (r : R) : (X - C r).eval r = 0 := by simp -- QUOTE.</p> <p>/- TEXT: In particular, there is a predicate, <code>IsRoot</code>, that holds for elements <code>r</code> in <code>R</code> where a polynomial vanishes. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] (P : R[X]) (r : R) : IsRoot P r \u2194 P.eval r = 0 := Iff.rfl -- QUOTE.</p> <p>/- TEXT: We would like to say that, assuming <code>R</code> has no zero divisor, a polynomial has at most as many roots as its degree, where the roots are counted with multiplicities. But once again the case of the zero polynomial is painful. So Mathlib defines <code>Polynomial.roots</code> to send a polynomial <code>P</code> to a multiset, i.e. the finite set that is defined to be empty if <code>P</code> is zero and the roots of <code>P</code>, with multiplicities, otherwise. This is defined only when the underlying ring is a domain since otherwise the definition does not have good properties. EXAMPLES: -/ -- QUOTE: example {R : Type*} [CommRing R] [IsDomain R] (r : R) : (X - C r).roots = {r} :=   roots_X_sub_C r</p> <p>example {R : Type*} [CommRing R] [IsDomain R] (r : R) (n : \u2115):     ((X - C r) ^ n).roots = n \u2022 {r} :=   by simp -- QUOTE.</p> <p>/- TEXT: Both <code>Polynomial.eval</code> and <code>Polynomial.roots</code> consider only the coefficients ring. They do not allow us to say that <code>X ^ 2 - 2 : \u211a[X]</code> has a root in <code>\u211d</code> or that <code>X ^ 2 + 1 : \u211d[X]</code> has a root in <code>\u2102</code>. For this, we need <code>Polynomial.aeval</code>, which will evaluate <code>P : R[X]</code> in any <code>R</code>-algebra. More precisely, given a semiring <code>A</code> and an instance of <code>Algebra R A</code>, <code>Polynomial.aeval</code> sends every element of <code>a</code> along the <code>R</code>-algebra morphism of evaluation at <code>a</code>. Since <code>AlgHom</code> has a coercion to functions, one can apply it to a polynomial. But <code>aeval</code> does not have a polynomial as an argument, so one cannot use dot notation like in <code>P.eval</code> above. EXAMPLES: -/ -- QUOTE: example : aeval Complex.I (X ^ 2 + 1 : \u211d[X]) = 0 := by simp</p> <p>-- QUOTE. /- TEXT: The function corresponding to <code>roots</code> in this context is <code>aroots</code> which takes a polynomial and then an algebra and outputs a multiset (with the same caveat about the zero polynomial as for <code>roots</code>). EXAMPLES: -/ -- QUOTE: open Complex Polynomial</p> <p>example : aroots (X ^ 2 + 1 : \u211d[X]) \u2102 = {Complex.I, -I} := by   suffices roots (X ^ 2 + 1 : \u2102[X]) = {I, -I} by simpa [aroots_def]   have factored : (X ^ 2 + 1 : \u2102[X]) = (X - C I) * (X - C (-I)) := by     rw [C_neg]     linear_combination show (C I * C I : \u2102[X]) = -1 by simp [\u2190 C_mul]   have p_ne_zero : (X - C I) * (X - C (-I)) \u2260 0 := by     intro H     apply_fun eval 0 at H     simp [eval] at H   simp only [factored, roots_mul p_ne_zero, roots_X_sub_C]   rfl</p> <p>-- Mathlib knows about D'Alembert-Gauss theorem: <code>\u2102</code> is algebraically closed. example : IsAlgClosed \u2102 := inferInstance</p> <p>-- QUOTE. /- TEXT: More generally, given an ring morphism <code>f : R \u2192+* S</code> one can evaluate <code>P : R[X]</code> at a point in <code>S</code> using <code>Polynomial.eval\u2082</code>. This one produces an actual function from <code>R[X]</code> to <code>S</code> since it does not assume the existence of a <code>Algebra R S</code> instance, so dot notation works as you would expect. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C08_Groups_and_Rings/S02_Rings/#check-complexofreal-r-c","title":"check (Complex.ofReal : \u211d \u2192+* \u2102)","text":"<p>example : (X ^ 2 + 1 : \u211d[X]).eval\u2082 Complex.ofReal Complex.I = 0 := by simp -- QUOTE.</p> <p>/- TEXT: Let us end by mentioning multivariate polynomials briefly. Given a commutative semiring <code>R</code>, the <code>R</code>-algebra of polynomials with coefficients in <code>R</code> and indeterminates indexed by a type <code>\u03c3</code> is <code>MVPolynomial \u03c3 R</code>. Given <code>i : \u03c3</code>, the corresponding polynomial is <code>MvPolynomial.X i</code>. (As usual, one can open the <code>MVPolynomial</code> namespace to shorten this to <code>X i</code>.) For instance, if we want two indeterminates we can use <code>Fin 2</code> as <code>\u03c3</code> and write the polynomial defining the unit circle in :math:`\\mathbb{R}^2`` as: EXAMPLES: -/ -- QUOTE: open MvPolynomial</p> <p>def circleEquation : MvPolynomial (Fin 2) \u211d := X 0 ^ 2 + X 1 ^ 2 - 1 -- QUOTE.</p> <p>/- TEXT: Recall that function application has a very high precedence so the expression above is read as <code>(X 0) ^ 2 + (X 1) ^ 2 - 1</code>. We can evaluate it to make sure the point with coordinates :math:<code>(1, 0)</code> is on the circle. Recall the <code>![...]</code> notation denotes elements of <code>Fin n \u2192 X</code> for some natural number <code>n</code> determined by the number of arguments and some type <code>X</code> determined by the type of arguments. EXAMPLES: -/ -- QUOTE: example : MvPolynomial.eval ![0, 1] circleEquation = 0 := by simp [circleEquation] -- QUOTE.</p> <p>end Polynomials</p>"},{"location":"MIL/C09_Topology/S01_Filters/","title":"S01 Filters","text":"<p>import MIL.Common import Mathlib.Topology.Instances.Real</p> <p>open Set Filter Topology</p> <p>/- TEXT: .. index:: Filter</p> <p>.. _filters:</p>"},{"location":"MIL/C09_Topology/S01_Filters/#filters","title":"Filters","text":"<p>A filter on a type <code>X</code> is a collection of sets of <code>X</code> that satisfies three conditions that we will spell out below. The notion supports two related ideas:</p> <ul> <li> <p>limits, including all the kinds of limits discussed above: finite and infinite limits of sequences, finite and infinite limits of functions at a point or at infinity, and so on.</p> </li> <li> <p>things happening eventually, including things happening for large enough <code>n : \u2115</code>, or sufficiently near a point <code>x</code>, or for sufficiently close pairs of points, or almost everywhere in the sense of measure theory. Dually, filters can also express the idea of things happening often: for arbitrarily large <code>n</code>, at a point in any neighborhood of a given point, etc.</p> </li> </ul> <p>The filters that correspond to these descriptions will be defined later in this section, but we can already name them:</p> <ul> <li><code>(atTop : Filter \u2115)</code>, made of sets of <code>\u2115</code> containing <code>{n | n \u2265 N}</code> for some <code>N</code></li> <li><code>\ud835\udcdd x</code>, made of neighborhoods of <code>x</code> in a topological space</li> <li><code>\ud835\udce4 X</code>, made of entourages of a uniform space (uniform spaces generalize metric spaces and topological groups)</li> <li><code>\u03bc.ae</code> , made of sets whose complement has zero measure with respect to a measure <code>\u03bc</code>.</li> </ul> <p>The general definition is as follows: a filter <code>F : Filter X</code> is a collection of sets <code>F.sets : Set (Set X)</code> satisfying the following:</p> <ul> <li><code>F.univ_sets : univ \u2208 F.sets</code></li> <li><code>F.sets_of_superset : \u2200 {U V}, U \u2208 F.sets \u2192 U \u2286 V \u2192 V \u2208 F.sets</code></li> <li><code>F.inter_sets : \u2200 {U V}, U \u2208 F.sets \u2192 V \u2208 F.sets \u2192 U \u2229 V \u2208 F.sets</code>.</li> </ul> <p>The first condition says that the set of all elements of <code>X</code> belongs to <code>F.sets</code>. The second condition says that if <code>U</code> belongs to <code>F.sets</code> then anything containing <code>U</code> also belongs to <code>F.sets</code>. The third condition says that <code>F.sets</code> is closed under finite intersections. In Mathlib, a filter <code>F</code> is defined to be a structure bundling <code>F.sets</code> and its three properties, but the properties carry no additional data, and it is convenient to blur the distinction between <code>F</code> and <code>F.sets</code>. We therefore define <code>U \u2208 F</code> to mean <code>U \u2208 F.sets</code>. This explains why the word <code>sets</code> appears in the names of some lemmas that that mention <code>U \u2208 F</code>.</p> <p>It may help to think of a filter as defining a notion of a \"sufficiently large\" set. The first condition then says that <code>univ</code> is sufficiently large, the second one says that a set containing a sufficiently large set is sufficiently large and the third one says that the intersection of two sufficiently large sets is sufficiently large.</p> <p>It may be even  more useful to think of a filter on a type <code>X</code> as a generalized element of <code>Set X</code>. For instance, <code>atTop</code> is the \"set of very large numbers\" and <code>\ud835\udcdd x\u2080</code> is the \"set of points very close to <code>x\u2080</code>.\" One manifestation of this view is that we can associate to any <code>s : Set X</code> the so-called principal filter consisting of all sets that contain <code>s</code>. This definition is already in Mathlib and has a notation <code>\ud835\udcdf</code> (localized in the <code>Filter</code> namespace). For the purpose of demonstration, we ask you to take this opportunity to work out the definition here. EXAMPLES: -/ -- QUOTE: def principal {\u03b1 : Type*} (s : Set \u03b1) : Filter \u03b1     where   sets := { t | s \u2286 t }   univ_sets := sorry   sets_of_superset := sorry   inter_sets := sorry -- QUOTE.</p> <p>-- SOLUTIONS: -- In the next example we could use <code>tauto</code> in each proof instead of knowing the lemmas example {\u03b1 : Type*} (s : Set \u03b1) : Filter \u03b1 :=   { sets := { t | s \u2286 t }     univ_sets := subset_univ s     sets_of_superset := fun hU hUV \u21a6 Subset.trans hU hUV     inter_sets := fun hU hV \u21a6 subset_inter hU hV }</p> <p>/- TEXT: For our second example, we ask you to define the filter <code>atTop : Filter \u2115</code>. (We could use any type with a preorder instead of <code>\u2115</code>.) EXAMPLES: -/ -- QUOTE: example : Filter \u2115 :=   { sets := { s | \u2203 a, \u2200 b, a \u2264 b \u2192 b \u2208 s }     univ_sets := sorry     sets_of_superset := sorry     inter_sets := sorry } -- QUOTE.</p> <p>-- SOLUTIONS: example : Filter \u2115 :=   { sets := { s | \u2203 a, \u2200 b, a \u2264 b \u2192 b \u2208 s }     univ_sets := by       use 42       simp     sets_of_superset := by       rintro U V \u27e8N, hN\u27e9 hUV       use N       tauto     inter_sets := by       rintro U V \u27e8N, hN\u27e9 \u27e8N', hN'\u27e9       use max N N'       intro b hb       rw [max_le_iff] at hb       constructor &lt;;&gt; tauto }</p> <p>/- TEXT: We can also directly define the filter <code>\ud835\udcdd x</code> of neighborhoods of any <code>x : \u211d</code>. In the real numbers, a neighborhood of <code>x</code> is a set containing an open interval :math:<code>(x_0 - \\varepsilon, x_0 + \\varepsilon)</code>, defined in Mathlib as <code>Ioo (x\u2080 - \u03b5) (x\u2080 + \u03b5)</code>. (This is notion of a neighborhood is only a special case of a more general construction in Mathlib.)</p> <p>With these examples, we can already define what is means for a function <code>f : X \u2192 Y</code> to converge to some <code>G : Filter Y</code> along some <code>F : Filter X</code>, as follows: BOTH: -/ -- QUOTE: def Tendsto\u2081 {X Y : Type*} (f : X \u2192 Y) (F : Filter X) (G : Filter Y) :=   \u2200 V \u2208 G, f \u207b\u00b9' V \u2208 F -- QUOTE.</p> <p>/- TEXT: When <code>X</code> is <code>\u2115</code> and <code>Y</code> is <code>\u211d</code>, <code>Tendsto\u2081 u atTop (\ud835\udcdd x)</code> is equivalent to saying that the sequence <code>u : \u2115 \u2192 \u211d</code> converges to the real number <code>x</code>. When both <code>X</code> and <code>Y</code> are <code>\u211d</code>, <code>Tendsto f (\ud835\udcdd x\u2080) (\ud835\udcdd y\u2080)</code> is equivalent to the familiar notion :math:<code>\\lim_{x \\to x\u2080} f(x) = y\u2080</code>. All of the other kinds of limits mentioned in the introduction are also equivalent to instances of <code>Tendsto\u2081</code> for suitable choices of filters on the source and target.</p> <p>The notion <code>Tendsto\u2081</code> above is definitionally equivalent to the notion <code>Tendsto</code> that is defined in Mathlib, but the latter is defined more abstractly. The problem with the definition of <code>Tendsto\u2081</code> is that it exposes a quantifier and elements of <code>G</code>, and it hides the intuition that we get by viewing filters as generalized sets. We can hide the quantifier <code>\u2200 V</code> and make the intuition more salient by using more algebraic and set-theoretic machinery. The first ingredient is the pushforward operation :math:<code>f_*</code> associated to any map <code>f : X \u2192 Y</code>, denoted <code>Filter.map f</code> in Mathlib. Given a filter <code>F</code> on <code>X</code>, <code>Filter.map f F : Filter Y</code> is defined so that <code>V \u2208 Filter.map f F \u2194 f \u207b\u00b9' V \u2208 F</code> holds definitionally. In this examples file we've opened the <code>Filter</code> namespace so that <code>Filter.map</code> can be written as <code>map</code>. This means that we can rewrite the definition of <code>Tendsto</code> using the order relation on <code>Filter Y</code>, which is reversed inclusion of the set of members. In other words, given <code>G H : Filter Y</code>, we have <code>G \u2264 H \u2194 \u2200 V : Set Y, V \u2208 H \u2192 V \u2208 G</code>. EXAMPLES: -/ -- QUOTE: def Tendsto\u2082 {X Y : Type*} (f : X \u2192 Y) (F : Filter X) (G : Filter Y) :=   map f F \u2264 G</p> <p>example {X Y : Type*} (f : X \u2192 Y) (F : Filter X) (G : Filter Y) :     Tendsto\u2082 f F G \u2194 Tendsto\u2081 f F G :=   Iff.rfl -- QUOTE.</p> <p>/- TEXT: It may seem that the order relation on filters is backward. But recall that we can view filters on <code>X</code> as generalized elements of <code>Set X</code>, via the inclusion of <code>\ud835\udcdf : Set X \u2192 Filter X</code> which maps any set <code>s</code> to the corresponding principal filter. This inclusion is order preserving, so the order relation on <code>Filter</code> can indeed be seen as the natural inclusion relation between generalized sets. In this analogy, pushforward is analogous to the direct image. And, indeed, <code>map f (\ud835\udcdf s) = \ud835\udcdf (f '' s)</code>.</p> <p>We can now understand intuitively why a sequence <code>u : \u2115 \u2192 \u211d</code> converges to a point <code>x\u2080</code> if and only if we have <code>map u atTop \u2264 \ud835\udcdd x\u2080</code>. The inequality means the \"direct image under <code>u</code>\" of \"the set of very big natural numbers\" is \"included\" in \"the set of points very close to <code>x\u2080</code>.\"</p> <p>As promised, the definition of <code>Tendsto\u2082</code> does not exhibit any quantifiers or sets. It also leverages the algebraic properties of the pushforward operation. First, each <code>Filter.map f</code> is monotone. And, second, <code>Filter.map</code> is compatible with composition. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C09_Topology/S01_Filters/#check-filtermap_mono-m-monotone-map-m","title":"check (@Filter.map_mono : \u2200 {\u03b1 \u03b2} {m : \u03b1 \u2192 \u03b2}, Monotone (map m))","text":""},{"location":"MIL/C09_Topology/S01_Filters/#check","title":"check","text":"<p>(@Filter.map_map :     \u2200 {\u03b1 \u03b2 \u03b3} {f : Filter \u03b1} {m : \u03b1 \u2192 \u03b2} {m' : \u03b2 \u2192 \u03b3}, map m' (map m f) = map (m' \u2218 m) f) -- QUOTE.</p> <p>/- TEXT: Together these two properties allow us to prove that limits compose, yielding in one shot all 256 variants of the composition lemma described in the introduction, and lots more. You can practice proving the following statement using either the definition of <code>Tendsto\u2081</code> in terms of the universal quantifier or the algebraic definition, together with the two lemmas above. EXAMPLES: -/ -- QUOTE: example {X Y Z : Type*} {F : Filter X} {G : Filter Y} {H : Filter Z} {f : X \u2192 Y} {g : Y \u2192 Z}     (hf : Tendsto\u2081 f F G) (hg : Tendsto\u2081 g G H) : Tendsto\u2081 (g \u2218 f) F H :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example {X Y Z : Type*} {F : Filter X} {G : Filter Y} {H : Filter Z} {f : X \u2192 Y} {g : Y \u2192 Z}     (hf : Tendsto\u2081 f F G) (hg : Tendsto\u2081 g G H) : Tendsto\u2081 (g \u2218 f) F H :=   calc     map (g \u2218 f) F = map g (map f F) := by rw [map_map]     _ \u2264 map g G := (map_mono hf)     _ \u2264 H := hg</p> <p>example {X Y Z : Type*} {F : Filter X} {G : Filter Y} {H : Filter Z} {f : X \u2192 Y} {g : Y \u2192 Z}     (hf : Tendsto\u2081 f F G) (hg : Tendsto\u2081 g G H) : Tendsto\u2081 (g \u2218 f) F H := by   intro V hV   rw [preimage_comp]   apply hf   apply hg   exact hV</p> <p>/- TEXT: The pushforward construction uses a map to push filters from the map source to the map target. There also a pullback operation, <code>Filter.comap</code>, going in the other direction. This generalizes the preimage operation on sets. For any map <code>f</code>, <code>Filter.map f</code> and <code>Filter.comap f</code> form what is known as a Galois connection, which is to say, they satisfy</p> <p><code>Filter.map_le_iff_le_comap : Filter.map f F \u2264 G \u2194 F \u2264 Filter.comap f G</code></p> <p>for every <code>F</code> and <code>G</code>. This operation could be used to provided another formulation of <code>Tendsto</code> that would be provably (but not definitionally) equivalent to the one in Mathlib.</p> <p>The <code>comap</code> operation can be used to restrict filters to a subtype. For instance, suppose we have <code>f : \u211d \u2192 \u211d</code>, <code>x\u2080 : \u211d</code> and <code>y\u2080 : \u211d</code>, and suppose we want to state that <code>f x</code> approaches <code>y\u2080</code> when <code>x</code> approaches <code>x\u2080</code> within the rational numbers. We can pull the filter <code>\ud835\udcdd x\u2080</code> back to <code>\u211a</code> using the coercion map <code>(\u2191) : \u211a \u2192 \u211d</code> and state <code>Tendsto (f \u2218 (\u2191) : \u211a \u2192 \u211d) (comap (\u2191) (\ud835\udcdd x\u2080)) (\ud835\udcdd y\u2080)</code>. EXAMPLES: -/ -- QUOTE: variable (f : \u211d \u2192 \u211d) (x\u2080 y\u2080 : \u211d)</p>"},{"location":"MIL/C09_Topology/S01_Filters/#check-comap-q-r-n-x0","title":"check comap ((\u2191) : \u211a \u2192 \u211d) (\ud835\udcdd x\u2080)","text":""},{"location":"MIL/C09_Topology/S01_Filters/#check-tendsto-f-comap-q-r-n-x0-n-y0","title":"check Tendsto (f \u2218 (\u2191)) (comap ((\u2191) : \u211a \u2192 \u211d) (\ud835\udcdd x\u2080)) (\ud835\udcdd y\u2080)","text":"<p>-- QUOTE.</p> <p>/- TEXT: The pullback operation is also compatible with composition, but it is contravariant, which is to say, it reverses the order of the arguments. EXAMPLES: -/ -- QUOTE: section variable {\u03b1 \u03b2 \u03b3 : Type*} (F : Filter \u03b1) {m : \u03b3 \u2192 \u03b2} {n : \u03b2 \u2192 \u03b1}</p>"},{"location":"MIL/C09_Topology/S01_Filters/#check-comap_comap-comap-m-comap-n-f-comap-n-m-f","title":"check (comap_comap : comap m (comap n F) = comap (n \u2218 m) F)","text":"<p>end -- QUOTE.</p> <p>/- TEXT: Let's now shift attention to the plane <code>\u211d \u00d7 \u211d</code> and try to understand how the neighborhoods of a point <code>(x\u2080, y\u2080)</code> are related to <code>\ud835\udcdd x\u2080</code> and <code>\ud835\udcdd y\u2080</code>. There is a product operation <code>Filter.prod : Filter X \u2192 Filter Y \u2192 Filter (X \u00d7 Y)</code>, denoted by <code>\u00d7\u02e2</code>, which answers this question: EXAMPLES: -/ -- QUOTE: example : \ud835\udcdd (x\u2080, y\u2080) = \ud835\udcdd x\u2080 \u00d7\u02e2 \ud835\udcdd y\u2080 :=   nhds_prod_eq -- QUOTE.</p> <p>/- TEXT: The product operation is defined in terms of the pullback operation and the <code>inf</code> operation:</p> <p><code>F \u00d7\u02e2 G = (comap Prod.fst F) \u2293 (comap Prod.snd G)</code>.</p> <p>Here the <code>inf</code> operation refers to the lattice structure on <code>Filter X</code> for any type <code>X</code>, whereby <code>F \u2293 G</code> is the greatest filter that is smaller than both <code>F</code> and <code>G</code>. Thus the <code>inf</code> operation generalizes the notion of the intersection of sets.</p> <p>A lot of proofs in Mathlib use all of the aforementioned structure (<code>map</code>, <code>comap</code>, <code>inf</code>, <code>sup</code>, and <code>prod</code>) to give algebraic proofs about convergence without ever referring to members of filters. You can practice doing this in a proof of the following lemma, unfolding the definition of <code>Tendsto</code> and <code>Filter.prod</code> if needed. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C09_Topology/S01_Filters/#check-le_inf_iff","title":"check le_inf_iff","text":"<p>example (f : \u2115 \u2192 \u211d \u00d7 \u211d) (x\u2080 y\u2080 : \u211d) :     Tendsto f atTop (\ud835\udcdd (x\u2080, y\u2080)) \u2194       Tendsto (Prod.fst \u2218 f) atTop (\ud835\udcdd x\u2080) \u2227 Tendsto (Prod.snd \u2218 f) atTop (\ud835\udcdd y\u2080) :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example (f : \u2115 \u2192 \u211d \u00d7 \u211d) (x\u2080 y\u2080 : \u211d) :     Tendsto f atTop (\ud835\udcdd (x\u2080, y\u2080)) \u2194       Tendsto (Prod.fst \u2218 f) atTop (\ud835\udcdd x\u2080) \u2227 Tendsto (Prod.snd \u2218 f) atTop (\ud835\udcdd y\u2080) :=   calc     Tendsto f atTop (\ud835\udcdd (x\u2080, y\u2080)) \u2194 map f atTop \u2264 \ud835\udcdd (x\u2080, y\u2080) := Iff.rfl     _ \u2194 map f atTop \u2264 \ud835\udcdd x\u2080 \u00d7\u02e2 \ud835\udcdd y\u2080 := by rw [nhds_prod_eq]     _ \u2194 map f atTop \u2264 comap Prod.fst (\ud835\udcdd x\u2080) \u2293 comap Prod.snd (\ud835\udcdd y\u2080) := Iff.rfl     _ \u2194 map f atTop \u2264 comap Prod.fst (\ud835\udcdd x\u2080) \u2227 map f atTop \u2264 comap Prod.snd (\ud835\udcdd y\u2080) := le_inf_iff     _ \u2194 map Prod.fst (map f atTop) \u2264 \ud835\udcdd x\u2080 \u2227 map Prod.snd (map f atTop) \u2264 \ud835\udcdd y\u2080 := by       rw [\u2190 map_le_iff_le_comap, \u2190 map_le_iff_le_comap]     _ \u2194 map (Prod.fst \u2218 f) atTop \u2264 \ud835\udcdd x\u2080 \u2227 map (Prod.snd \u2218 f) atTop \u2264 \ud835\udcdd y\u2080 := by       rw [map_map, map_map]</p> <p>-- an alternative solution example (f : \u2115 \u2192 \u211d \u00d7 \u211d) (x\u2080 y\u2080 : \u211d) :     Tendsto f atTop (\ud835\udcdd (x\u2080, y\u2080)) \u2194       Tendsto (Prod.fst \u2218 f) atTop (\ud835\udcdd x\u2080) \u2227 Tendsto (Prod.snd \u2218 f) atTop (\ud835\udcdd y\u2080) := by   rw [nhds_prod_eq]   unfold Tendsto SProd.sprod Filter.instSProd Filter.prod   erw [le_inf_iff, \u2190 map_le_iff_le_comap, map_map, \u2190 map_le_iff_le_comap, map_map]</p> <p>/- TEXT: The ordered type <code>Filter X</code> is actually a complete lattice, which is to say, there is a bottom element, there is a top element, and every set of filters on <code>X</code> has an <code>Inf</code> and a <code>Sup</code>.</p> <p>Note that given the second property in the definition of a filter (if <code>U</code> belongs to <code>F</code> then anything larger than <code>U</code> also belongs to <code>F</code>), the first property (the set of all inhabitants of <code>X</code> belongs to <code>F</code>) is equivalent to the property that <code>F</code> is not the empty collection of sets. This shouldn't be confused with the more subtle question as to whether the empty set is an element of <code>F</code>. The definition of a filter does not prohibit <code>\u2205 \u2208 F</code>, but if the empty set is in <code>F</code> then every set is in <code>F</code>, which is to say, <code>\u2200 U : Set X, U \u2208 F</code>. In this case, <code>F</code> is a rather trivial filter, which is precisely the bottom element of the complete lattice <code>Filter X</code>. This contrasts with the definition of filters in Bourbaki, which doesn't allow filters containing the empty set.</p> <p>Because we include the trivial filter in our definition, we sometimes need to explicitly assume nontriviality in some lemmas. In return, however, the theory has nicer global properties. We have already seen that including the trivial filter gives us a bottom element. It also allows us to define <code>principal : Set X \u2192 Filter X</code>, which maps  <code>\u2205</code> to <code>\u22a5</code>, without adding a precondition to rule out the empty set. And it allows us to define the pullback operation without a precondition as well. Indeed, it can happen that <code>comap f F = \u22a5</code> although <code>F \u2260 \u22a5</code>. For instance, given <code>x\u2080 : \u211d</code> and <code>s : Set \u211d</code>, the pullback of <code>\ud835\udcdd x\u2080</code> under the coercion from the subtype corresponding to <code>s</code> is nontrivial if and only if <code>x\u2080</code> belongs to the closure of <code>s</code>.</p> <p>In order to manage lemmas that do need to assume some filter is nontrivial, Mathlib has a type class <code>Filter.NeBot</code>, and the library has lemmas that assume <code>(F : Filter X) [F.NeBot]</code>. The instance database knows, for example, that <code>(atTop : Filter \u2115).NeBot</code>, and it knows that pushing forward a nontrivial filter gives a nontrivial filter. As a result, a lemma assuming <code>[F.NeBot]</code> will automatically apply to <code>map u atTop</code> for any sequence <code>u</code>.</p> <p>Our tour of the algebraic properties of filters and their relation to limits is essentially done, but we have not yet justified our claim to have recaptured the usual limit notions. Superficially, it may seem that <code>Tendsto u atTop (\ud835\udcdd x\u2080)</code> is stronger than the notion of convergence defined in :numref:<code>sequences_and_convergence</code> because we ask that every neighborhood of <code>x\u2080</code> has a preimage belonging to <code>atTop</code>, whereas the usual definition only requires this for the standard neighborhoods <code>Ioo (x\u2080 - \u03b5) (x\u2080 + \u03b5)</code>. The key is that, by definition, every neighborhood contains such a standard one. This observation leads to the notion of a filter basis.</p> <p>Given <code>F : Filter X</code>, a family of sets <code>s : \u03b9 \u2192 Set X</code> is a basis for <code>F</code> if for every set <code>U</code>, we have <code>U \u2208 F</code> if and only if it contains some <code>s i</code>. In other words, formally speaking, <code>s</code> is a basis if it satisfies <code>\u2200 U : Set X, U \u2208 F \u2194 \u2203 i, s i \u2286 U</code>. It is even more flexible to consider a predicate on <code>\u03b9</code> that selects only some of the values <code>i</code> in the indexing type. In the case of <code>\ud835\udcdd x\u2080</code>, we want <code>\u03b9</code> to be <code>\u211d</code>, we write <code>\u03b5</code> for <code>i</code>, and the predicate should select the positive values of <code>\u03b5</code>. So the fact that the sets <code>Ioo  (x\u2080 - \u03b5) (x\u2080 + \u03b5)</code> form a basis for the neighborhood topology on <code>\u211d</code> is stated as follows: EXAMPLES: -/ -- QUOTE: example (x\u2080 : \u211d) : HasBasis (\ud835\udcdd x\u2080) (fun \u03b5 : \u211d \u21a6 0 &lt; \u03b5) fun \u03b5 \u21a6 Ioo (x\u2080 - \u03b5) (x\u2080 + \u03b5) :=   nhds_basis_Ioo_pos x\u2080 -- QUOTE.</p> <p>/- TEXT: There is also a nice basis for the filter <code>atTop</code>. The lemma <code>Filter.HasBasis.tendsto_iff</code> allows us to reformulate a statement of the form <code>Tendsto f F G</code> given bases for <code>F</code> and <code>G</code>. Putting these pieces together gives us essentially the notion of convergence that we used in :numref:<code>sequences_and_convergence</code>. EXAMPLES: -/ -- QUOTE: example (u : \u2115 \u2192 \u211d) (x\u2080 : \u211d) :     Tendsto u atTop (\ud835\udcdd x\u2080) \u2194 \u2200 \u03b5 &gt; 0, \u2203 N, \u2200 n \u2265 N, u n \u2208 Ioo (x\u2080 - \u03b5) (x\u2080 + \u03b5) := by   have : atTop.HasBasis (fun _ : \u2115 \u21a6 True) Ici := atTop_basis   rw [this.tendsto_iff (nhds_basis_Ioo_pos x\u2080)]   simp -- QUOTE.</p> <p>/- TEXT: We now show how filters facilitate working with properties that hold for sufficiently large numbers or for points that are sufficiently close to a given point. In :numref:<code>sequences_and_convergence</code>, we were often faced with the situation where we knew that some property <code>P n</code> holds for sufficiently large <code>n</code> and that some other property <code>Q n</code> holds for sufficiently large <code>n</code>. Using <code>cases</code> twice gave us <code>N_P</code> and <code>N_Q</code> satisfying <code>\u2200 n \u2265 N_P, P n</code> and <code>\u2200 n \u2265 N_Q, Q n</code>. Using <code>set N := max N_P N_Q</code>, we could eventually prove <code>\u2200 n \u2265 N, P n \u2227 Q n</code>. Doing this repeatedly becomes tiresome.</p> <p>We can do better by noting that the statement \"<code>P n</code> and <code>Q n</code> hold for large enough <code>n</code>\" means that we have <code>{n | P n} \u2208 atTop</code> and <code>{n | Q n} \u2208 atTop</code>. The fact that <code>atTop</code> is a filter implies that the intersection of two elements of <code>atTop</code> is again in <code>atTop</code>, so we have <code>{n | P n \u2227 Q n} \u2208 atTop</code>. Writing <code>{n | P n} \u2208 atTop</code> is unpleasant, but we can use the more suggestive notation <code>\u2200\u1da0 n in atTop, P n</code>. Here the superscripted <code>f</code> stands for \"Filter.\" You can think of the notation as saying that for all <code>n</code> in the \"set of very large numbers,\" <code>P n</code> holds. The <code>\u2200\u1da0</code> notation stands for <code>Filter.Eventually</code>, and the lemma <code>Filter.Eventually.and</code> uses the intersection property of filters to do what we just described: EXAMPLES: -/ -- QUOTE: example (P Q : \u2115 \u2192 Prop) (hP : \u2200\u1da0 n in atTop, P n) (hQ : \u2200\u1da0 n in atTop, Q n) :     \u2200\u1da0 n in atTop, P n \u2227 Q n :=   hP.and hQ -- QUOTE.</p> <p>/- TEXT: This notation is so convenient and intuitive that we also have specializations when <code>P</code> is an equality or inequality statement. For example, let <code>u</code> and <code>v</code> be two sequences of real numbers, and let us show that if <code>u n</code> and <code>v n</code> coincide for sufficiently large <code>n</code> then <code>u</code> tends to <code>x\u2080</code> if and only if <code>v</code> tends to <code>x\u2080</code>. First we'll use the generic <code>Eventually</code> and then the one specialized for the equality predicate, <code>EventuallyEq</code>. The two statements are definitionally equivalent so the same proof work in both cases. EXAMPLES: -/ -- QUOTE: example (u v : \u2115 \u2192 \u211d) (h : \u2200\u1da0 n in atTop, u n = v n) (x\u2080 : \u211d) :     Tendsto u atTop (\ud835\udcdd x\u2080) \u2194 Tendsto v atTop (\ud835\udcdd x\u2080) :=   tendsto_congr' h</p> <p>example (u v : \u2115 \u2192 \u211d) (h : u =\u1da0[atTop] v) (x\u2080 : \u211d) :     Tendsto u atTop (\ud835\udcdd x\u2080) \u2194 Tendsto v atTop (\ud835\udcdd x\u2080) :=   tendsto_congr' h -- QUOTE.</p> <p>/- TEXT: It is instructive to review the definition of filters in terms of <code>Eventually</code>. Given <code>F : Filter X</code>, for any predicates <code>P</code> and <code>Q</code> on <code>X</code>,</p> <ul> <li>the condition <code>univ \u2208 F</code> ensures <code>(\u2200 x, P x) \u2192 \u2200\u1da0 x in F, P x</code>,</li> <li>the condition <code>U \u2208 F \u2192 U \u2286 V \u2192 V \u2208 F</code> ensures <code>(\u2200\u1da0 x in F, P x) \u2192 (\u2200 x, P x \u2192 Q x) \u2192 \u2200\u1da0 x in F, Q x</code>, and</li> <li>the condition <code>U \u2208 F \u2192 V \u2208 F \u2192 U \u2229 V \u2208 F</code> ensures <code>(\u2200\u1da0 x in F, P x) \u2192 (\u2200\u1da0 x in F, Q x) \u2192 \u2200\u1da0 x in F, P x \u2227 Q x</code>. EXAMPLES: -/ -- QUOTE:</li> </ul>"},{"location":"MIL/C09_Topology/S01_Filters/#check-eventually_of_forall","title":"check eventually_of_forall","text":""},{"location":"MIL/C09_Topology/S01_Filters/#check-eventuallymono","title":"check Eventually.mono","text":""},{"location":"MIL/C09_Topology/S01_Filters/#check-eventuallyand","title":"check Eventually.and","text":"<p>-- QUOTE.</p> <p>/- TEXT: The second item, corresponding to <code>Eventually.mono</code>, supports nice ways of using filters, especially when combined with <code>Eventually.and</code>. The <code>filter_upwards</code> tactic allows us to combine them. Compare: EXAMPLES: -/ -- QUOTE: example (P Q R : \u2115 \u2192 Prop) (hP : \u2200\u1da0 n in atTop, P n) (hQ : \u2200\u1da0 n in atTop, Q n)     (hR : \u2200\u1da0 n in atTop, P n \u2227 Q n \u2192 R n) : \u2200\u1da0 n in atTop, R n := by   apply (hP.and (hQ.and hR)).mono   rintro n \u27e8h, h', h''\u27e9   exact h'' \u27e8h, h'\u27e9</p> <p>example (P Q R : \u2115 \u2192 Prop) (hP : \u2200\u1da0 n in atTop, P n) (hQ : \u2200\u1da0 n in atTop, Q n)     (hR : \u2200\u1da0 n in atTop, P n \u2227 Q n \u2192 R n) : \u2200\u1da0 n in atTop, R n := by   filter_upwards [hP, hQ, hR] with n h h' h''   exact h'' \u27e8h, h'\u27e9 -- QUOTE.</p> <p>/- TEXT: Readers who know about measure theory will note that the filter <code>\u03bc.ae</code> of sets whose complement has measure zero (aka \"the set consisting of almost every point\") is not very useful as the source or target of <code>Tendsto</code>, but it can be conveniently used with <code>Eventually</code> to say that a property holds for almost every point.</p> <p>There is a dual version of <code>\u2200\u1da0 x in F, P x</code>, which is occasionally useful: <code>\u2203\u1da0 x in F, P x</code> means <code>{x | \u00acP x} \u2209 F</code>. For example, <code>\u2203\u1da0 n in atTop, P n</code> means there are arbitrarily large <code>n</code> such that <code>P n</code> holds. The <code>\u2203\u1da0</code> notation stands for <code>Filter.Frequently</code>.</p> <p>For a more sophisticated example, consider the following statement about a sequence <code>u</code>, a set <code>M</code>, and a value <code>x</code>:</p> <p>If <code>u</code> converges to <code>x</code> and <code>u n</code> belongs to <code>M</code> for   sufficiently large <code>n</code> then <code>x</code> is in the closure of <code>M</code>.</p> <p>This can be formalized as follows:</p> <p><code>Tendsto u atTop (\ud835\udcdd x) \u2192 (\u2200\u1da0 n in atTop, u n \u2208 M) \u2192 x \u2208 closure M</code>.</p> <p>This is a special case of the theorem <code>mem_closure_of_tendsto</code> from the topology library. See if you can prove it using the quoted lemmas, using the fact that <code>ClusterPt x F</code> means <code>(\ud835\udcdd x \u2293 F).NeBot</code> and that, by definition, the assumption <code>\u2200\u1da0 n in atTop, u n \u2208 M</code> means  <code>M \u2208 map u atTop</code>. EXAMPLES: -/ -- QUOTE:</p>"},{"location":"MIL/C09_Topology/S01_Filters/#check-mem_closure_iff_clusterpt","title":"check mem_closure_iff_clusterPt","text":""},{"location":"MIL/C09_Topology/S01_Filters/#check-le_principal_iff","title":"check le_principal_iff","text":""},{"location":"MIL/C09_Topology/S01_Filters/#check-nebot_of_le","title":"check neBot_of_le","text":"<p>example (u : \u2115 \u2192 \u211d) (M : Set \u211d) (x : \u211d) (hux : Tendsto u atTop (\ud835\udcdd x))     (huM : \u2200\u1da0 n in atTop, u n \u2208 M) : x \u2208 closure M :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example (u : \u2115 \u2192 \u211d) (M : Set \u211d) (x : \u211d) (hux : Tendsto u atTop (\ud835\udcdd x))     (huM : \u2200\u1da0 n in atTop, u n \u2208 M) : x \u2208 closure M :=   mem_closure_iff_clusterPt.mpr (neBot_of_le &lt;| le_inf hux &lt;| le_principal_iff.mpr huM)</p>"},{"location":"MIL/C09_Topology/S02_Metric_Spaces/","title":"S02 Metric Spaces","text":"<p>import MIL.Common import Mathlib.Topology.Instances.Real import Mathlib.Analysis.NormedSpace.BanachSteinhaus</p> <p>open Set Filter open Topology Filter</p> <p>/- TEXT: .. index:: metric space</p> <p>.. _metric_spaces:</p>"},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#metric-spaces","title":"Metric spaces","text":"<p>Examples in the previous section focus on sequences of real numbers. In this section we will go up a bit in generality and focus on metric spaces. A metric space is a type <code>X</code> equipped with a distance function <code>dist : X \u2192 X \u2192 \u211d</code> which is a generalization of the function <code>fun x y \u21a6 |x - y|</code> from the case where <code>X = \u211d</code>.</p> <p>Introducing such a space is easy and we will check all properties required from the distance function. BOTH: -/ -- QUOTE: variable {X : Type*} [MetricSpace X] (a b c : X)</p>"},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-dist-a-b-r","title":"check (dist a b : \u211d)","text":""},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-dist_nonneg-0-dist-a-b","title":"check (dist_nonneg : 0 \u2264 dist a b)","text":""},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-dist_eq_zero-dist-a-b-0-a-b","title":"check (dist_eq_zero : dist a b = 0 \u2194 a = b)","text":""},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-dist_comm-a-b-dist-a-b-dist-b-a","title":"check (dist_comm a b : dist a b = dist b a)","text":""},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-dist_triangle-a-b-c-dist-a-c-dist-a-b-dist-b-c","title":"check (dist_triangle a b c : dist a c \u2264 dist a b + dist b c)","text":"<p>-- QUOTE.</p> <p>/- TEXT: Note we also have variants where the distance can be infinite or where <code>dist a b</code> can be zero without having <code>a = b</code> or both. They are called <code>EMetricSpace</code>, <code>PseudoMetricSpace</code> and <code>PseudoEMetricSpace</code> respectively (here \"e\" stands for \"extended\").</p> <p>BOTH: -/ -- Note the next three lines are not quoted, their purpose is to make sure those things don't get renamed while we're looking elsewhere.</p>"},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-emetricspace","title":"check EMetricSpace","text":""},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-pseudometricspace","title":"check PseudoMetricSpace","text":""},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-pseudoemetricspace","title":"check PseudoEMetricSpace","text":"<p>/- TEXT: Note that our journey from <code>\u211d</code> to metric spaces jumped over the special case of normed spaces that also require linear algebra and will be explained as part of the calculus chapter.</p> <p>Convergence and continuity <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>Using distance functions, we can already define convergent sequences and continuous functions between metric spaces. They are actually defined in a more general setting covered in the next section, but we have lemmas recasting the definition in terms of distances. BOTH: -/ -- QUOTE: example {u : \u2115 \u2192 X} {a : X} :     Tendsto u atTop (\ud835\udcdd a) \u2194 \u2200 \u03b5 &gt; 0, \u2203 N, \u2200 n \u2265 N, dist (u n) a &lt; \u03b5 :=   Metric.tendsto_atTop</p> <p>example {X Y : Type*} [MetricSpace X] [MetricSpace Y] {f : X \u2192 Y} :     Continuous f \u2194       \u2200 x : X, \u2200 \u03b5 &gt; 0, \u2203 \u03b4 &gt; 0, \u2200 x', dist x' x &lt; \u03b4 \u2192 dist (f x') (f x) &lt; \u03b5 :=   Metric.continuous_iff -- QUOTE.</p> <p>/- TEXT: .. index:: continuity, tactics ; continuity</p> <p>A lot of lemmas have some continuity assumptions, so we end up proving a lot of continuity results and there is a <code>continuity</code> tactic devoted to this task. Let's prove a continuity statement that will be needed in an exercise below. Notice that Lean knows how to treat a product of two metric spaces as a metric space, so it makes sense to consider continuous functions from <code>X \u00d7 X</code> to <code>\u211d</code>. In particular the (uncurried version of the) distance function is such a function.</p> <p>BOTH: -/ -- QUOTE: example {X Y : Type*} [MetricSpace X] [MetricSpace Y] {f : X \u2192 Y} (hf : Continuous f) :     Continuous fun p : X \u00d7 X \u21a6 dist (f p.1) (f p.2) := by continuity -- QUOTE.</p> <p>/- TEXT: This tactic is a bit slow, so it is also useful to know how to do it by hand. We first need to use that <code>fun p : X \u00d7 X \u21a6 f p.1</code> is continuous because it is the composition of <code>f</code>, which is continuous by assumption <code>hf</code>, and the projection <code>prod.fst</code> whose continuity is the content of the lemma <code>continuous_fst</code>. The composition property is <code>Continuous.comp</code> which is in the <code>Continuous</code> namespace so we can use dot notation to compress <code>Continuous.comp hf continuous_fst</code> into <code>hf.comp continuous_fst</code> which is actually more readable since it really reads as composing our assumption and our lemma. We can do the same for the second component to get continuity of <code>fun p : X \u00d7 X \u21a6 f p.2</code>. We then assemble those two continuities using <code>Continuous.prod_mk</code> to get <code>(hf.comp continuous_fst).prod_mk (hf.comp continuous_snd) : Continuous (fun p : X \u00d7 X \u21a6 (f p.1, f p.2))</code> and compose once more to get our full proof. BOTH: -/ -- QUOTE: example {X Y : Type*} [MetricSpace X] [MetricSpace Y] {f : X \u2192 Y} (hf : Continuous f) :     Continuous fun p : X \u00d7 X \u21a6 dist (f p.1) (f p.2) :=   continuous_dist.comp ((hf.comp continuous_fst).prod_mk (hf.comp continuous_snd)) -- QUOTE.</p> <p>/- TEXT: The combination of <code>Continuous.prod_mk</code> and <code>continuous_dist</code> via <code>Continuous.comp</code> feels clunky, even when heavily using dot notation as above. A more serious issue is that this nice proof requires a lot of planning. Lean accepts the above proof term because it is a full term proving a statement which is definitionally equivalent to our goal, the crucial definition to unfold being that of a composition of functions. Indeed our target function <code>fun p : X \u00d7 X \u21a6 dist (f p.1) (f p.2)</code> is not presented as a composition. The proof term we provided proves continuity of <code>dist \u2218 (fun p : X \u00d7 X \u21a6 (f p.1, f p.2))</code> which happens to be definitionally equal to our target function. But if we try to build this proof gradually using tactics starting with <code>apply continuous_dist.comp</code> then Lean's elaborator will fail to recognize a composition and refuse to apply this lemma. It is especially bad at this when products of types are involved.</p> <p>A better lemma to apply here is <code>Continuous.dist {f g : X \u2192 Y} : Continuous f \u2192 Continuous g \u2192 Continuous (fun x \u21a6 dist (f x) (g x))</code> which is nicer to Lean's elaborator and also provides a shorter proof when directly providing a full proof term, as can be seen from the following two new proofs of the above statement: BOTH: -/ -- QUOTE: example {X Y : Type*} [MetricSpace X] [MetricSpace Y] {f : X \u2192 Y} (hf : Continuous f) :     Continuous fun p : X \u00d7 X \u21a6 dist (f p.1) (f p.2) := by   apply Continuous.dist   exact hf.comp continuous_fst   exact hf.comp continuous_snd</p> <p>example {X Y : Type*} [MetricSpace X] [MetricSpace Y] {f : X \u2192 Y} (hf : Continuous f) :     Continuous fun p : X \u00d7 X \u21a6 dist (f p.1) (f p.2) :=   (hf.comp continuous_fst).dist (hf.comp continuous_snd) -- QUOTE.</p> <p>/- TEXT: Note that, without the elaboration issue coming from composition, another way to compress our proof would be to use <code>Continuous.prod_map</code> which is sometimes useful and gives as an alternate proof term <code>continuous_dist.comp (hf.prod_map hf)</code> which even shorter to type.</p> <p>Since it is sad to decide between a version which is better for elaboration and a version which is shorter to type, let us wrap this discussion with a last bit of compression offered by <code>Continuous.fst'</code> which allows to compress <code>hf.comp continuous_fst</code> to <code>hf.fst'</code> (and the same with <code>snd</code>) and get our final proof, now bordering obfuscation.</p> <p>BOTH: -/ -- QUOTE: example {X Y : Type*} [MetricSpace X] [MetricSpace Y] {f : X \u2192 Y} (hf : Continuous f) :     Continuous fun p : X \u00d7 X \u21a6 dist (f p.1) (f p.2) :=   hf.fst'.dist hf.snd' -- QUOTE.</p> <p>/- TEXT: It's your turn now to prove some continuity lemma. After trying the continuity tactic, you will need <code>Continuous.add</code>, <code>continuous_pow</code> and <code>continuous_id</code> to do it by hand.</p> <p>BOTH: -/ -- QUOTE: example {f : \u211d \u2192 X} (hf : Continuous f) : Continuous fun x : \u211d \u21a6 f (x ^ 2 + x) :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example {f : \u211d \u2192 X} (hf : Continuous f) : Continuous fun x : \u211d \u21a6 f (x ^ 2 + x) :=   hf.comp &lt;| (continuous_pow 2).add continuous_id</p> <p>/- TEXT: So far we saw continuity as a global notion, but one can also define continuity at a point. BOTH: -/ -- QUOTE: example {X Y : Type*} [MetricSpace X] [MetricSpace Y] (f : X \u2192 Y) (a : X) :     ContinuousAt f a \u2194 \u2200 \u03b5 &gt; 0, \u2203 \u03b4 &gt; 0, \u2200 {x}, dist x a &lt; \u03b4 \u2192 dist (f x) (f a) &lt; \u03b5 :=   Metric.continuousAt_iff -- QUOTE.</p> <p>/- TEXT:</p> <p>Balls, open sets and closed sets <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>Once we have a distance function, the most important geometric definitions are (open) balls and closed balls.</p> <p>BOTH: -/ -- QUOTE: variable (r : \u211d)</p> <p>example : Metric.ball a r = { b | dist b a &lt; r } :=   rfl</p> <p>example : Metric.closedBall a r = { b | dist b a \u2264 r } :=   rfl -- QUOTE.</p> <p>/- TEXT: Note that <code>r</code> is any real number here, there is no sign restriction. Of course some statements do require a radius condition. BOTH: -/ -- QUOTE: example (hr : 0 &lt; r) : a \u2208 Metric.ball a r :=   Metric.mem_ball_self hr</p> <p>example (hr : 0 \u2264 r) : a \u2208 Metric.closedBall a r :=   Metric.mem_closedBall_self hr -- QUOTE.</p> <p>/- TEXT: Once we have balls, we can define open sets. They are actually defined in a more general setting covered in the next section, but we have lemmas recasting the definition in terms of balls.</p> <p>BOTH: -/ -- QUOTE: example (s : Set X) : IsOpen s \u2194 \u2200 x \u2208 s, \u2203 \u03b5 &gt; 0, Metric.ball x \u03b5 \u2286 s :=   Metric.isOpen_iff -- QUOTE.</p> <p>/- TEXT: Then closed sets are sets whose complement is open. Their important property is they are closed under limits. The closure of a set is the smallest closed set containing it. BOTH: -/ -- QUOTE: example {s : Set X} : IsClosed s \u2194 IsOpen (s\u1d9c) :=   isOpen_compl_iff.symm</p> <p>example {s : Set X} (hs : IsClosed s) {u : \u2115 \u2192 X} (hu : Tendsto u atTop (\ud835\udcdd a))     (hus : \u2200 n, u n \u2208 s) : a \u2208 s :=   hs.mem_of_tendsto hu (eventually_of_forall hus)</p> <p>example {s : Set X} : a \u2208 closure s \u2194 \u2200 \u03b5 &gt; 0, \u2203 b \u2208 s, a \u2208 Metric.ball b \u03b5 :=   Metric.mem_closure_iff -- QUOTE.</p> <p>/- TEXT: Do the next exercise without using <code>mem_closure_iff_seq_limit</code> BOTH: -/ -- QUOTE: example {u : \u2115 \u2192 X} (hu : Tendsto u atTop (\ud835\udcdd a)) {s : Set X} (hs : \u2200 n, u n \u2208 s) :     a \u2208 closure s :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example {u : \u2115 \u2192 X} (hu : Tendsto u atTop (\ud835\udcdd a)) {s : Set X} (hs : \u2200 n, u n \u2208 s) : a \u2208 closure s := by   rw [Metric.tendsto_atTop] at hu   rw [Metric.mem_closure_iff]   intro \u03b5 \u03b5_pos   rcases hu \u03b5 \u03b5_pos with \u27e8N, hN\u27e9   refine' \u27e8u N, hs _, _\u27e9   rw [dist_comm]   exact hN N le_rfl</p> <p>/- TEXT:</p> <p>Remember from the filters sections that neighborhood filters play a big role in Mathlib. In the metric space context, the crucial point is that balls provide bases for those filters. The main lemmas here are <code>Metric.nhds_basis_ball</code> and <code>Metric.nhds_basis_closedBall</code> that claim this for open and closed balls with positive radius. The center point is an implicit argument so we can invoke <code>Filter.HasBasis.mem_iff</code> as in the following example.</p> <p>BOTH: -/ -- QUOTE: example {x : X} {s : Set X} : s \u2208 \ud835\udcdd x \u2194 \u2203 \u03b5 &gt; 0, Metric.ball x \u03b5 \u2286 s :=   Metric.nhds_basis_ball.mem_iff</p> <p>example {x : X} {s : Set X} : s \u2208 \ud835\udcdd x \u2194 \u2203 \u03b5 &gt; 0, Metric.closedBall x \u03b5 \u2286 s :=   Metric.nhds_basis_closedBall.mem_iff -- QUOTE.</p> <p>/- TEXT:</p> <p>Compactness <sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>Compactness is an important topological notion. It distinguishes subsets of a metric space that enjoy the same kind of properties as segments in reals compared to other intervals:</p> <ul> <li>Any sequence taking value in a compact set has a subsequence that converges in this set</li> <li>Any continuous function on a nonempty compact set with values in real numbers is bounded and   achieves its bounds somewhere (this is called the extreme values theorem).</li> <li>Compact sets are closed sets.</li> </ul> <p>Let us first check that the unit interval in reals is indeed a compact set, and then check the above claims for compact sets in general metric spaces. In the second statement we only need continuity on the given set so we will use <code>ContinuousOn</code> instead of <code>Continuous</code>, and we will give separate statements for the minimum and the maximum. Of course all these results are deduced from more general versions, some of which will be discussed in later sections.</p> <p>BOTH: -/ -- QUOTE: example : IsCompact (Set.Icc 0 1 : Set \u211d) :=   isCompact_Icc</p> <p>example {s : Set X} (hs : IsCompact s) {u : \u2115 \u2192 X} (hu : \u2200 n, u n \u2208 s) :     \u2203 a \u2208 s, \u2203 \u03c6 : \u2115 \u2192 \u2115, StrictMono \u03c6 \u2227 Tendsto (u \u2218 \u03c6) atTop (\ud835\udcdd a) :=   hs.tendsto_subseq hu</p> <p>example {s : Set X} (hs : IsCompact s) (hs' : s.Nonempty) {f : X \u2192 \u211d}       (hfs : ContinuousOn f s) :     \u2203 x \u2208 s, \u2200 y \u2208 s, f x \u2264 f y :=   hs.exists_forall_le hs' hfs</p> <p>example {s : Set X} (hs : IsCompact s) (hs' : s.Nonempty) {f : X \u2192 \u211d}       (hfs : ContinuousOn f s) :     \u2203 x \u2208 s, \u2200 y \u2208 s, f y \u2264 f x :=   hs.exists_forall_ge hs' hfs</p> <p>example {s : Set X} (hs : IsCompact s) : IsClosed s :=   hs.isClosed -- QUOTE.</p> <p>/- TEXT:</p> <p>We can also specify that a metric spaces is globally compact, using an extra <code>Prop</code>-valued type class:</p> <p>BOTH: -/ -- QUOTE: example {X : Type*} [MetricSpace X] [CompactSpace X] : IsCompact (univ : Set X) :=   isCompact_univ -- QUOTE.</p> <p>/- TEXT:</p> <p>In a compact metric space any closed set is compact, this is <code>IsClosed.isCompact</code>.</p> <p>BOTH: -/</p>"},{"location":"MIL/C09_Topology/S02_Metric_Spaces/#check-iscompactisclosed","title":"check IsCompact.isClosed","text":"<p>/- TEXT: Uniformly continuous functions <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>We now turn to uniformity notions on metric spaces : uniformly continuous functions, Cauchy sequences and completeness. Again those are defined in a more general context but we have lemmas in the metric name space to access their elementary definitions. We start with uniform continuity.</p> <p>BOTH: -/ -- QUOTE: example {X : Type*} [MetricSpace X] {Y : Type*} [MetricSpace Y] {f : X \u2192 Y} :     UniformContinuous f \u2194       \u2200 \u03b5 &gt; 0, \u2203 \u03b4 &gt; 0, \u2200 {a b : X}, dist a b &lt; \u03b4 \u2192 dist (f a) (f b) &lt; \u03b5 :=   Metric.uniformContinuous_iff -- QUOTE.</p> <p>/- TEXT: In order to practice manipulating all those definitions, we will prove that continuous functions from a compact metric space to a metric space are uniformly continuous (we will see a more general version in a later section).</p> <p>We will first give an informal sketch. Let <code>f : X \u2192 Y</code> be a continuous function from a compact metric space to a metric space. We fix <code>\u03b5 &gt; 0</code> and start looking for some <code>\u03b4</code>.</p> <p>Let <code>\u03c6 : X \u00d7 X \u2192 \u211d := fun p \u21a6 dist (f p.1) (f p.2)</code> and let <code>K := { p : X \u00d7 X | \u03b5 \u2264 \u03c6 p }</code>. Observe <code>\u03c6</code> is continuous since <code>f</code> and distance are continuous. And <code>K</code> is clearly closed (use <code>isClosed_le</code>) hence compact since <code>X</code> is compact.</p> <p>Then we discuss two possibilities using <code>eq_empty_or_nonempty</code>. If <code>K</code> is empty then we are clearly done (we can set <code>\u03b4 = 1</code> for instance). So let's assume <code>K</code> is not empty, and use the extreme value theorem to choose <code>(x\u2080, x\u2081)</code> attaining the infimum of the distance function on <code>K</code>. We can then set <code>\u03b4 = dist x\u2080 x\u2081</code> and check everything works.</p> <p>BOTH: -/ -- QUOTE: example {X : Type*} [MetricSpace X] [CompactSpace X]       {Y : Type*} [MetricSpace Y] {f : X \u2192 Y}     (hf : Continuous f) : UniformContinuous f :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example {X : Type*} [MetricSpace X] [CompactSpace X] {Y : Type*} [MetricSpace Y] {f : X \u2192 Y}     (hf : Continuous f) : UniformContinuous f := by   rw [Metric.uniformContinuous_iff]   intro \u03b5 \u03b5_pos   let \u03c6 : X \u00d7 X \u2192 \u211d := fun p \u21a6 dist (f p.1) (f p.2)   have \u03c6_cont : Continuous \u03c6 := hf.fst'.dist hf.snd'   let K := { p : X \u00d7 X | \u03b5 \u2264 \u03c6 p }   have K_closed : IsClosed K := isClosed_le continuous_const \u03c6_cont   have K_cpct : IsCompact K := K_closed.isCompact   rcases eq_empty_or_nonempty K with hK | hK   \u00b7 use 1, by norm_num     intro x y _     have : (x, y) \u2209 K := by simp [hK]     simpa [K] using this   \u00b7 rcases K_cpct.exists_forall_le hK continuous_dist.continuousOn with \u27e8\u27e8x\u2080, x\u2081\u27e9, xx_in, H\u27e9     use dist x\u2080 x\u2081     constructor     \u00b7 change _ &lt; _       rw [dist_pos]       intro h       have : \u03b5 \u2264 0 := by simpa [K, \u03c6, *] using xx_in       linarith     \u00b7 intro x x'       contrapose!       intro hxx'       exact H (x, x') hxx'</p> <p>/- TEXT: Completeness <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>A Cauchy sequence in a metric space is a sequence whose terms get closer and closer to each other. There are a couple of equivalent ways to state that idea. In particular converging sequences are Cauchy. The converse is true only in so-called complete spaces.</p> <p>BOTH: -/ -- QUOTE: example (u : \u2115 \u2192 X) :     CauchySeq u \u2194 \u2200 \u03b5 &gt; 0, \u2203 N : \u2115, \u2200 m \u2265 N, \u2200 n \u2265 N, dist (u m) (u n) &lt; \u03b5 :=   Metric.cauchySeq_iff</p> <p>example (u : \u2115 \u2192 X) :     CauchySeq u \u2194 \u2200 \u03b5 &gt; 0, \u2203 N : \u2115, \u2200 n \u2265 N, dist (u n) (u N) &lt; \u03b5 :=   Metric.cauchySeq_iff'</p> <p>example [CompleteSpace X] (u : \u2115 \u2192 X) (hu : CauchySeq u) :     \u2203 x, Tendsto u atTop (\ud835\udcdd x) :=   cauchySeq_tendsto_of_complete hu -- QUOTE.</p> <p>/- TEXT:</p> <p>We'll practice using this definition by proving a convenient criterion which is a special case of a criterion appearing in Mathlib. This is also a good opportunity to practice using big sums in a geometric context. In addition to the explanations from the filters section, you will probably need <code>tendsto_pow_atTop_nhds_0_of_lt_1</code>, <code>Tendsto.mul</code> and <code>dist_le_range_sum_dist</code>. BOTH: -/ open BigOperators</p> <p>open Finset</p> <p>-- QUOTE: theorem cauchySeq_of_le_geometric_two' {u : \u2115 \u2192 X}     (hu : \u2200 n : \u2115, dist (u n) (u (n + 1)) \u2264 (1 / 2) ^ n) : CauchySeq u := by   rw [Metric.cauchySeq_iff']   intro \u03b5 \u03b5_pos   obtain \u27e8N, hN\u27e9 : \u2203 N : \u2115, 1 / 2 ^ N * 2 &lt; \u03b5 := by sorry   use N   intro n hn   obtain \u27e8k, rfl : n = N + k\u27e9 := le_iff_exists_add.mp hn   calc     dist (u (N + k)) (u N) = dist (u (N + 0)) (u (N + k)) := sorry     _ \u2264 \u2211 i in range k, dist (u (N + i)) (u (N + (i + 1))) := sorry     _ \u2264 \u2211 i in range k, (1 / 2 : \u211d) ^ (N + i) := sorry     _ = 1 / 2 ^ N * \u2211 i in range k, (1 / 2 : \u211d) ^ i := sorry     _ \u2264 1 / 2 ^ N * 2 := sorry     _ &lt; \u03b5 := sorry</p> <p>-- QUOTE.</p> <p>-- SOLUTIONS: example {u : \u2115 \u2192 X} (hu : \u2200 n : \u2115, dist (u n) (u (n + 1)) \u2264 (1 / 2) ^ n) : CauchySeq u := by   rw [Metric.cauchySeq_iff']   intro \u03b5 \u03b5_pos   obtain \u27e8N, hN\u27e9 : \u2203 N : \u2115, 1 / 2 ^ N * 2 &lt; \u03b5 := by     have : Tendsto (fun N : \u2115 \u21a6 (1 / 2 ^ N * 2 : \u211d)) atTop (\ud835\udcdd 0) := by       rw [\u2190 zero_mul (2 : \u211d)]       apply Tendsto.mul       simp_rw [\u2190 one_div_pow (2 : \u211d)]       apply tendsto_pow_atTop_nhds_0_of_lt_1 &lt;;&gt; linarith       exact tendsto_const_nhds     rcases(atTop_basis.tendsto_iff (nhds_basis_Ioo_pos (0 : \u211d))).mp this \u03b5 \u03b5_pos with \u27e8N, _, hN\u27e9     exact \u27e8N, by simpa using (hN N left_mem_Ici).2\u27e9   use N   intro n hn   obtain \u27e8k, rfl : n = N + k\u27e9 := le_iff_exists_add.mp hn   calc     dist (u (N + k)) (u N) = dist (u (N + 0)) (u (N + k)) := by rw [dist_comm, add_zero]     _ \u2264 \u2211 i in range k, dist (u (N + i)) (u (N + (i + 1))) :=       (dist_le_range_sum_dist (fun i \u21a6 u (N + i)) k)     _ \u2264 \u2211 i in range k, (1 / 2 : \u211d) ^ (N + i) := (sum_le_sum fun i _ \u21a6 hu &lt;| N + i)     _ = 1 / 2 ^ N * \u2211 i in range k, (1 / 2 : \u211d) ^ i := by simp_rw [\u2190 one_div_pow, pow_add, \u2190 mul_sum]     _ \u2264 1 / 2 ^ N * 2 :=       (mul_le_mul_of_nonneg_left (sum_geometric_two_le _)         (one_div_nonneg.mpr (pow_nonneg (zero_le_two : (0 : \u211d) \u2264 2) _)))     _ &lt; \u03b5 := hN</p> <p>/- TEXT:</p> <p>We are ready for the final boss of this section: Baire's theorem for complete metric spaces! The proof skeleton below shows interesting techniques. It uses the <code>choose</code> tactic in its exclamation mark variant (you should experiment with removing this exclamation mark) and it shows how to define something inductively in the middle of a proof using <code>Nat.rec_on</code>.</p> <p>BOTH: -/ -- QUOTE: open Metric</p> <p>example [CompleteSpace X] (f : \u2115 \u2192 Set X) (ho : \u2200 n, IsOpen (f n)) (hd : \u2200 n, Dense (f n)) :     Dense (\u22c2 n, f n) := by   let B : \u2115 \u2192 \u211d := fun n \u21a6 (1 / 2) ^ n   have Bpos : \u2200 n, 0 &lt; B n   sorry   /- Translate the density assumption into two functions <code>center</code> and <code>radius</code> associating     to any n, x, \u03b4, \u03b4pos a center and a positive radius such that     <code>closedBall center radius</code> is included both in <code>f n</code> and in <code>closedBall x \u03b4</code>.     We can also require <code>radius \u2264 (1/2)^(n+1)</code>, to ensure we get a Cauchy sequence later. -/   have :     \u2200 (n : \u2115) (x : X),       \u2200 \u03b4 &gt; 0, \u2203 y : X, \u2203 r &gt; 0, r \u2264 B (n + 1) \u2227 closedBall y r \u2286 closedBall x \u03b4 \u2229 f n :=     by sorry   choose! center radius Hpos HB Hball using this   intro x   rw [mem_closure_iff_nhds_basis nhds_basis_closedBall]   intro \u03b5 \u03b5pos   /- <code>\u03b5</code> is positive. We have to find a point in the ball of radius <code>\u03b5</code> around <code>x</code>     belonging to all <code>f n</code>. For this, we construct inductively a sequence     <code>F n = (c n, r n)</code> such that the closed ball <code>closedBall (c n) (r n)</code> is included     in the previous ball and in <code>f n</code>, and such that <code>r n</code> is small enough to ensure     that <code>c n</code> is a Cauchy sequence. Then <code>c n</code> converges to a limit which belongs     to all the <code>f n</code>. -/   let F : \u2115 \u2192 X \u00d7 \u211d := fun n \u21a6     Nat.recOn n (Prod.mk x (min \u03b5 (B 0)))       fun n p \u21a6 Prod.mk (center n p.1 p.2) (radius n p.1 p.2)   let c : \u2115 \u2192 X := fun n \u21a6 (F n).1   let r : \u2115 \u2192 \u211d := fun n \u21a6 (F n).2   have rpos : \u2200 n, 0 &lt; r n := by sorry   have rB : \u2200 n, r n \u2264 B n := by sorry   have incl : \u2200 n, closedBall (c (n + 1)) (r (n + 1)) \u2286 closedBall (c n) (r n) \u2229 f n := by     sorry   have cdist : \u2200 n, dist (c n) (c (n + 1)) \u2264 B n := by sorry   have : CauchySeq c := cauchySeq_of_le_geometric_two' cdist   -- as the sequence <code>c n</code> is Cauchy in a complete space, it converges to a limit <code>y</code>.   rcases cauchySeq_tendsto_of_complete this with \u27e8y, ylim\u27e9   -- this point <code>y</code> will be the desired point. We will check that it belongs to all   -- <code>f n</code> and to <code>ball x \u03b5</code>.   use y   have I : \u2200 n, \u2200 m \u2265 n, closedBall (c m) (r m) \u2286 closedBall (c n) (r n) := by sorry   have yball : \u2200 n, y \u2208 closedBall (c n) (r n) := by sorry   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example [CompleteSpace X] (f : \u2115 \u2192 Set X) (ho : \u2200 n, IsOpen (f n)) (hd : \u2200 n, Dense (f n)) :     Dense (\u22c2 n, f n) := by   let B : \u2115 \u2192 \u211d := fun n \u21a6 (1 / 2) ^ n   have Bpos : \u2200 n, 0 &lt; B n := fun n \u21a6 pow_pos sorry n   /- Translate the density assumption into two functions <code>center</code> and <code>radius</code> associating     to any n, x, \u03b4, \u03b4pos a center and a positive radius such that     <code>closedBall center radius</code> is included both in <code>f n</code> and in <code>closedBall x \u03b4</code>.     We can also require <code>radius \u2264 (1/2)^(n+1)</code>, to ensure we get a Cauchy sequence later. -/   have :     \u2200 (n : \u2115) (x : X),       \u2200 \u03b4 &gt; 0, \u2203 y : X, \u2203 r &gt; 0, r \u2264 B (n + 1) \u2227 closedBall y r \u2286 closedBall x \u03b4 \u2229 f n := by     intro n x \u03b4 \u03b4pos     have : x \u2208 closure (f n) := hd n x     rcases Metric.mem_closure_iff.1 this (\u03b4 / 2) (half_pos \u03b4pos) with \u27e8y, ys, xy\u27e9     rw [dist_comm] at xy     obtain \u27e8r, rpos, hr\u27e9 : \u2203 r &gt; 0, closedBall y r \u2286 f n :=       nhds_basis_closedBall.mem_iff.1 (isOpen_iff_mem_nhds.1 (ho n) y ys)     refine' \u27e8y, min (min (\u03b4 / 2) r) (B (n + 1)), , _, fun z hz \u21a6 \u27e8, _\u27e9\u27e9     show 0 &lt; min (min (\u03b4 / 2) r) (B (n + 1))     exact lt_min (lt_min (half_pos \u03b4pos) rpos) (Bpos (n + 1))     show min (min (\u03b4 / 2) r) (B (n + 1)) \u2264 B (n + 1)     exact min_le_right _ _     show z \u2208 closedBall x \u03b4     exact       calc         dist z x \u2264 dist z y + dist y x := dist_triangle _ _ _         _ \u2264 min (min (\u03b4 / 2) r) (B (n + 1)) + \u03b4 / 2 := (add_le_add hz xy.le)         _ \u2264 \u03b4 / 2 + \u03b4 / 2 := (add_le_add_right ((min_le_left _ _).trans (min_le_left _ _)) _)         _ = \u03b4 := add_halves \u03b4</p> <pre><code>show z \u2208 f n\nexact\n  hr\n    (calc\n      dist z y \u2264 min (min (\u03b4 / 2) r) (B (n + 1)) := hz\n      _ \u2264 r := (min_le_left _ _).trans (min_le_right _ _)\n      )\n</code></pre> <p>choose! center radius Hpos HB Hball using this   refine' fun x \u21a6 (mem_closure_iff_nhds_basis nhds_basis_closedBall).2 fun \u03b5 \u03b5pos \u21a6 _   /- <code>\u03b5</code> is positive. We have to find a point in the ball of radius <code>\u03b5</code> around <code>x</code> belonging to all     <code>f n</code>. For this, we construct inductively a sequence <code>F n = (c n, r n)</code> such that the closed ball     <code>closedBall (c n) (r n)</code> is included in the previous ball and in <code>f n</code>, and such that     <code>r n</code> is small enough to ensure that <code>c n</code> is a Cauchy sequence. Then <code>c n</code> converges to a     limit which belongs to all the <code>f n</code>. -/   let F : \u2115 \u2192 X \u00d7 \u211d := fun n \u21a6     Nat.recOn n (Prod.mk x (min \u03b5 (B 0))) fun n p \u21a6 Prod.mk (center n p.1 p.2) (radius n p.1 p.2)   let c : \u2115 \u2192 X := fun n \u21a6 (F n).1   let r : \u2115 \u2192 \u211d := fun n \u21a6 (F n).2   have rpos : \u2200 n, 0 &lt; r n := by     intro n     induction' n with n hn     exact lt_min \u03b5pos (Bpos 0)     exact Hpos n (c n) (r n) hn   have rB : \u2200 n, r n \u2264 B n := by     intro n     induction' n with n hn     exact min_le_right _ _     exact HB n (c n) (r n) (rpos n)   have incl : \u2200 n, closedBall (c (n + 1)) (r (n + 1)) \u2286 closedBall (c n) (r n) \u2229 f n := fun n \u21a6     Hball n (c n) (r n) (rpos n)   have cdist : \u2200 n, dist (c n) (c (n + 1)) \u2264 B n := by     intro n     rw [dist_comm]     have A : c (n + 1) \u2208 closedBall (c (n + 1)) (r (n + 1)) :=       mem_closedBall_self (rpos &lt;| n + 1).le     have I :=       calc         closedBall (c (n + 1)) (r (n + 1)) \u2286 closedBall (c n) (r n) :=           (incl n).trans Set.inter_subset_left         _ \u2286 closedBall (c n) (B n) := closedBall_subset_closedBall (rB n)</p> <pre><code>exact I A\n</code></pre> <p>have : CauchySeq c := cauchySeq_of_le_geometric_two' cdist   -- as the sequence <code>c n</code> is Cauchy in a complete space, it converges to a limit <code>y</code>.   rcases cauchySeq_tendsto_of_complete this with \u27e8y, ylim\u27e9   -- this point <code>y</code> will be the desired point. We will check that it belongs to all   -- <code>f n</code> and to <code>ball x \u03b5</code>.   use y   have I : \u2200 n, \u2200 m \u2265 n, closedBall (c m) (r m) \u2286 closedBall (c n) (r n) := by     intro n     refine' Nat.le_induction _ fun m hnm h \u21a6 _     \u00b7 exact Subset.rfl     \u00b7 exact (incl m).trans (Set.inter_subset_left.trans h)   have yball : \u2200 n, y \u2208 closedBall (c n) (r n) := by     intro n     refine' isClosed_ball.mem_of_tendsto ylim _     refine' (Filter.eventually_ge_atTop n).mono fun m hm \u21a6 _     exact I n m hm (mem_closedBall_self (rpos _).le)   constructor   \u00b7 suffices \u2200 n, y \u2208 f n by rwa [Set.mem_iInter]     intro n     have : closedBall (c (n + 1)) (r (n + 1)) \u2286 f n :=       Subset.trans (incl n) Set.inter_subset_right     exact this (yball (n + 1))   calc     dist y x \u2264 r 0 := yball 0     _ \u2264 \u03b5 := min_le_left _ _</p> <p>/- TEXT:</p> <p>BOTH: -/</p>"},{"location":"MIL/C09_Topology/S03_Topological_Spaces/","title":"S03 Topological Spaces","text":"<p>import MIL.Common import Mathlib.Topology.Instances.Real import Mathlib.Analysis.NormedSpace.BanachSteinhaus</p> <p>open Set Filter Topology</p> <p>/- TEXT: .. index:: topological space</p> <p>.. _topological_spaces:</p>"},{"location":"MIL/C09_Topology/S03_Topological_Spaces/#topological-spaces","title":"Topological spaces","text":"<p>Fundamentals <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>We now go up in generality and introduce topological spaces. We will review the two main ways to define topological spaces and then explain how the category of topological spaces is much better behaved than the category of metric spaces. Note that we won't be using Mathlib category theory here, only having a somewhat categorical point of view.</p> <p>The first way to think about the transition from metric spaces to topological spaces is that we only remember the notion of open sets (or equivalently the notion of closed sets). From this point of view, a topological space is a type equipped with a collection of sets that are called open sets. This collection has to satisfy a number of axioms presented below (this collection is slightly redundant but we will ignore that). BOTH: -/ -- QUOTE: section variable {X : Type*} [TopologicalSpace X]</p> <p>example : IsOpen (univ : Set X) :=   isOpen_univ</p> <p>example : IsOpen (\u2205 : Set X) :=   isOpen_empty</p> <p>example {\u03b9 : Type*} {s : \u03b9 \u2192 Set X} (hs : \u2200 i, IsOpen (s i)) : IsOpen (\u22c3 i, s i) :=   isOpen_iUnion hs</p> <p>example {\u03b9 : Type*} [Fintype \u03b9] {s : \u03b9 \u2192 Set X} (hs : \u2200 i, IsOpen (s i)) :     IsOpen (\u22c2 i, s i) :=   isOpen_iInter_of_finite hs -- QUOTE.</p> <p>/- TEXT:</p> <p>Closed sets are then defined as sets whose complement  is open. A function between topological spaces is (globally) continuous if all preimages of open sets are open. BOTH: -/ -- QUOTE: variable {Y : Type*} [TopologicalSpace Y]</p> <p>example {f : X \u2192 Y} : Continuous f \u2194 \u2200 s, IsOpen s \u2192 IsOpen (f \u207b\u00b9' s) :=   continuous_def -- QUOTE.</p> <p>/- TEXT: With this definition we already see that, compared to metric spaces, topological spaces only remember enough information to talk about continuous functions: two topological structures on a type are the same if and only if they have the same continuous functions (indeed the identity function will be continuous in both direction if and only if the two structures have the same open sets).</p> <p>However as soon as we move on to continuity at a point we see the limitations of the approach based on open sets. In Mathlib we frequently think of topological spaces as types equipped with a neighborhood filter <code>\ud835\udcdd x</code> attached to each point <code>x</code> (the corresponding function <code>X \u2192 Filter X</code> satisfies certain conditions explained further down). Remember from the filters section that these gadgets play two related roles. First <code>\ud835\udcdd x</code> is seen as the generalized set of points of <code>X</code> that are close to <code>x</code>. And then it is seen as giving a way to say, for any predicate <code>P : X \u2192 Prop</code>, that this predicate holds for points that are close enough to <code>x</code>. Let us state that <code>f : X \u2192 Y</code> is continuous at <code>x</code>. The purely filtery way is to say that the direct image under <code>f</code> of the generalized set of points that are close to <code>x</code> is contained in the generalized set of points that are close to <code>f x</code>. Recall this is spelled either <code>map f (\ud835\udcdd x) \u2264 \ud835\udcdd (f x)</code> or <code>Tendsto f (\ud835\udcdd x) (\ud835\udcdd (f x))</code>.</p> <p>BOTH: -/ -- QUOTE: example {f : X \u2192 Y} {x : X} : ContinuousAt f x \u2194 map f (\ud835\udcdd x) \u2264 \ud835\udcdd (f x) :=   Iff.rfl -- QUOTE.</p> <p>/- TEXT: One can also spell it using both neighborhoods seen as ordinary sets and a neighborhood filter seen as a generalized set: \"for any neighborhood <code>U</code> of <code>f x</code>, all points close to <code>x</code> are sent to <code>U</code>\". Note that the proof is again <code>iff.rfl</code>, this point of view is definitionally equivalent to the previous one.</p> <p>BOTH: -/ -- QUOTE: example {f : X \u2192 Y} {x : X} : ContinuousAt f x \u2194 \u2200 U \u2208 \ud835\udcdd (f x), \u2200\u1da0 x in \ud835\udcdd x, f x \u2208 U :=   Iff.rfl -- QUOTE.</p> <p>/- TEXT: We now explain how to go from one point of view to the other. In terms of open sets, we can simply define members of <code>\ud835\udcdd x</code> as sets that contain an open set containing <code>x</code>.</p> <p>BOTH: -/ -- QUOTE: example {x : X} {s : Set X} : s \u2208 \ud835\udcdd x \u2194 \u2203 t, t \u2286 s \u2227 IsOpen t \u2227 x \u2208 t :=   mem_nhds_iff -- QUOTE.</p> <p>/- TEXT: To go in the other direction we need to discuss the condition that <code>\ud835\udcdd : X \u2192 Filter X</code> must satisfy in order to be the neighborhood function of a topology.</p> <p>The first constraint is that <code>\ud835\udcdd x</code>, seen as a generalized set, contains the set <code>{x}</code> seen as the generalized set <code>pure x</code> (explaining this weird name would be too much of a digression, so we simply accept it for now). Another way to say it is that if a predicate holds for points close to <code>x</code> then it holds at <code>x</code>.</p> <p>BOTH: -/ -- QUOTE: example (x : X) : pure x \u2264 \ud835\udcdd x :=   pure_le_nhds x</p> <p>example (x : X) (P : X \u2192 Prop) (h : \u2200\u1da0 y in \ud835\udcdd x, P y) : P x :=   h.self_of_nhds -- QUOTE.</p> <p>/- TEXT: Then a more subtle requirement is that, for any predicate <code>P : X \u2192 Prop</code> and any <code>x</code>, if <code>P y</code> holds for <code>y</code> close to <code>x</code> then for <code>y</code> close to <code>x</code> and <code>z</code> close to <code>y</code>, <code>P z</code> holds. More precisely we have: BOTH: -/ -- QUOTE: example {P : X \u2192 Prop} {x : X} (h : \u2200\u1da0 y in \ud835\udcdd x, P y) : \u2200\u1da0 y in \ud835\udcdd x, \u2200\u1da0 z in \ud835\udcdd y, P z :=   eventually_eventually_nhds.mpr h -- QUOTE.</p> <p>/- TEXT: Those two results characterize the functions <code>X \u2192 Filter X</code> that are neighborhood functions for a topological space structure on <code>X</code>. There is a still a function <code>TopologicalSpace.mkOfNhds : (X \u2192 Filter X) \u2192 TopologicalSpace X</code> but it will give back its input as a neighborhood function only if it satisfies the above two constraints. More precisely we have a lemma <code>TopologicalSpace.nhds_mkOfNhds</code> saying that in a different way and our next exercise deduces this different way from how we stated it above. BOTH: -/</p>"},{"location":"MIL/C09_Topology/S03_Topological_Spaces/#check-topologicalspacemkofnhds","title":"check TopologicalSpace.mkOfNhds","text":""},{"location":"MIL/C09_Topology/S03_Topological_Spaces/#check-topologicalspacenhds_mkofnhds","title":"check TopologicalSpace.nhds_mkOfNhds","text":"<p>-- QUOTE: example {\u03b1 : Type*} (n : \u03b1 \u2192 Filter \u03b1) (H\u2080 : \u2200 a, pure a \u2264 n a)     (H : \u2200 a : \u03b1, \u2200 p : \u03b1 \u2192 Prop, (\u2200\u1da0 x in n a, p x) \u2192 \u2200\u1da0 y in n a, \u2200\u1da0 x in n y, p x) :     \u2200 a, \u2200 s \u2208 n a, \u2203 t \u2208 n a, t \u2286 s \u2227 \u2200 a' \u2208 t, s \u2208 n a' :=   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example {\u03b1 : Type*} (n : \u03b1 \u2192 Filter \u03b1) (H\u2080 : \u2200 a, pure a \u2264 n a)     (H : \u2200 a : \u03b1, \u2200 p : \u03b1 \u2192 Prop, (\u2200\u1da0 x in n a, p x) \u2192 \u2200\u1da0 y in n a, \u2200\u1da0 x in n y, p x) :     \u2200 a, \u2200 s \u2208 n a, \u2203 t \u2208 n a, t \u2286 s \u2227 \u2200 a' \u2208 t, s \u2208 n a' := by   intro a s s_in   refine' \u27e8{ y | s \u2208 n y }, H a (fun x \u21a6 x \u2208 s) s_in, _, by tauto\u27e9   rintro y (hy : s \u2208 n y)   exact H\u2080 y hy</p> <p>-- BOTH: end</p> <p>-- BOTH. /- TEXT: Note that <code>TopologicalSpace.mkOfNhds</code> is not so frequently used, but it still good to know in what precise sense the neighborhood filters is all there is in a topological space structure.</p> <p>The next thing to know in order to efficiently use topological spaces in Mathlib is that we use a lot of formal properties of <code>TopologicalSpace : Type u \u2192 Type u</code>. From a purely mathematical point of view, those formal properties are a very clean way to explain how topological spaces solve issues that metric spaces have. From this point of view, the issues solved by topological spaces is that metric spaces enjoy very little functoriality, and have very bad categorical properties in general. This comes on top of the fact already discussed that metric spaces contain a lot of geometrical information that is not topologically relevant.</p> <p>Let us focus on functoriality first. A metric space structure can be induced on a subset or, equivalently, it can be pulled back by an injective map. But that's pretty much everything. They cannot be pulled back by general map or pushed forward, even by surjective maps.</p> <p>In particular there is no sensible distance to put on a quotient of a metric space or on an uncountable products of metric spaces. Consider for instance the type <code>\u211d \u2192 \u211d</code>, seen as a product of copies of <code>\u211d</code> indexed by <code>\u211d</code>. We would like to say that pointwise convergence of sequences of functions is a respectable notion of convergence. But there is no distance on <code>\u211d \u2192 \u211d</code> that gives this notion of convergence. Relatedly, there is no distance ensuring that a map <code>f : X \u2192 (\u211d \u2192 \u211d)</code> is continuous if and only if <code>fun x \u21a6 f x t</code> is continuous for every <code>t : \u211d</code>.</p> <p>We now review the data used to solve all those issues. First we can use any map <code>f : X \u2192 Y</code> to push or pull topologies from one side to the other. Those two operations form a Galois connection.</p> <p>BOTH: -/ -- QUOTE: variable {X Y : Type*}</p> <p>example (f : X \u2192 Y) : TopologicalSpace X \u2192 TopologicalSpace Y :=   TopologicalSpace.coinduced f</p> <p>example (f : X \u2192 Y) : TopologicalSpace Y \u2192 TopologicalSpace X :=   TopologicalSpace.induced f</p> <p>example (f : X \u2192 Y) (T_X : TopologicalSpace X) (T_Y : TopologicalSpace Y) :     TopologicalSpace.coinduced f T_X \u2264 T_Y \u2194 T_X \u2264 TopologicalSpace.induced f T_Y :=   coinduced_le_iff_le_induced -- QUOTE.</p> <p>/- TEXT: Those operations are compactible with composition of functions. As usual, pushing forward is covariant and pulling back is contravariant, see <code>coinduced_compose</code> and <code>induced_compose</code>. On paper we will use notations :math:<code>f_*T</code> for <code>TopologicalSpace.coinduced f T</code> and :math:<code>f^*T</code> for <code>TopologicalSpace.induced f T</code>. BOTH: -/</p>"},{"location":"MIL/C09_Topology/S03_Topological_Spaces/#check-coinduced_compose","title":"check coinduced_compose","text":""},{"location":"MIL/C09_Topology/S03_Topological_Spaces/#check-induced_compose","title":"check induced_compose","text":"<p>/- TEXT:</p> <p>Then the next big piece is a complete lattice structure on <code>TopologicalSpace X</code> for any given structure. If you think of topologies as being primarily the data of open sets then you expect the order relation on <code>TopologicalSpace X</code> to come from <code>Set (Set X)</code>, ie you expect <code>t \u2264 t'</code> if a set <code>u</code> is open for <code>t'</code> as soon as it is open for <code>t</code>. However we already know that Mathlib focuses on neighborhoods more than open sets so, for any <code>x : X</code> we want the map from topological spaces to neighborhoods <code>fun T : TopologicalSpace X \u21a6 @nhds X T x</code> to be order preserving. And we know the order relation on <code>Filter X</code> is designed to ensure an order preserving <code>principal : Set X \u2192 Filter X</code>, allowing to see filters as generalized sets. So the order relation we do use on  <code>TopologicalSpace X</code> is opposite to the one coming from <code>Set (Set X)</code>.</p> <p>BOTH: -/ -- QUOTE: example {T T' : TopologicalSpace X} : T \u2264 T' \u2194 \u2200 s, T'.IsOpen s \u2192 T.IsOpen s :=   Iff.rfl -- QUOTE.</p> <p>/- TEXT:</p> <p>Now we can recover continuity by combining the push-forward (or pull-back) operation with the order relation.</p> <p>BOTH: -/ -- QUOTE: example (T_X : TopologicalSpace X) (T_Y : TopologicalSpace Y) (f : X \u2192 Y) :     Continuous f \u2194 TopologicalSpace.coinduced f T_X \u2264 T_Y :=   continuous_iff_coinduced_le -- QUOTE.</p> <p>/- TEXT: With this definition and the compatibility of push-forward and composition, we get for free the universal property that, for any topological space :math:<code>Z</code>, a function :math:<code>g : Y \u2192 Z</code> is continuous for the topology :math:<code>f_*T_X</code> if and only if :math:<code>g \u2218 f</code> is continuous.</p> <p>.. math::   g \\text{ continuous } &amp;\u21d4 g_(f_*T_X) \u2264 T_Z \\   &amp;\u21d4 (g \u2218 f)_ T_X \u2264 T_Z \\   &amp;\u21d4 g \u2218 f \\text{ continuous}</p> <p>BOTH: -/ -- QUOTE: example {Z : Type*} (f : X \u2192 Y) (T_X : TopologicalSpace X) (T_Z : TopologicalSpace Z)       (g : Y \u2192 Z) :     @Continuous Y Z (TopologicalSpace.coinduced f T_X) T_Z g \u2194       @Continuous X Z T_X T_Z (g \u2218 f) := by   rw [continuous_iff_coinduced_le, coinduced_compose, continuous_iff_coinduced_le] -- QUOTE.</p> <p>/- TEXT: So we already get quotient topologies (using the projection map as <code>f</code>). This wasn't using that <code>TopologicalSpace X</code> is a complete lattice for all <code>X</code>. Let's now see how all this structure proves the existence of the product topology by abstract non-sense. We considered the case of <code>\u211d \u2192 \u211d</code> above, but let's now consider the general case of <code>\u03a0 i, X i</code> for some <code>\u03b9 : Type*</code> and <code>X : \u03b9 \u2192 Type*</code>. We want, for any topological space <code>Z</code> and any function <code>f : Z \u2192 \u03a0 i, X i</code>, that <code>f</code> is continuous if and only if <code>(fun x \u21a6 x i) \u2218 f</code> is continuous for all <code>i</code>. Let us explore that constraint \"on paper\" using notation :math:<code>p_i</code> for the projection <code>(fun (x : \u03a0 i, X i) \u21a6 x i)</code>:</p> <p>.. math::   (\u2200 i, p_i \u2218 f \\text{ continuous}) &amp;\u21d4 \u2200 i, (p_i \u2218 f)* T_Z \u2264 T{X_i} \\   &amp;\u21d4 \u2200 i, (p_i)* f* T_Z \u2264 T_{X_i}\\   &amp;\u21d4 \u2200 i, f_* T_Z \u2264 (p_i)^T_{X_i}\\   &amp;\u21d4  f_ T_Z \u2264 \\inf \\left[(p_i)^*T_{X_i}\\right]</p> <p>So we see that what is the topology we want on <code>\u03a0 i, X i</code>: BOTH: -/ -- QUOTE: example (\u03b9 : Type*) (X : \u03b9 \u2192 Type*) (T_X : \u2200 i, TopologicalSpace (X i)) :     (Pi.topologicalSpace : TopologicalSpace (\u2200 i, X i)) =       \u2a05 i, TopologicalSpace.induced (fun x \u21a6 x i) (T_X i) :=   rfl -- QUOTE.</p> <p>/- TEXT:</p> <p>This ends our tour of how Mathlib thinks that topological spaces fix defects of the theory of metric spaces by being a more functorial theory and having a complete lattice structure for any fixed type.</p> <p>Separation and countability <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup></p> <p>We saw that the category of topological spaces have very nice properties. The price to pay for this is existence of rather pathological topological spaces. There are a number of assumptions you can make on a topological space to ensure its behavior is closer to what metric spaces do. The most important is <code>T2Space</code>, also called \"Hausdorff\", that will ensure that limits are unique. A stronger separation property is <code>T3Space</code> that ensures in addition the <code>RegularSpace</code> property: each point has a basis of closed neighborhoods.</p> <p>BOTH: -/ -- QUOTE: example [TopologicalSpace X] [T2Space X] {u : \u2115 \u2192 X} {a b : X} (ha : Tendsto u atTop (\ud835\udcdd a))     (hb : Tendsto u atTop (\ud835\udcdd b)) : a = b :=   tendsto_nhds_unique ha hb</p> <p>example [TopologicalSpace X] [RegularSpace X] (a : X) :     (\ud835\udcdd a).HasBasis (fun s : Set X \u21a6 s \u2208 \ud835\udcdd a \u2227 IsClosed s) id :=   closed_nhds_basis a -- QUOTE.</p> <p>/- TEXT: Note that, in every topological space, each point has a basis of open neighborhood, by definition.</p> <p>BOTH: -/ -- QUOTE: example [TopologicalSpace X] {x : X} :     (\ud835\udcdd x).HasBasis (fun t : Set X \u21a6 t \u2208 \ud835\udcdd x \u2227 IsOpen t) id :=   nhds_basis_opens' x -- QUOTE.</p> <p>/- TEXT: Our main goal is now to prove the basic theorem which allows extension by continuity. From Bourbaki's general topology book, I.8.5, Theorem 1 (taking only the non-trivial implication):</p> <p>Let :math:<code>X</code> be a topological space, :math:<code>A</code> a dense subset of :math:<code>X</code>, :math:<code>f : A \u2192 Y</code> a continuous mapping of :math:<code>A</code> into a :math:<code>T_3</code> space :math:<code>Y</code>. If, for each :math:<code>x</code> in :math:<code>X</code>, :math:<code>f(y)</code> tends to a limit in :math:<code>Y</code> when :math:<code>y</code> tends to :math:<code>x</code> while remaining in :math:<code>A</code> then there exists a continuous extension :math:<code>\u03c6</code> of :math:<code>f</code> to :math:<code>X</code>.</p> <p>Actually Mathlib contains a more general version of the above lemma, <code>DenseInducing.continuousAt_extend</code>, but we'll stick to Bourbaki's version here.</p> <p>Remember that, given <code>A : Set X</code>, <code>\u21a5A</code> is the subtype associated to <code>A</code>, and Lean will automatically insert that funny up arrow when needed. And the (inclusion) coercion map is <code>(\u2191) : A \u2192 X</code>. The assumption \"tends to :math:<code>x</code> while remaining in :math:<code>A</code>\" corresponds to the pull-back filter <code>comap (\u2191) (\ud835\udcdd x)</code>.</p> <p>Let's first prove an auxiliary lemma, extracted to simplify the context (in particular we don't need Y to be a topological space here).</p> <p>BOTH: -/ -- QUOTE: theorem aux {X Y A : Type*} [TopologicalSpace X] {c : A \u2192 X}       {f : A \u2192 Y} {x : X} {F : Filter Y}       (h : Tendsto f (comap c (\ud835\udcdd x)) F) {V' : Set Y} (V'_in : V' \u2208 F) :     \u2203 V \u2208 \ud835\udcdd x, IsOpen V \u2227 c \u207b\u00b9' V \u2286 f \u207b\u00b9' V' := by /- EXAMPLES:   sorry</p> <p>SOLUTIONS: -/   simpa [and_assoc] using ((nhds_basis_opens' x).comap c).tendsto_left_iff.mp h V' V'_in -- QUOTE.</p> <p>/- TEXT: Let's now turn to the main proof of the extension by continuity theorem.</p> <p>When Lean needs a topology on <code>\u21a5A</code> it will automatically use the induced topology. The only relevant lemma is <code>nhds_induced (\u2191) : \u2200 a : \u21a5A, \ud835\udcdd a = comap (\u2191) (\ud835\udcdd \u2191a)</code> (this is actually a general lemma about induced topologies).</p> <p>The proof outline is:</p> <p>The main assumption and the axiom of choice give a function <code>\u03c6</code> such that <code>\u2200 x, Tendsto f (comap (\u2191) (\ud835\udcdd x)) (\ud835\udcdd (\u03c6 x))</code> (because <code>Y</code> is Hausdorff, <code>\u03c6</code> is entirely determined, but we won't need that until we try to prove that <code>\u03c6</code> indeed extends <code>f</code>).</p> <p>Let's first prove <code>\u03c6</code> is continuous. Fix any <code>x : X</code>. Since <code>Y</code> is regular, it suffices to check that for every closed neighborhood <code>V'</code> of <code>\u03c6 x</code>, <code>\u03c6 \u207b\u00b9' V' \u2208 \ud835\udcdd x</code>. The limit assumption gives (through the auxiliary lemma above) some <code>V \u2208 \ud835\udcdd x</code> such <code>IsOpen V \u2227 (\u2191) \u207b\u00b9' V \u2286 f \u207b\u00b9' V'</code>. Since <code>V \u2208 \ud835\udcdd x</code>, it suffices to prove <code>V \u2286 \u03c6 \u207b\u00b9' V'</code>, ie  <code>\u2200 y \u2208 V, \u03c6 y \u2208 V'</code>. Let's fix <code>y</code> in <code>V</code>. Because <code>V</code> is open, it is a neighborhood of <code>y</code>. In particular <code>(\u2191) \u207b\u00b9' V \u2208 comap (\u2191) (\ud835\udcdd y)</code> and a fortiori <code>f \u207b\u00b9' V' \u2208 comap (\u2191) (\ud835\udcdd y)</code>. In addition <code>comap (\u2191) (\ud835\udcdd y) \u2260 \u22a5</code> because <code>A</code> is dense. Because we know <code>Tendsto f (comap (\u2191) (\ud835\udcdd y)) (\ud835\udcdd (\u03c6 y))</code> this implies <code>\u03c6 y \u2208 closure V'</code> and, since <code>V'</code> is closed, we have proved <code>\u03c6 y \u2208 V'</code>.</p> <p>It remains to prove that <code>\u03c6</code> extends <code>f</code>. This is where the continuity of <code>f</code> enters the discussion, together with the fact that <code>Y</code> is Hausdorff. BOTH: -/ -- QUOTE: example [TopologicalSpace X] [TopologicalSpace Y] [T3Space Y] {A : Set X}     (hA : \u2200 x, x \u2208 closure A) {f : A \u2192 Y} (f_cont : Continuous f)     (hf : \u2200 x : X, \u2203 c : Y, Tendsto f (comap (\u2191) (\ud835\udcdd x)) (\ud835\udcdd c)) :     \u2203 \u03c6 : X \u2192 Y, Continuous \u03c6 \u2227 \u2200 a : A, \u03c6 a = f a := by /- EXAMPLES:   sorry</p>"},{"location":"MIL/C09_Topology/S03_Topological_Spaces/#check-hasbasistendsto_right_iff","title":"check HasBasis.tendsto_right_iff","text":"<p>SOLUTIONS: -/   choose \u03c6 h\u03c6 using hf   use \u03c6   constructor   \u00b7 rw [continuous_iff_continuousAt]     intro x     suffices \u2200 V' \u2208 \ud835\udcdd (\u03c6 x), IsClosed V' \u2192 \u03c6 \u207b\u00b9' V' \u2208 \ud835\udcdd x by       simpa [ContinuousAt, (closed_nhds_basis (\u03c6 x)).tendsto_right_iff]     intro V' V'_in V'_closed     obtain \u27e8V, V_in, V_op, hV\u27e9 : \u2203 V \u2208 \ud835\udcdd x, IsOpen V \u2227 (\u2191) \u207b\u00b9' V \u2286 f \u207b\u00b9' V' := aux (h\u03c6 x) V'_in     suffices : \u2200 y \u2208 V, \u03c6 y \u2208 V'     exact mem_of_superset V_in this     intro y y_in     have hVx : V \u2208 \ud835\udcdd y := V_op.mem_nhds y_in     haveI : (comap ((\u2191) : A \u2192 X) (\ud835\udcdd y)).NeBot := by simpa [mem_closure_iff_comap_neBot] using hA y     apply V'_closed.mem_of_tendsto (h\u03c6 y)     exact mem_of_superset (preimage_mem_comap hVx) hV   \u00b7 intro a     have lim : Tendsto f (\ud835\udcdd a) (\ud835\udcdd (\u03c6 a)) := by simpa [nhds_induced] using h\u03c6 a     exact tendsto_nhds_unique lim f_cont.continuousAt -- QUOTE.</p> <p>/- TEXT: In addition to separation property, the main kind of assumption you can make on a topological space to bring it closer to metric spaces is countability assumption. The main one is first countability asking that every point has a countable neighborhood basis. In particular this ensures that closure of sets can be understood using sequences.</p> <p>BOTH: -/ -- QUOTE: example [TopologicalSpace X] [FirstCountableTopology X]       {s : Set X} {a : X} :     a \u2208 closure s \u2194 \u2203 u : \u2115 \u2192 X, (\u2200 n, u n \u2208 s) \u2227 Tendsto u atTop (\ud835\udcdd a) :=   mem_closure_iff_seq_limit -- QUOTE.</p> <p>/- TEXT: Compactness <sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>Let us now discuss how compactness is defined for topological spaces. As usual there are several ways to think about it and Mathlib goes for the filter version.</p> <p>We first need to define cluster points of filters. Given a filter <code>F</code> on a topological space <code>X</code>, a point <code>x : X</code> is a cluster point of <code>F</code> if <code>F</code>, seen as a generalized set, has non-empty intersection with the generalized set of points that are close to <code>x</code>.</p> <p>Then we can say that a set <code>s</code> is compact if every nonempty generalized set <code>F</code> contained in <code>s</code>, ie such that <code>F \u2264 \ud835\udcdf s</code>, has a cluster point in <code>s</code>.</p> <p>BOTH: -/ -- QUOTE: variable [TopologicalSpace X]</p> <p>example {F : Filter X} {x : X} : ClusterPt x F \u2194 NeBot (\ud835\udcdd x \u2293 F) :=   Iff.rfl</p> <p>example {s : Set X} :     IsCompact s \u2194 \u2200 (F : Filter X) [NeBot F], F \u2264 \ud835\udcdf s \u2192 \u2203 a \u2208 s, ClusterPt a F :=   Iff.rfl -- QUOTE.</p> <p>/- TEXT: For instance if <code>F</code> is <code>map u atTop</code>, the image under <code>u : \u2115 \u2192 X</code> of <code>atTop</code>, the generalized set of very large natural numbers, then the assumption <code>F \u2264 \ud835\udcdf s</code> means that <code>u n</code> belongs to <code>s</code> for <code>n</code> large enough. Saying that <code>x</code> is a cluster point of <code>map u atTop</code> says the image of very large numbers intersects the set of points that are close to <code>x</code>. In case <code>\ud835\udcdd x</code> has a countable basis, we can interpret this as saying that <code>u</code> has a subsequence converging to <code>x</code>, and we get back what compactness looks like in metric spaces. BOTH: -/ -- QUOTE: example [FirstCountableTopology X] {s : Set X} {u : \u2115 \u2192 X} (hs : IsCompact s)     (hu : \u2200 n, u n \u2208 s) : \u2203 a \u2208 s, \u2203 \u03c6 : \u2115 \u2192 \u2115, StrictMono \u03c6 \u2227 Tendsto (u \u2218 \u03c6) atTop (\ud835\udcdd a) :=   hs.tendsto_subseq hu -- QUOTE.</p> <p>/- TEXT: Cluster points behave nicely with continuous functions.</p> <p>BOTH: -/ -- QUOTE: variable [TopologicalSpace Y]</p> <p>example {x : X} {F : Filter X} {G : Filter Y} (H : ClusterPt x F) {f : X \u2192 Y}     (hfx : ContinuousAt f x) (hf : Tendsto f F G) : ClusterPt (f x) G :=   ClusterPt.map H hfx hf -- QUOTE.</p> <p>/- TEXT: As an exercise, we will prove that the image of a compact set under a continuous map is compact. In addition to what we saw already, you should use <code>Filter.push_pull</code> and <code>NeBot.of_map</code>. BOTH: -/ -- QUOTE: -- EXAMPLES: example [TopologicalSpace Y] {f : X \u2192 Y} (hf : Continuous f) {s : Set X} (hs : IsCompact s) :     IsCompact (f '' s) := by   intro F F_ne F_le   have map_eq : map f (\ud835\udcdf s \u2293 comap f F) = \ud835\udcdf (f '' s) \u2293 F := by sorry   have Hne : (\ud835\udcdf s \u2293 comap f F).NeBot := by sorry   have Hle : \ud835\udcdf s \u2293 comap f F \u2264 \ud835\udcdf s := inf_le_left   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example [TopologicalSpace Y] {f : X \u2192 Y} (hf : Continuous f) {s : Set X} (hs : IsCompact s) :     IsCompact (f '' s) := by   intro F F_ne F_le   have map_eq : map f (\ud835\udcdf s \u2293 comap f F) = \ud835\udcdf (f '' s) \u2293 F := by rw [Filter.push_pull, map_principal]   have Hne : (\ud835\udcdf s \u2293 comap f F).NeBot := by     apply NeBot.of_map     rwa [map_eq, inf_of_le_right F_le]   have Hle : \ud835\udcdf s \u2293 comap f F \u2264 \ud835\udcdf s := inf_le_left   rcases hs Hle with \u27e8x, x_in, hx\u27e9   refine' \u27e8f x, mem_image_of_mem f x_in, _\u27e9   apply hx.map hf.continuousAt   rw [Tendsto, map_eq]   exact inf_le_right</p> <p>/- TEXT: One can also express compactness in terms of open covers: <code>s</code> is compact if every family of open sets that cover <code>s</code> has a finite covering sub-family.</p> <p>BOTH: -/ -- QUOTE: example {\u03b9 : Type*} {s : Set X} (hs : IsCompact s) (U : \u03b9 \u2192 Set X) (hUo : \u2200 i, IsOpen (U i))     (hsU : s \u2286 \u22c3 i, U i) : \u2203 t : Finset \u03b9, s \u2286 \u22c3 i \u2208 t, U i :=   hs.elim_finite_subcover U hUo hsU -- QUOTE.</p>"},{"location":"MIL/C10_Differential_Calculus/S01_Elementary_Differential_Calculus/","title":"S01 Elementary Differential Calculus","text":"<p>import MIL.Common import Mathlib.Analysis.SpecialFunctions.Trigonometric.Deriv import Mathlib.Analysis.Calculus.Deriv.Pow import Mathlib.Analysis.Calculus.MeanValue</p> <p>open Set Filter open Topology Filter Classical Real</p> <p>noncomputable section</p> <p>/- TEXT: .. index:: elementary calculus</p> <p>.. _elementary_differential_calculus:</p>"},{"location":"MIL/C10_Differential_Calculus/S01_Elementary_Differential_Calculus/#elementary-differential-calculus","title":"Elementary Differential Calculus","text":"<p>Let <code>f</code> be a function from the reals to the reals. There is a difference between talking about the derivative of <code>f</code> at a single point and talking about the derivative function. In Mathlib, the first notion is represented as follows. EXAMPLES: -/ -- QUOTE: open Real</p> <p>/-- The sin function has derivative 1 at 0. -/ example : HasDerivAt sin 1 0 := by simpa using hasDerivAt_sin 0 -- QUOTE.</p> <p>/- TEXT: We can also express that <code>f</code> is differentiable at a point without specifying its derivative there by writing <code>DifferentiableAt \u211d</code>. We specify <code>\u211d</code> explicitly because in a slightly more general context, when talking about functions from <code>\u2102</code> to <code>\u2102</code>, we want to be able to distinguish between being differentiable in the real sense and being differentiable in the sense of the complex derivative. EXAMPLES: -/ -- QUOTE: example (x : \u211d) : DifferentiableAt \u211d sin x :=   (hasDerivAt_sin x).differentiableAt -- QUOTE.</p> <p>/- TEXT: It would be inconvenient to have to provide a proof of differentiability every time we want to refer to a derivative. So Mathlib provides a function <code>deriv f : \u211d \u2192 \u211d</code> that is defined for any function <code>f : \u211d \u2192 \u211d</code> but is defined to take the value <code>0</code> at any point where <code>f</code> is not differentiable. EXAMPLES: -/ -- QUOTE: example {f : \u211d \u2192 \u211d} {x a : \u211d} (h : HasDerivAt f a x) : deriv f x = a :=   h.deriv</p> <p>example {f : \u211d \u2192 \u211d} {x : \u211d} (h : \u00acDifferentiableAt \u211d f x) : deriv f x = 0 :=   deriv_zero_of_not_differentiableAt h -- QUOTE.</p> <p>/- TEXT: Of course there are many lemmas about <code>deriv</code> that do require differentiability assumptions. For instance, you should think about a counterexample to the next lemma without the differentiability assumptions. EXAMPLES: -/ -- QUOTE: example {f g : \u211d \u2192 \u211d} {x : \u211d} (hf : DifferentiableAt \u211d f x) (hg : DifferentiableAt \u211d g x) :     deriv (f + g) x = deriv f x + deriv g x :=   deriv_add hf hg -- QUOTE.</p> <p>/- TEXT: Interestingly, however, there are statements that can avoid differentiability assumptions by taking advantage of the fact that the value of <code>deriv</code> defaults to zero when the function is not differentiable. So making sense of the following statement requires knowing the precise definition of <code>deriv</code>. EXAMPLES: -/ -- QUOTE: example {f : \u211d \u2192 \u211d} {a : \u211d} (h : IsLocalMin f a) : deriv f a = 0 :=   h.deriv_eq_zero -- QUOTE.</p> <p>/- TEXT: We can even state Rolle's theorem without any differentiability assumptions, which seems even weirder. EXAMPLES: -/ -- QUOTE: open Set</p> <p>example {f : \u211d \u2192 \u211d} {a b : \u211d} (hab : a &lt; b) (hfc : ContinuousOn f (Icc a b)) (hfI : f a = f b) :     \u2203 c \u2208 Ioo a b, deriv f c = 0 :=   exists_deriv_eq_zero hab hfc hfI -- QUOTE.</p> <p>/- TEXT: Of course, this trick does not work for the general mean value theorem. EXAMPLES: -/ -- QUOTE: example (f : \u211d \u2192 \u211d) {a b : \u211d} (hab : a &lt; b) (hf : ContinuousOn f (Icc a b))     (hf' : DifferentiableOn \u211d f (Ioo a b)) : \u2203 c \u2208 Ioo a b, deriv f c = (f b - f a) / (b - a) :=   exists_deriv_eq_slope f hab hf hf' -- QUOTE.</p> <p>/- TEXT: Lean can automatically compute some simple derivatives using the <code>simp</code> tactic. EXAMPLES: -/ -- QUOTE: example : deriv (fun x : \u211d \u21a6 x ^ 5) 6 = 5 * 6 ^ 4 := by simp</p> <p>example : deriv sin \u03c0 = -1 := by simp -- QUOTE.</p>"},{"location":"MIL/C10_Differential_Calculus/S02_Differential_Calculus_in_Normed_Spaces/","title":"S02 Differential Calculus in Normed Spaces","text":"<p>import MIL.Common import Mathlib.Analysis.NormedSpace.BanachSteinhaus import Mathlib.Analysis.NormedSpace.FiniteDimension import Mathlib.Analysis.Calculus.InverseFunctionTheorem.FDeriv import Mathlib.Analysis.Calculus.ContDiff.RCLike import Mathlib.Analysis.Calculus.FDeriv.Prod</p> <p>open Set Filter</p> <p>open Topology Filter</p> <p>noncomputable section</p> <p>/- TEXT: .. index:: normed space</p> <p>.. _normed_spaces:</p>"},{"location":"MIL/C10_Differential_Calculus/S02_Differential_Calculus_in_Normed_Spaces/#differential-calculus-in-normed-spaces","title":"Differential Calculus in Normed Spaces","text":"<p>Normed spaces <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^</p> <p>Differentiation can be generalized beyond <code>\u211d</code> using the notion of a normed vector space, which encapsulates both direction and distance. We start with the notion of a normed group, which as an additive commutative group equipped with a real-valued norm function satisfying the following conditions. EXAMPLES: -/ section</p> <p>-- QUOTE: variable {E : Type*} [NormedAddCommGroup E]</p> <p>example (x : E) : 0 \u2264 \u2016x\u2016 :=   norm_nonneg x</p> <p>example {x : E} : \u2016x\u2016 = 0 \u2194 x = 0 :=   norm_eq_zero</p> <p>example (x y : E) : \u2016x + y\u2016 \u2264 \u2016x\u2016 + \u2016y\u2016 :=   norm_add_le x y -- QUOTE.</p> <p>/- TEXT: Every normed space is a metric space with distance function :math:<code>d(x, y) = \\| x - y \\|</code>, and hence it is also a topological space. Lean and Mathlib know this. EXAMPLES: -/ -- QUOTE: example : MetricSpace E := by infer_instance</p> <p>example {X : Type*} [TopologicalSpace X] {f : X \u2192 E} (hf : Continuous f) :     Continuous fun x \u21a6 \u2016f x\u2016 :=   hf.norm -- QUOTE.</p> <p>/- TEXT: In order to use the notion of a norm with concepts from linear algebra, we add the assumption <code>NormedSpace \u211d E</code> on top of <code>NormedAddGroup E</code>. This stipulates that <code>E</code> is a vector space over <code>\u211d</code> and that scalar multiplication satisfies the following condition. EXAMPLES: -/ -- QUOTE: variable [NormedSpace \u211d E]</p> <p>example (a : \u211d) (x : E) : \u2016a \u2022 x\u2016 = |a| * \u2016x\u2016 :=   norm_smul a x -- QUOTE.</p> <p>/- TEXT: A complete normed space is known as a Banach space. Every finite-dimensional vector space is complete. EXAMPLES: -/ -- QUOTE: example [FiniteDimensional \u211d E] : CompleteSpace E := by infer_instance -- QUOTE.</p> <p>/- TEXT: In all the previous examples, we used the real numbers as the base field. More generally, we can make sense of calculus with a vector space over any nontrivially normed field. These are fields that are equipped with a real-valued norm that is multiplicative and has the property that not every element has norm zero or one (equivalently, there is an element whose norm is bigger than one). EXAMPLES: -/ -- QUOTE: example (\ud835\udd5c : Type*) [NontriviallyNormedField \ud835\udd5c] (x y : \ud835\udd5c) : \u2016x * y\u2016 = \u2016x\u2016 * \u2016y\u2016 :=   norm_mul x y</p> <p>example (\ud835\udd5c : Type*) [NontriviallyNormedField \ud835\udd5c] : \u2203 x : \ud835\udd5c, 1 &lt; \u2016x\u2016 :=   NormedField.exists_one_lt_norm \ud835\udd5c -- QUOTE.</p> <p>/- TEXT: A finite-dimensional vector space over a nontrivially normed field is complete as long as the field itself is complete. EXAMPLES: -/ -- QUOTE: example (\ud835\udd5c : Type*) [NontriviallyNormedField \ud835\udd5c] (E : Type*) [NormedAddCommGroup E]     [NormedSpace \ud835\udd5c E] [CompleteSpace \ud835\udd5c] [FiniteDimensional \ud835\udd5c E] : CompleteSpace E :=   FiniteDimensional.complete \ud835\udd5c E -- QUOTE.</p> <p>end</p> <p>/- TEXT: Continuous linear maps <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^</p> <p>We now turn to the morphisms in the category of normed spaces, namely, continuous linear maps. In Mathlib, the type of <code>\ud835\udd5c</code>-linear continuous maps between normed spaces <code>E</code> and <code>F</code> is written <code>E \u2192L[\ud835\udd5c] F</code>. They are implemented as bundled maps, which means that an element of this type a structure that that includes the function itself and the properties of being linear and continuous. Lean will insert a coercion so that a continuous linear map can be treated as a function. EXAMPLES: -/ section</p> <p>-- QUOTE: variable {\ud835\udd5c : Type*} [NontriviallyNormedField \ud835\udd5c] {E : Type*} [NormedAddCommGroup E]   [NormedSpace \ud835\udd5c E] {F : Type*} [NormedAddCommGroup F] [NormedSpace \ud835\udd5c F]</p> <p>example : E \u2192L[\ud835\udd5c] E :=   ContinuousLinearMap.id \ud835\udd5c E</p> <p>example (f : E \u2192L[\ud835\udd5c] F) : E \u2192 F :=   f</p> <p>example (f : E \u2192L[\ud835\udd5c] F) : Continuous f :=   f.cont</p> <p>example (f : E \u2192L[\ud835\udd5c] F) (x y : E) : f (x + y) = f x + f y :=   f.map_add x y</p> <p>example (f : E \u2192L[\ud835\udd5c] F) (a : \ud835\udd5c) (x : E) : f (a \u2022 x) = a \u2022 f x :=   f.map_smul a x -- QUOTE.</p> <p>/- TEXT: Continuous linear maps have an operator norm that is characterized by the following properties. EXAMPLES: -/ -- QUOTE: variable (f : E \u2192L[\ud835\udd5c] F)</p> <p>example (x : E) : \u2016f x\u2016 \u2264 \u2016f\u2016 * \u2016x\u2016 :=   f.le_op_norm x</p> <p>example {M : \u211d} (hMp : 0 \u2264 M) (hM : \u2200 x, \u2016f x\u2016 \u2264 M * \u2016x\u2016) : \u2016f\u2016 \u2264 M :=   f.op_norm_le_bound hMp hM -- QUOTE.</p> <p>end</p> <p>/- TEXT: There is also a notion of bundled continuous linear isomorphism. Their type of such isomorphisms is <code>E \u2243L[\ud835\udd5c] F</code>.</p> <p>As a challenging exercise, you can prove the Banach-Steinhaus theorem, also known as the Uniform Boundedness Principle. The principle states that a family of continuous linear maps from a Banach space into a normed space is pointwise bounded, then the norms of these linear maps are uniformly bounded. The main ingredient is Baire's theorem <code>nonempty_interior_of_iUnion_of_closed</code>. (You proved a version of this in the topology chapter.) Minor ingredients include <code>continuous_linear_map.opNorm_le_of_shell</code>, <code>interior_subset</code> and <code>interior_iInter_subset</code> and <code>is_closed_le</code>. BOTH: -/ section</p> <p>-- QUOTE: variable {\ud835\udd5c : Type*} [NontriviallyNormedField \ud835\udd5c] {E : Type*} [NormedAddCommGroup E]   [NormedSpace \ud835\udd5c E] {F : Type*} [NormedAddCommGroup F] [NormedSpace \ud835\udd5c F]</p> <p>open Metric</p> <p>-- EXAMPLES: example {\u03b9 : Type*} [CompleteSpace E] {g : \u03b9 \u2192 E \u2192L[\ud835\udd5c] F} (h : \u2200 x, \u2203 C, \u2200 i, \u2016g i x\u2016 \u2264 C) :     \u2203 C', \u2200 i, \u2016g i\u2016 \u2264 C' := by   -- sequence of subsets consisting of those <code>x : E</code> with norms <code>\u2016g i x\u2016</code> bounded by <code>n</code>   let e : \u2115 \u2192 Set E := fun n \u21a6 \u22c2 i : \u03b9, { x : E | \u2016g i x\u2016 \u2264 n }   -- each of these sets is closed   have hc : \u2200 n : \u2115, IsClosed (e n)   sorry   -- the union is the entire space; this is where we use <code>h</code>   have hU : (\u22c3 n : \u2115, e n) = univ   sorry   /- apply the Baire category theorem to conclude that for some <code>m : \u2115</code>,        <code>e m</code> contains some <code>x</code> -/   obtain \u27e8m, x, hx\u27e9 : \u2203 m, \u2203 x, x \u2208 interior (e m) := sorry   obtain \u27e8\u03b5, \u03b5_pos, h\u03b5\u27e9 : \u2203 \u03b5 &gt; 0, ball x \u03b5 \u2286 interior (e m) := sorry   obtain \u27e8k, hk\u27e9 : \u2203 k : \ud835\udd5c, 1 &lt; \u2016k\u2016 := sorry   -- show all elements in the ball have norm bounded by <code>m</code> after applying any <code>g i</code>   have real_norm_le : \u2200 z \u2208 ball x \u03b5, \u2200 (i : \u03b9), \u2016g i z\u2016 \u2264 m   sorry   have \u03b5k_pos : 0 &lt; \u03b5 / \u2016k\u2016 := sorry   refine' \u27e8(m + m : \u2115) / (\u03b5 / \u2016k\u2016), fun i \u21a6 ContinuousLinearMap.opNorm_le_of_shell \u03b5_pos _ hk _\u27e9   sorry   sorry -- QUOTE.</p> <p>-- SOLUTIONS: example {\u03b9 : Type*} [CompleteSpace E] {g : \u03b9 \u2192 E \u2192L[\ud835\udd5c] F} (h : \u2200 x, \u2203 C, \u2200 i, \u2016g i x\u2016 \u2264 C) :     \u2203 C', \u2200 i, \u2016g i\u2016 \u2264 C' := by   -- sequence of subsets consisting of those <code>x : E</code> with norms <code>\u2016g i x\u2016</code> bounded by <code>n</code>   let e : \u2115 \u2192 Set E := fun n \u21a6 \u22c2 i : \u03b9, { x : E | \u2016g i x\u2016 \u2264 n }   -- each of these sets is closed   have hc : \u2200 n : \u2115, IsClosed (e n) := fun i \u21a6     isClosed_iInter fun i \u21a6 isClosed_le (g i).cont.norm continuous_const   -- the union is the entire space; this is where we use <code>h</code>   have hU : (\u22c3 n : \u2115, e n) = univ := by     refine' eq_univ_of_forall fun x \u21a6 _     rcases h x with \u27e8C, hC\u27e9     obtain \u27e8m, hm\u27e9 := exists_nat_ge C     exact \u27e8e m, mem_range_self m, mem_iInter.mpr fun i \u21a6 le_trans (hC i) hm\u27e9   /- apply the Baire category theorem to conclude that for some <code>m : \u2115</code>,        <code>e m</code> contains some <code>x</code> -/   obtain \u27e8m : \u2115, x : E, hx : x \u2208 interior (e m)\u27e9 := nonempty_interior_of_iUnion_of_closed hc hU   obtain \u27e8\u03b5, \u03b5_pos, h\u03b5 : ball x \u03b5 \u2286 interior (e m)\u27e9 := isOpen_iff.mp isOpen_interior x hx   obtain \u27e8k : \ud835\udd5c, hk : 1 &lt; \u2016k\u2016\u27e9 := NormedField.exists_one_lt_norm \ud835\udd5c   -- show all elements in the ball have norm bounded by <code>m</code> after applying any <code>g i</code>   have real_norm_le : \u2200 z \u2208 ball x \u03b5, \u2200 (i : \u03b9), \u2016g i z\u2016 \u2264 m := by     intro z hz i     replace hz := mem_iInter.mp (interior_iInter_subset _ (h\u03b5 hz)) i     apply interior_subset hz   have \u03b5k_pos : 0 &lt; \u03b5 / \u2016k\u2016 := div_pos \u03b5_pos (zero_lt_one.trans hk)   refine' \u27e8(m + m : \u2115) / (\u03b5 / \u2016k\u2016), fun i \u21a6 ContinuousLinearMap.opNorm_le_of_shell \u03b5_pos _ hk _\u27e9   \u00b7 exact div_nonneg (Nat.cast_nonneg _) \u03b5k_pos.le   intro y le_y y_lt   calc     \u2016g i y\u2016 = \u2016g i (y + x) - g i x\u2016 := by rw [(g i).map_add, add_sub_cancel_right]     _ \u2264 \u2016g i (y + x)\u2016 + \u2016g i x\u2016 := (norm_sub_le _ _)     _ \u2264 m + m :=       (add_le_add (real_norm_le (y + x) (by rwa [add_comm, add_mem_ball_iff_norm]) i)         (real_norm_le x (mem_ball_self \u03b5_pos) i))     _ = (m + m : \u2115) := by norm_cast     _ \u2264 (m + m : \u2115) * (\u2016y\u2016 / (\u03b5 / \u2016k\u2016)) :=       (le_mul_of_one_le_right (Nat.cast_nonneg _)         ((one_le_div &lt;| div_pos \u03b5_pos (zero_lt_one.trans hk)).2 le_y))     _ = (m + m : \u2115) / (\u03b5 / \u2016k\u2016) * \u2016y\u2016 := (mul_comm_div _ _ _).symm</p> <p>-- BOTH: end</p> <p>/- TEXT: Asymptotic comparisons <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^</p> <p>Defining differentiability also requires asymptotic comparisons. Mathlib has an extensive library covering the big O and little o relations, whose definitions are shown below. Opening the <code>asymptotics</code> locale allows us to use the corresponding notation. Here we will only use little o to define differentiability. EXAMPLES: -/ -- QUOTE: open Asymptotics</p> <p>example {\u03b1 : Type*} {E : Type*} [NormedGroup E] {F : Type*} [NormedGroup F] (c : \u211d)     (l : Filter \u03b1) (f : \u03b1 \u2192 E) (g : \u03b1 \u2192 F) : IsBigOWith c l f g \u2194 \u2200\u1da0 x in l, \u2016f x\u2016 \u2264 c * \u2016g x\u2016 :=   isBigOWith_iff</p> <p>example {\u03b1 : Type*} {E : Type*} [NormedGroup E] {F : Type*} [NormedGroup F]     (l : Filter \u03b1) (f : \u03b1 \u2192 E) (g : \u03b1 \u2192 F) : f =O[l] g \u2194 \u2203 C, IsBigOWith C l f g :=   isBigO_iff_isBigOWith</p> <p>example {\u03b1 : Type*} {E : Type*} [NormedGroup E] {F : Type*} [NormedGroup F]     (l : Filter \u03b1) (f : \u03b1 \u2192 E) (g : \u03b1 \u2192 F) : f =o[l] g \u2194 \u2200 C &gt; 0, IsBigOWith C l f g :=   isLittleO_iff_forall_isBigOWith</p> <p>example {\u03b1 : Type*} {E : Type*} [NormedAddCommGroup E] (l : Filter \u03b1) (f g : \u03b1 \u2192 E) :     f ~[l] g \u2194 (f - g) =o[l] g :=   Iff.rfl -- QUOTE.</p> <p>/- TEXT: Differentiability <sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup><sup>^</sup>^^</p> <p>We are now ready to discuss differentiable functions between normed spaces. In analogy the elementary one-dimensional, Mathlib defines a predicate <code>HasFDerivAt</code> and a function <code>fderiv</code>. Here the letter \"f\" stands for Fr\u00e9chet. EXAMPLES: -/ section</p> <p>-- QUOTE: open Topology</p> <p>variable {\ud835\udd5c : Type*} [NontriviallyNormedField \ud835\udd5c] {E : Type*} [NormedAddCommGroup E]   [NormedSpace \ud835\udd5c E] {F : Type*} [NormedAddCommGroup F] [NormedSpace \ud835\udd5c F]</p> <p>example (f : E \u2192 F) (f' : E \u2192L[\ud835\udd5c] F) (x\u2080 : E) :     HasFDerivAt f f' x\u2080 \u2194 (fun x \u21a6 f x - f x\u2080 - f' (x - x\u2080)) =o[\ud835\udcdd x\u2080] fun x \u21a6 x - x\u2080 :=   hasFDerivAtFilter_iff_isLittleO ..</p> <p>example (f : E \u2192 F) (f' : E \u2192L[\ud835\udd5c] F) (x\u2080 : E) (hff' : HasFDerivAt f f' x\u2080) : fderiv \ud835\udd5c f x\u2080 = f' :=   hff'.fderiv -- QUOTE.</p> <p>/- TEXT: We also have iterated derivatives that take values in the type of multilinear maps <code>E [\u00d7n]\u2192L[\ud835\udd5c] F</code>, and we have continuously differential functions. The type <code>WithTop \u2115</code> is <code>\u2115</code> with an additional element <code>\u22a4</code> that is bigger than every natural number. So :math:<code>\\mathcal{C}^\\infty</code> functions are functions <code>f</code> that satisfy <code>ContDiff \ud835\udd5c \u22a4 f</code>. EXAMPLES: -/ -- QUOTE: example (n : \u2115) (f : E \u2192 F) : E \u2192 E[\u00d7n]\u2192L[\ud835\udd5c] F :=   iteratedFDeriv \ud835\udd5c n f</p> <p>example (n : WithTop \u2115) {f : E \u2192 F} :     ContDiff \ud835\udd5c n f \u2194       (\u2200 m : \u2115, (m : WithTop \u2115) \u2264 n \u2192 Continuous fun x \u21a6 iteratedFDeriv \ud835\udd5c m f x) \u2227         \u2200 m : \u2115, (m : WithTop \u2115) &lt; n \u2192 Differentiable \ud835\udd5c fun x \u21a6 iteratedFDeriv \ud835\udd5c m f x :=   contDiff_iff_continuous_differentiable -- QUOTE.</p> <p>/- TEXT: There is a stricter notion of differentiability called <code>HasStrictFDerivAt</code>, which is used in the statement of the inverse function theorem and the statement of the implicit function theorem, both of which are in Mathlib. Over <code>\u211d</code> or <code>\u2102</code>, continuously differentiable functions are strictly differentiable. EXAMPLES: -/ -- QUOTE: example {\ud835\udd42 : Type*} [RCLike \ud835\udd42] {E : Type*} [NormedAddCommGroup E] [NormedSpace \ud835\udd42 E] {F : Type*}     [NormedAddCommGroup F] [NormedSpace \ud835\udd42 F] {f : E \u2192 F} {x : E} {n : WithTop \u2115}     (hf : ContDiffAt \ud835\udd42 n f x) (hn : 1 \u2264 n) : HasStrictFDerivAt f (fderiv \ud835\udd42 f x) x :=   hf.hasStrictFDerivAt hn -- QUOTE.</p> <p>/- TEXT: The local inverse theorem is stated using an operation that produces an inverse function from a function and the assumptions that the function is strictly differentiable at a point <code>a</code> and that its derivative is an isomorphism.</p> <p>The first example below gets this local inverse. The next one states that it is indeed a local inverse from the left and from the right, and that it is strictly differentiable. EXAMPLES: -/ -- QUOTE: section LocalInverse variable [CompleteSpace E] {f : E \u2192 F} {f' : E \u2243L[\ud835\udd5c] F} {a : E}</p> <p>example (hf : HasStrictFDerivAt f (f' : E \u2192L[\ud835\udd5c] F) a) : F \u2192 E :=   HasStrictFDerivAt.localInverse f f' a hf</p> <p>example (hf : HasStrictFDerivAt f (f' : E \u2192L[\ud835\udd5c] F) a) :     \u2200\u1da0 x in \ud835\udcdd a, hf.localInverse f f' a (f x) = x :=   hf.eventually_left_inverse</p> <p>example (hf : HasStrictFDerivAt f (f' : E \u2192L[\ud835\udd5c] F) a) :     \u2200\u1da0 x in \ud835\udcdd (f a), f (hf.localInverse f f' a x) = x :=   hf.eventually_right_inverse</p> <p>example {f : E \u2192 F} {f' : E \u2243L[\ud835\udd5c] F} {a : E}   (hf : HasStrictFDerivAt f (f' : E \u2192L[\ud835\udd5c] F) a) :     HasStrictFDerivAt (HasStrictFDerivAt.localInverse f f' a hf) (f'.symm : F \u2192L[\ud835\udd5c] E) (f a) :=   HasStrictFDerivAt.to_localInverse hf</p> <p>end LocalInverse -- QUOTE.</p> <p>/- TEXT: This has been only a quick tour of the differential calculus in Mathlib. The library contains many variations that we have not discussed. For example, you may want to use one-sided derivatives in the one-dimensional setting. The means to do so are found in Mathlib in a more general context; see <code>HasFDerivWithinAt</code> or the even more general <code>HasFDerivAtFilter</code>. EXAMPLES: -/</p>"},{"location":"MIL/C10_Differential_Calculus/S02_Differential_Calculus_in_Normed_Spaces/#check-hasfderivwithinat","title":"check HasFDerivWithinAt","text":""},{"location":"MIL/C10_Differential_Calculus/S02_Differential_Calculus_in_Normed_Spaces/#check-hasfderivatfilter","title":"check HasFDerivAtFilter","text":"<p>end</p>"},{"location":"MIL/C11_Integration_and_Measure_Theory/S01_Elementary_Integration/","title":"S01 Elementary Integration","text":"<p>import MIL.Common import Mathlib.MeasureTheory.Integral.IntervalIntegral import Mathlib.Analysis.SpecialFunctions.Integrals import Mathlib.Analysis.Convolution</p> <p>local macro_rules | <code>($x ^ $y) =&gt;</code>(HPow.hPow $x $y)</p> <p>open Set Filter</p> <p>open Topology Filter</p> <p>noncomputable section</p> <p>/- TEXT: .. index:: integration</p> <p>.. _elementary_integration:</p>"},{"location":"MIL/C11_Integration_and_Measure_Theory/S01_Elementary_Integration/#elementary-integration","title":"Elementary Integration","text":"<p>We first focus on integration of functions on finite intervals in <code>\u211d</code>. We can integrate elementary functions. EXAMPLES: -/ -- QUOTE: open MeasureTheory intervalIntegral</p> <p>open Interval -- this introduces the notation <code>[[a, b]]</code> for the segment from <code>min a b</code> to <code>max a b</code></p> <p>example (a b : \u211d) : (\u222b x in a..b, x) = (b ^ 2 - a ^ 2) / 2 :=   integral_id</p> <p>example {a b : \u211d} (h : (0 : \u211d) \u2209 [[a, b]]) : (\u222b x in a..b, 1 / x) = Real.log (b / a) :=   integral_one_div h -- QUOTE.</p> <p>/- TEXT: The fundamental theorem of calculus relates integration and differentiation. Below we give simplified statements of the two parts of this theorem. The first part says that integration provides an inverse to differentiation and the second one specifies how to compute integrals of derivatives. (These two parts are very closely related, but their optimal versions, which are not shown here, are not equivalent.) EXAMPLES: -/ -- QUOTE: example (f : \u211d \u2192 \u211d) (hf : Continuous f) (a b : \u211d) : deriv (fun u \u21a6 \u222b x : \u211d in a..u, f x) b = f b :=   (integral_hasStrictDerivAt_right (hf.intervalIntegrable _ _) (hf.stronglyMeasurableAtFilter _ _)         hf.continuousAt).hasDerivAt.deriv</p> <p>example {f : \u211d \u2192 \u211d} {a b : \u211d} {f' : \u211d \u2192 \u211d} (h : \u2200 x \u2208 [[a, b]], HasDerivAt f (f' x) x)     (h' : IntervalIntegrable f' volume a b) : (\u222b y in a..b, f' y) = f b - f a :=   integral_eq_sub_of_hasDerivAt h h' -- QUOTE.</p> <p>/- TEXT: Convolution is also defined in Mathlib and its basic properties are proved. EXAMPLES: -/ -- QUOTE: open Convolution</p> <p>example (f : \u211d \u2192 \u211d) (g : \u211d \u2192 \u211d) : f \u22c6 g = fun x \u21a6 \u222b t, f t * g (x - t) :=   rfl -- QUOTE.</p>"},{"location":"MIL/C11_Integration_and_Measure_Theory/S02_Measure_Theory/","title":"S02 Measure Theory","text":"<p>import MIL.Common import Mathlib.Analysis.NormedSpace.FiniteDimension import Mathlib.Analysis.Convolution import Mathlib.MeasureTheory.Function.Jacobian import Mathlib.MeasureTheory.Integral.Bochner import Mathlib.MeasureTheory.Measure.Lebesgue.Basic</p> <p>open Set Filter</p> <p>noncomputable section</p> <p>/- TEXT: .. index:: measure theory</p> <p>.. _measure_theory:</p>"},{"location":"MIL/C11_Integration_and_Measure_Theory/S02_Measure_Theory/#measure-theory","title":"Measure Theory","text":"<p>The general context for integration in Mathlib is measure theory. Even the elementary integrals of the previous section are in fact Bochner integrals. Bochner integration is a generalization of Lebesgue integration where the target space can be any Banach space, not necessarily finite dimensional.</p> <p>The first component in the development of measure theory is the notion of a :math:<code>\\sigma</code>-algebra of sets, which are called the measurable sets. The type class <code>MeasurableSpace</code> serves to equip a type with such a structure. The sets <code>empty</code> and <code>univ</code> are measurable, the complement of a measurable set is measurable, and a countable union or intersection of measurable sets is measurable. Note that these axioms are redundant; if you <code>#print MeasurableSpace</code>, you will see the ones that Mathlib uses. As the examples below show, countability assumptions can be expressed using the <code>Encodable</code> type class. BOTH: -/ -- QUOTE: variable {\u03b1 : Type*} [MeasurableSpace \u03b1]</p> <p>-- EXAMPLES: example : MeasurableSet (\u2205 : Set \u03b1) :=   MeasurableSet.empty</p> <p>example : MeasurableSet (univ : Set \u03b1) :=   MeasurableSet.univ</p> <p>example {s : Set \u03b1} (hs : MeasurableSet s) : MeasurableSet (s\u1d9c) :=   hs.compl</p> <p>example : Encodable \u2115 := by infer_instance</p> <p>example (n : \u2115) : Encodable (Fin n) := by infer_instance</p> <p>-- BOTH: variable {\u03b9 : Type*} [Encodable \u03b9]</p> <p>-- EXAMPLES: example {f : \u03b9 \u2192 Set \u03b1} (h : \u2200 b, MeasurableSet (f b)) : MeasurableSet (\u22c3 b, f b) :=   MeasurableSet.iUnion h</p> <p>example {f : \u03b9 \u2192 Set \u03b1} (h : \u2200 b, MeasurableSet (f b)) : MeasurableSet (\u22c2 b, f b) :=   MeasurableSet.iInter h -- QUOTE.</p> <p>/- TEXT: Once a type is measurable, we can measure it. On paper, a measure on a set (or type) equipped with a :math:<code>\\sigma</code>-algebra is a function from the measurable sets to the extended non-negative reals that is additive on countable disjoint unions. In Mathlib, we don't want to carry around measurability assumptions every time we write an application of the measure to a set. So we extend the measure to any set <code>s</code> as the infimum of measures of measurable sets containing <code>s</code>. Of course, many lemmas still require measurability assumptions, but not all. BOTH: -/ -- QUOTE: open MeasureTheory variable {\u03bc : Measure \u03b1}</p> <p>-- EXAMPLES: example (s : Set \u03b1) : \u03bc s = \u2a05 (t : Set \u03b1) (_ : s \u2286 t) (_ : MeasurableSet t), \u03bc t :=   measure_eq_iInf s</p> <p>example (s : \u03b9 \u2192 Set \u03b1) : \u03bc (\u22c3 i, s i) \u2264 \u2211' i, \u03bc (s i) :=   measure_iUnion_le s</p> <p>example {f : \u2115 \u2192 Set \u03b1} (hmeas : \u2200 i, MeasurableSet (f i)) (hdis : Pairwise (Disjoint on f)) :     \u03bc (\u22c3 i, f i) = \u2211' i, \u03bc (f i) :=   \u03bc.m_iUnion hmeas hdis -- QUOTE.</p> <p>/- TEXT: Once a type has a measure associated with it, we say that a property <code>P</code> holds almost everywhere if the set of elements where the property fails has measure 0. The collection of properties that hold almost everywhere form a filter, but Mathlib introduces special notation for saying that a property holds almost everywhere. EXAMPLES: -/ -- QUOTE: example {P : \u03b1 \u2192 Prop} : (\u2200\u1d50 x \u2202\u03bc, P x) \u2194 \u2200\u1da0 x in ae \u03bc, P x :=   Iff.rfl -- QUOTE.</p>"},{"location":"MIL/C11_Integration_and_Measure_Theory/S03_Integration/","title":"S03 Integration","text":"<p>import MIL.Common import Mathlib.Analysis.NormedSpace.FiniteDimension import Mathlib.Analysis.Convolution import Mathlib.MeasureTheory.Function.Jacobian import Mathlib.MeasureTheory.Integral.Bochner import Mathlib.MeasureTheory.Measure.Lebesgue.Basic</p> <p>open Set Filter</p> <p>open Topology Filter ENNReal</p> <p>open MeasureTheory</p> <p>noncomputable section variable {\u03b1 : Type*} [MeasurableSpace \u03b1] variable {\u03bc : Measure \u03b1}</p> <p>/- TEXT: .. _integration:</p>"},{"location":"MIL/C11_Integration_and_Measure_Theory/S03_Integration/#integration","title":"Integration","text":"<p>Now that we have measurable spaces and measures we can consider integrals. As explained above, Mathlib uses a very general notion of integration that allows any Banach space as the target. As usual, we don't want our notation to carry around assumptions, so we define integration in such a way that an integral is equal to zero if the function in question is not integrable. Most lemmas having to do with integrals have integrability assumptions. EXAMPLES: -/ -- QUOTE: section variable {E : Type*} [NormedAddCommGroup E] [NormedSpace \u211d E] [CompleteSpace E] {f : \u03b1 \u2192 E}</p> <p>example {f g : \u03b1 \u2192 E} (hf : Integrable f \u03bc) (hg : Integrable g \u03bc) :     \u222b a, f a + g a \u2202\u03bc = \u222b a, f a \u2202\u03bc + \u222b a, g a \u2202\u03bc :=   integral_add hf hg -- QUOTE.</p> <p>/- TEXT: As an example of the complex interactions between our various conventions, let us see how to integrate constant functions. Recall that a measure <code>\u03bc</code> takes values in <code>\u211d\u22650\u221e</code>, the type of extended non-negative reals. There is a function <code>ENNReal.toReal : \u211d\u22650\u221e \u2192 \u211d</code> which sends <code>\u22a4</code>, the point at infinity, to zero. For any <code>s : Set \u03b1</code>, if <code>\u03bc s = \u22a4</code>, then nonzero constant functions are not integrable on <code>s</code>. In that case, their integrals are equal to zero by definition, as is <code>(\u03bc s).toReal</code>. So in all cases we have the following lemma. EXAMPLES: -/ -- QUOTE: example {s : Set \u03b1} (c : E) : \u222b x in s, c \u2202\u03bc = (\u03bc s).toReal \u2022 c :=   setIntegral_const c -- QUOTE.</p> <p>/- TEXT: We now quickly explain how to access the most important theorems in integration theory, starting with the dominated convergence theorem. There are several versions in Mathlib, and here we only show the most basic one. EXAMPLES: -/ -- QUOTE: open Filter</p> <p>example {F : \u2115 \u2192 \u03b1 \u2192 E} {f : \u03b1 \u2192 E} (bound : \u03b1 \u2192 \u211d) (hmeas : \u2200 n, AEStronglyMeasurable (F n) \u03bc)     (hint : Integrable bound \u03bc) (hbound : \u2200 n, \u2200\u1d50 a \u2202\u03bc, \u2016F n a\u2016 \u2264 bound a)     (hlim : \u2200\u1d50 a \u2202\u03bc, Tendsto (fun n : \u2115 \u21a6 F n a) atTop (\ud835\udcdd (f a))) :     Tendsto (fun n \u21a6 \u222b a, F n a \u2202\u03bc) atTop (\ud835\udcdd (\u222b a, f a \u2202\u03bc)) :=   tendsto_integral_of_dominated_convergence bound hmeas hint hbound hlim -- QUOTE.</p> <p>/- TEXT: Then we have Fubini's theorem for integrals on product type. EXAMPLES: -/ -- QUOTE: example {\u03b1 : Type*} [MeasurableSpace \u03b1] {\u03bc : Measure \u03b1} [SigmaFinite \u03bc] {\u03b2 : Type*}     [MeasurableSpace \u03b2] {\u03bd : Measure \u03b2} [SigmaFinite \u03bd] (f : \u03b1 \u00d7 \u03b2 \u2192 E)     (hf : Integrable f (\u03bc.prod \u03bd)) : \u222b z, f z \u2202 \u03bc.prod \u03bd = \u222b x, \u222b y, f (x, y) \u2202\u03bd \u2202\u03bc :=   integral_prod f hf -- QUOTE.</p> <p>end</p> <p>/- TEXT: There is a very general version of convolution that applies to any continuous bilinear form. EXAMPLES: -/ section</p> <p>-- QUOTE: open Convolution</p> <p>-- EXAMPLES: variable {\ud835\udd5c : Type*} {G : Type*} {E : Type*} {E' : Type*} {F : Type*} [NormedAddCommGroup E]   [NormedAddCommGroup E'] [NormedAddCommGroup F] [NontriviallyNormedField \ud835\udd5c] [NormedSpace \ud835\udd5c E]   [NormedSpace \ud835\udd5c E'] [NormedSpace \ud835\udd5c F] [MeasurableSpace G] [NormedSpace \u211d F] [CompleteSpace F]   [Sub G]</p> <p>example (f : G \u2192 E) (g : G \u2192 E') (L : E \u2192L[\ud835\udd5c] E' \u2192L[\ud835\udd5c] F) (\u03bc : Measure G) :     f \u22c6[L, \u03bc] g = fun x \u21a6 \u222b t, L (f t) (g (x - t)) \u2202\u03bc :=   rfl -- QUOTE.</p> <p>end</p> <p>/- TEXT: Finally, Mathlib has a very general version of the change-of-variables formula. In the statement below, <code>BorelSpace E</code> means the :math:<code>\\sigma</code>-algebra on <code>E</code> is generated by the open sets of <code>E</code>, and <code>IsAddHaarMeasure \u03bc</code> means that the measure <code>\u03bc</code> is left-invariant, gives finite mass to compact sets, and give positive mass to open sets. EXAMPLES: -/ -- QUOTE: example {E : Type*} [NormedAddCommGroup E] [NormedSpace \u211d E] [FiniteDimensional \u211d E]     [MeasurableSpace E] [BorelSpace E] (\u03bc : Measure E) [\u03bc.IsAddHaarMeasure] {F : Type*}     [NormedAddCommGroup F] [NormedSpace \u211d F] [CompleteSpace F] {s : Set E} {f : E \u2192 E}     {f' : E \u2192 E \u2192L[\u211d] E} (hs : MeasurableSet s)     (hf : \u2200 x : E, x \u2208 s \u2192 HasFDerivWithinAt f (f' x) s x) (h_inj : InjOn f s) (g : E \u2192 F) :     \u222b x in f '' s, g x \u2202\u03bc = \u222b x in s, |(f' x).det| \u2022 g (f x) \u2202\u03bc :=   integral_image_eq_integral_abs_det_fderiv_smul \u03bc hs hf h_inj g -- QUOTE.</p>"},{"location":"en/","title":"\u76ee\u5f55","text":"<p>this is a demo</p>"},{"location":"en/C01_Introduction/","title":"Introduction","text":""},{"location":"en/C01_Introduction/#getting-started","title":"Getting Started","text":"<p>The goal of this book is to teach you to formalize mathematics using the Lean 4 interactive proof assistant. It assumes that you know some mathematics, but it does not require much. Although we will cover examples ranging from number theory to measure theory and analysis, we will focus on elementary aspects of those fields, in the hopes that if they are not familiar to you, you can pick them up as you go. We also don't presuppose any background with formal methods. Formalization can be seen as a kind of computer programming: we will write mathematical definitions, theorems, and proofs in a regimented language, like a programming language, that Lean can understand. In return, Lean provides feedback and information, interprets expressions and guarantees that they are well-formed, and ultimately certifies the correctness of our proofs.</p> <p>You can learn more about Lean from the Lean project page and the Lean community web pages. This tutorial is based on Lean's large and ever-growing library, Mathlib. We also strongly recommend joining the Lean Zulip online chat group if you haven't already. You'll find a lively and welcoming community of Lean enthusiasts there, happy to answer questions and offer moral support.</p> <p>Although you can read a pdf or html version of this book online, it designed to be read interactively, running Lean from inside the VS Code editor. To get started:</p> <ol> <li> <p>Install Lean 4 and VS Code following    these installation instructions.</p> </li> <li> <p>Make sure you have git installed.</p> </li> <li> <p>Follow these instructions    to fetch the <code>mathematics*in*lean</code> repository and open it up in VS Code.</p> </li> <li> <p>Each section in this book has an associated Lean file with examples and exercises.    You can find them in the folder <code>MIL</code>, organized by chapter.    We strongly recommend making a copy of that folder and experimenting and doing the    exercises in that copy.    This leaves the originals intact, and it also makes it easier to update the repository as it changes (see below).    You can call the copy <code>my*files</code> or whatever you want and use it to create    your own Lean files as well.</p> </li> </ol> <p>At that point, you can open the textbook in a side panel in VS Code as follows:</p> <ol> <li> <p>Type <code>ctrl-shift-P</code> (<code>command-shift-P</code> in macOS).</p> </li> <li> <p>Type <code>Lean 4: Open Documentation View</code> in the bar that appears, and then    press return. (You can press return to select it as soon as it is highlighted    in the menu.)</p> </li> <li> <p>In the window that opens, click on <code>Open documentation of current project</code>.</p> </li> </ol> <p>Alternatively, you can run Lean and VS Code in the cloud, using Gitpod. You can find instructions as to how to do that on the Mathematics in Lean project page on Github. We still recommend working in a copy of the <code>MIL</code> folder, as described above.</p> <p>This textbook and the associated repository are still a work in progress. You can update the repository by typing <code>git pull</code> followed by <code>lake exe cache get</code> inside the <code>mathematics*in*lean</code> folder. (This assumes that you have not changed the contents of the <code>MIL</code> folder, which is why we suggested making a copy.)</p> <p>We intend for you to work on the exercises in the <code>MIL</code> folder while reading the textbook, which contains explanations, instructions, and hints. The text will often include examples, like this one:</p> <pre><code>#eval \"Hello, World!\"\n</code></pre> <p>You should be able to find the corresponding example in the associated Lean file. If you click on the line, VS Code will show you Lean's feedback in the <code>Lean Goal</code> window, and if you hover your cursor over the <code>#eval</code> command VS Code will show you Lean's response to this command in a pop-up window. You are encouraged to edit the file and try examples of your own.</p> <p>This book moreover provides lots of challenging exercises for you to try. Don't rush past these! Lean is about doing mathematics interactively, not just reading about it. Working through the exercises is central to the experience. You don't have to do all of them; when you feel comfortable that you have mastered the relevant skills, feel free to move on. You can always compare your solutions to the ones in the <code>solutions</code> folder associated with each section.</p>"},{"location":"en/C01_Introduction/#overview","title":"Overview","text":"<p>Put simply, Lean is a tool for building complex expressions in a formal language known as dependent type theory.</p> <p>.. index:: check, commands ; check</p> <p>Every expression has a type, and you can use the <code>#check</code> command to print it. Some expressions have types like <code>\u2115</code> or <code>\u2115 \u2192 \u2115</code>. These are mathematical objects.</p> <pre><code>#check 2 + 2\n\ndef f (x : \u2115) :=\n  x + 3\n\n#check f\n</code></pre> <p>Some expressions have type <code>Prop</code>. These are mathematical statements.</p> <pre><code>#check 2 + 2 = 4\n\ndef FermatLastTheorem :=\n  \u2200 x y z n : \u2115, n &gt; 2 \u2227 x * y * z \u2260 0 \u2192 x ^ n + y ^ n \u2260 z ^ n\n\n#check FermatLastTheorem\n</code></pre> <p>Some expressions have a type, <code>P</code>, where <code>P</code> itself has type <code>Prop</code>. Such an expression is a proof of the proposition <code>P</code>.</p> <pre><code>theorem easy : 2 + 2 = 4 :=\n  rfl\n\n#check easy\n\ntheorem hard : FermatLastTheorem :=\n  sorry\n\n#check hard\n</code></pre> <p>If you manage to construct an expression of type <code>FermatLastTheorem</code> and Lean accepts it as a term of that type, you have done something very impressive. (Using <code>sorry</code> is cheating, and Lean knows it.) So now you know the game. All that is left to learn are the rules.</p> <p>This book is complementary to a companion tutorial, Theorem Proving in Lean, which provides a more thorough introduction to the underlying logical framework and core syntax of Lean. Theorem Proving in Lean is for people who prefer to read a user manual cover to cover before using a new dishwasher. If you are the kind of person who prefers to hit the start button and figure out how to activate the potscrubber feature later, it makes more sense to start here and refer back to Theorem Proving in Lean as necessary.</p> <p>Another thing that distinguishes Mathematics in Lean from Theorem Proving in Lean is that here we place a much greater emphasis on the use of tactics. Given that we are trying to build complex expressions, Lean offers two ways of going about it: we can write down the expressions themselves (that is, suitable text descriptions thereof), or we can provide Lean with instructions as to how to construct them. For example, the following expression represents a proof of the fact that if <code>n</code> is even then so is <code>m * n</code>:</p> <pre><code>example : \u2200 m n : Nat, Even n \u2192 Even (m * n) := fun m n \u27e8k, (hk : n = k + k)\u27e9 \u21a6\n  have hmn : m * n = m * k + m * k := by rw [hk, mul*add]\n  show \u2203 l, m \\* n = l + l from \u27e8\\*, hmn\u27e9\n</code></pre> <p>The proof term can be compressed to a single line:</p> <pre><code>example : \u2200 m n : Nat, Even n \u2192 Even (m * n) :=\nfun m n \u27e8k, hk\u27e9 \u21a6 \u27e8m * k, by rw [hk, mul*add]\u27e9\n</code></pre> <p>The following is, instead, a tactic-style proof of the same theorem, where lines starting with <code>--</code> are comments, hence ignored by Lean:</p> <pre><code>example : \u2200 m n : Nat, Even n \u2192 Even (m * n) := by\n  -- Say m and n are natural numbers, and assume n=2*k.\n  rintro m n \u27e8k, hk\u27e9\n  -- We need to prove m*n is twice a natural number. Let's show it's twice m*k.\n  use m \\* k\n  -- Substitute for n,\n  rw [hk]\n  -- and now it's obvious.\n  ring\n</code></pre> <p>As you enter each line of such a proof in VS Code, Lean displays the proof state in a separate window, telling you what facts you have already established and what tasks remain to prove your theorem. You can replay the proof by stepping through the lines, since Lean will continue to show you the state of the proof at the point where the cursor is. In this example, you will then see that the first line of the proof introduces <code>m</code> and <code>n</code> (we could have renamed them at that point, if we wanted to), and also decomposes the hypothesis <code>Even n</code> to a <code>k</code> and the assumption that <code>n = 2 * k</code>. The second line, <code>use m * k</code>, declares that we are going to show that <code>m * n</code> is even by showing <code>m * n = 2 * (m * k)</code>. The next line uses the <code>rewrite</code> tactic to replace <code>n</code> by <code>2 * k</code> in the goal, and the <code>ring</code> tactic solves the resulting goal <code>m * (2 * k) = 2 * (m * k)</code>.</p> <p>The ability to build a proof in small steps with incremental feedback is extremely powerful. For that reason, tactic proofs are often easier and quicker to write than proof terms. There isn't a sharp distinction between the two: tactic proofs can be inserted in proof terms, as we did with the phrase <code>by rw [hk, mul*add]</code> in the example above. We will also see that, conversely, it is often useful to insert a short proof term in the middle of a tactic proof. That said, in this book, our emphasis will be on the use of tactics.</p> <p>In our example, the tactic proof can also be reduced to a one-liner:</p> <pre><code>example : \u2200 m n : Nat, Even n \u2192 Even (m * n) := by\n  rintro m n \u27e8k, hk\u27e9; use m * k; rw [hk]; ring\n</code></pre> <p>Here we have used tactics to carry out small proof steps. But they can also provide substantial automation, and justify longer calculations and bigger inferential steps. For example, we can invoke Lean's simplifier with specific rules for simplifying statements about parity to prove our theorem automatically.</p> <pre><code>example : \u2200 m n : Nat, Even n \u2192 Even (m * n) := by\n  intros; simp [*, parity*simps]\n</code></pre> <p>Another big difference between the two introductions is that Theorem Proving in Lean depends only on core Lean and its built-in tactics, whereas Mathematics in Lean is built on top of Lean's powerful and ever-growing library, Mathlib. As a result, we can show you how to use some of the mathematical objects and theorems in the library, and some of the very useful tactics. This book is not meant to be used as an complete overview of the library; the community web pages contain extensive documentation. Rather, our goal is to introduce you to the style of thinking that underlies that formalization, and point out basic entry points so that you are comfortable browsing the library and finding things on your own.</p> <p>Interactive theorem proving can be frustrating, and the learning curve is steep. But the Lean community is very welcoming to newcomers, and people are available on the Lean Zulip chat group round the clock to answer questions. We hope to see you there, and have no doubt that soon enough you, too, will be able to answer such questions and contribute to the development of Mathlib.</p> <p>So here is your mission, should you choose to accept it: dive in, try the exercises, come to Zulip with questions, and have fun. But be forewarned: interactive theorem proving will challenge you to think about mathematics and mathematical reasoning in fundamentally new ways. Your life may never be the same.</p> <p>Acknowledgments. We are grateful to Gabriel Ebner for setting up the infrastructure for running this tutorial in VS Code, and to Scott Morrison and Mario Carneiro for help porting it from Lean 4. We are also grateful for help and corrections from Takeshi Abe, Julian Berman, Alex Best, Bulwi Cha, Bryan Gin-ge Chen, Steven Clontz, Mauricio Collaris, Johan Commelin, Mark Czubin, Alexandru Duca, Denis Gorbachev, Winston de Greef, Mathieu Guay-Paquet, Julian K\u00fclshammer, Martin C. Martin, Giovanni Mascellani, Isaiah Mindich, Hunter Monroe, Pietro Monticone, Oliver Nash, Bartosz Piotrowski, Nicolas Rolland, Keith Rush, Guilherme Silva, Pedro S\u00e1nchez Terraf, Floris van Doorn, and Eric Wieser. Our work has been partially supported by the Hoskinson Center for Formal Mathematics.</p>"},{"location":"en/C02_Basics/","title":"Basics","text":"<p>This chapter is designed to introduce you to the nuts and bolts of mathematical reasoning in Lean: calculating, applying lemmas and theorems, and reasoning about generic structures.</p>"},{"location":"en/C02_Basics/#calculating","title":"Calculating","text":"<p>We generally learn to carry out mathematical calculations without thinking of them as proofs. But when we justify each step in a calculation, as Lean requires us to do, the net result is a proof that the left-hand side of the calculation is equal to the right-hand side.</p> <p>In Lean, stating a theorem is tantamount to stating a goal, namely, the goal of proving the theorem. Lean provides the rewriting tactic <code>rw</code>, to replace the left-hand side of an identity by the right-hand side in the goal. If <code>a</code>, <code>b</code>, and <code>c</code> are real numbers, <code>mul_assoc a b c</code> is the identity <code>a * b * c = a * (b * c)</code> and <code>mul_comm a b</code> is the identity <code>a * b = b * a</code>. Lean provides automation that generally eliminates the need to refer the facts like these explicitly, but they are useful for the purposes of illustration. In Lean, multiplication associates to the left, so the left-hand side of <code>mul_assoc</code> could also be written <code>(a * b) * c</code>. However, it is generally good style to be mindful of Lean's notational conventions and leave out parentheses when Lean does as well.</p> <p>Let's try out <code>rw</code>.</p> <pre><code>example (a b c : \u211d) : a * b * c = b * (a * c) := by\n  rw [mul_comm a b]\n  rw [mul_assoc b a c]\n</code></pre> <p>The <code>import</code> lines at the beginning of the associated examples file import the theory of the real numbers from Mathlib, as well as useful automation. For the sake of brevity, we generally suppress information like this in the textbook.</p> <p>You are welcome to make changes to see what happens. You can type the <code>\u211d</code> character as <code>\\R</code> or <code>\\real</code> in VS Code. The symbol doesn't appear until you hit space or the tab key. If you hover over a symbol when reading a Lean file, VS Code will show you the syntax that can be used to enter it. If you are curious to see all available abbreviations, you can hit Ctrl-Shift-P and then type abbreviations to get access to the <code>Lean 4: Show all abbreviations</code> command. If your keyboard does not have an easily accessible backslash, you can change the leading character by changing the <code>lean4.input.leader</code> setting.</p> <p>When a cursor is in the middle of a tactic proof, Lean reports on the current proof state in the Lean Infoview window. As you move your cursor past each step of the proof, you can see the state change. A typical proof state in Lean might look as follows:</p> <pre><code>1 goal\nx y : \u2115,\nh\u2081 : Prime x,\nh\u2082 : \u00acEven x,\nh\u2083 : y &gt; x\n\u22a2 y \u2265 4\n</code></pre> <p>The lines before the one* that begins with <code>\u22a2</code> denote the _context*: they are the objects and assumptions currently at play. In this example, these include two objects, <code>x</code> and <code>y</code>, each a natural number. They also include three assumptions, labelled <code>h\u2081</code>, <code>h\u2082</code>, and <code>h\u2083</code>. In Lean, everything in a context is labelled with an identifier. You can type these subscripted labels as <code>h\\1</code>, <code>h\\2</code>, and <code>h\\3</code>, but any legal identifiers would do: you can use <code>h1</code>, <code>h2</code>, <code>h3</code> instead, or <code>foo</code>, <code>bar</code>, and <code>baz</code>. The last line represents the goal, that is, the fact to be proved. Sometimes people use target for the fact to be proved, and goal for the combination of the context and the target. In practice, the intended meaning is usually clear.</p> <p>Try proving these identities, in each case replacing <code>sorry</code> by a tactic proof. With the <code>rw</code> tactic, you can use a left arrow (<code>\\l</code>) to reverse an identity. For example, <code>rw [\u2190 mul_assoc a b c]</code> replaces <code>a * (b * c)</code> by <code>a * b * c</code> in the current goal. Note that the left-pointing arrow refers to going from right to left in the identity provided by <code>mul_assoc</code>, it has nothing to do with the left or right side of the goal.</p> <pre><code>example (a b c : \u211d) : c * b * a = b * (a * c) := by\n  sorry\n\nexample (a b c : \u211d) : a * (b * c) = b * (a * c) := by\n  sorry\n</code></pre> <p>You can also use identities like <code>mul_assoc</code> and <code>mul_comm</code> without arguments. In this case, the rewrite tactic tries to match the left-hand side with an expression in the goal, using the first pattern it finds.</p> <pre><code>example (a b c : \u211d) : a * b * c = b * c * a := by\n  rw [mul_assoc]\n  rw [mul_comm]\n</code></pre> <p>You can also provide partial information. For example, <code>mul_comm a</code> matches any pattern of the form <code>a * ?</code> and rewrites it to <code>? * a</code>. Try doing the first of these examples without providing any arguments at all, and the second with only one argument.</p> <pre><code>example (a b c : \u211d) : a * (b * c) = b * (c * a) := by\n  sorry\n\nexample (a b c : \u211d) : a * (b * c) = b * (a * c) := by\n  sorry\n</code></pre> <p>You can also use <code>rw</code> with facts from the local context.</p> <pre><code>example (a b c d e f : \u211d) (h : a * b = c * d) (h' : e = f) : a * (b * e) = c * (d * f) := by\n  rw [h']\n  rw [\u2190 mul_assoc]\n  rw [h]\n  rw [mul_assoc]\n</code></pre> <p>Try these, using the theorem <code>sub_self</code> for the second one:</p> <pre><code>example (a b c d e f : \u211d) (h : b * c = e * f) : a * b * c * d = a * e * f * d := by\n  sorry\n\nexample (a b c d : \u211d) (hyp : c = b * a - d) (hyp' : d = a * b) : c = 0 := by\n  sorry\n</code></pre> <p>Multiple rewrite commands can be carried out with a single command, by listing the relevant identities separated by commas inside the square brackets.</p> <pre><code>example (a b c d e f : \u211d) (h : a * b = c * d) (h' : e = f) : a * (b * e) = c * (d * f) := by\n  rw [h', \u2190 mul_assoc, h, mul_assoc]\n</code></pre> <p>You still see the incremental progress by placing the cursor after a comma in any list of rewrites.</p> <p>Another trick is that we can declare variables once and for all outside an example or theorem. Lean then includes them automatically.</p> <pre><code>variable (a b c d e f : \u211d)\n\nexample (h : a * b = c * d) (h' : e = f) : a * (b * e) = c * (d * f) := by\n  rw [h', \u2190 mul_assoc, h, mul_assoc]\n</code></pre> <p>Inspection of the tactic state at the beginning of the above proof reveals that Lean indeed included all variables. We can delimit the scope of the declaration by putting it in a <code>section ... end</code> block. Finally, recall from the introduction that Lean provides us with a command to determine the type of an expression:</p> <pre><code>section\nvariable (a b c : \u211d)\n\n#check a\n#check a + b\n#check (a : \u211d)\n#check mul_comm a b\n#check (mul_comm a b : a * b = b * a)\n#check mul_assoc c a b\n#check mul_comm a\n#check mul_comm\n\nend\n</code></pre> <p>The <code>#check</code> command works for both objects and facts. In response to the command <code>#check a</code>, Lean reports that <code>a</code> has type <code>\u211d</code>. In response to the command <code>#check mul_comm a b</code>, Lean reports that <code>mul_comm a b</code> is a proof of the fact <code>a * b = b * a</code>. The command <code>#check (a : \u211d)</code> states our expectation that the type of <code>a</code> is <code>\u211d</code>, and Lean will raise an error if that is not the case. We will explain the output of the last three <code>#check</code> commands later, but in the meanwhile, you can take a look at them, and experiment with some <code>#check</code> commands of your own.</p> <p>Let's try some more examples. The theorem <code>two_mul a</code> says that <code>2 * a = a + a</code>. The theorems <code>add_mul</code> and <code>mul_add</code> express the distributivity of multiplication over addition, and the theorem <code>add_assoc</code> expresses the associativity of addition. Use the <code>#check</code> command to see the precise statements.</p> <pre><code>example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b := by\n  rw [mul_add, add_mul, add_mul]\n  rw [\u2190 add_assoc, add_assoc (a * a)]\n  rw [mul_comm b a, \u2190 two_mul]\n</code></pre> <p>Whereas it is possible to figure out what it going on in this proof by stepping through it in the editor, it is hard to read on its own. Lean provides a more structured way of writing proofs like this using the <code>calc</code> keyword.</p> <pre><code>example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b :=\n  calc\n    (a + b) * (a + b) = a * a + b * a + (a * b + b * b) := by\n      rw [mul_add, add_mul, add_mul]\n    * = a * a + (b * a + a * b) + b * b := by\n      rw [\u2190 add_assoc, add_assoc (a * a)]\n    * = a * a + 2 * (a * b) + b * b := by\n      rw [mul_comm b a, \u2190 two_mul]\n</code></pre> <p>Notice that the proof does not begin with <code>by</code>: an expression that begins with <code>calc</code> is a proof term. A <code>calc</code> expression can also be used inside a tactic proof, but Lean interprets it as the instruction to use the resulting proof term to solve the goal. The <code>calc</code> syntax is finicky: the underscores and justification have to be in the format indicated above. Lean uses indentation to determine things like where a block of tactics or a <code>calc</code> block begins and ends; try changing the indentation in the proof above to see what happens.</p> <p>one way to write a <code>calc</code> proof is to outline it first using the <code>sorry</code> tactic for justification, make sure Lean accepts the expression modulo these, and then justify the individual steps using tactics.</p> <pre><code>example : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b :=\n  calc\n    (a + b) * (a + b) = a * a + b * a + (a * b + b * b) := by\n      sorry\n    * = a * a + (b * a + a * b) + b * b := by\n      sorry\n    * = a * a + 2 * (a * b) + b * b := by\n      sorry\n</code></pre> <p>Try proving the following identity using both a pure <code>rw</code> proof and a more structured <code>calc</code> proof:</p> <pre><code>example : (a + b) * (c + d) = a * c + a * d + b * c + b * d := by\n  sorry\n</code></pre> <p>The following exercise is a little more challenging. You can use the theorems listed underneath.</p> <pre><code>example (a b : \u211d) : (a + b) * (a - b) = a ^ 2 - b ^ 2 := by\n  sorry\n\n#check pow_two a\n#check mul_sub a b c\n#check add_mul a b c\n#check add_sub a b c\n#check sub_sub a b c\n#check add_zero a\n</code></pre> <p>We can also perform rewriting in an assumption in the context. For example, <code>rw [mul_comm a b] at hyp</code> replaces <code>a * b</code> by <code>b * a</code> in the assumption <code>hyp</code>.</p> <pre><code>example (a b c d : \u211d) (hyp : c = d * a + b) (hyp' : b = a * d) : c = 2 * a * d := by\n  rw [hyp'] at hyp\n  rw [mul_comm d a] at hyp\n  rw [\u2190 two_mul (a * d)] at hyp\n  rw [\u2190 mul_assoc 2 a d] at hyp\n  exact hyp\n</code></pre> <p>In the last step, the <code>exact</code> tactic can use <code>hyp</code> to solve the goal because at that point <code>hyp</code> matches the goal exactly.</p> <p>We close this section by noting that Mathlib provides a useful bit of automation with a <code>ring</code> tactic, which is designed to prove identities in any commutative ring as long as they follow purely from the ring axioms, without using any local assumption. ring</p> <pre><code>example : c * b * a = b * (a * c) := by\n  ring\nexample : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b := by\n  ring\n\nexample : (a + b) * (a - b) = a ^ 2 - b ^ 2 := by\n  ring\n\nexample (hyp : c = d * a + b) (hyp' : b = a * d) : c = 2 * a * d := by\n  rw [hyp, hyp']\n  ring\n</code></pre> <p>The <code>ring</code> tactic is imported indirectly when we import <code>Mathlib.Data.Real.Basic</code>, but we will see in the next section that it can be used for calculations on structures other than the real numbers. It can be imported explicitly with the command <code>import Mathlib.Tactic</code>. We will see there are similar tactics for other common kind of algebraic structures.</p> <p>There is a variation of <code>rw</code> called <code>nth_rw</code> that allows you to replace only particular instances of an expression in the goal. Possible matches are enumerated starting with 1, so in the following example, <code>nth_rw 2 [h]</code> replaces the second occurrence of <code>a + b</code> with <code>c</code>.</p> <pre><code>example (a b c : \u2115) (h : a + b = c) : (a + b) * (a + b) = a * c + b * c := by\n  nth_rw 2 [h]\n  rw [add_mul]\n</code></pre>"},{"location":"en/C02_Basics/#proving-identities-in-algebraic-structures","title":"Proving Identities in Algebraic Structures","text":"<p>Mathematically, a ring consists of a collection of objects, \\(R\\), operations \\(+\\) \\(\\times\\), and constants \\(0\\) and \\(1\\), and an operation \\(x \\to -x\\) such that:</p> <ul> <li> <p>\\(R\\) with \\(+\\) is an abelian group, with \\(0\\)   as the additive identity and negation as inverse.</p> </li> <li> <p>Multiplication is associative with identity \\(1\\),   and multiplication distributes over addition.</p> </li> </ul> <p>In Lean, the collection of objects is represented as a type, <code>R</code>. The ring axioms are as follows:</p> <pre><code>variable (R : Type *) [Ring R]\n\n#check (add_assoc : \u2200 a b c : R, a + b + c = a + (b + c))\n#check (add_comm : \u2200 a b : R, a + b = b + a)\n#check (zero_add : \u2200 a : R, 0 + a = a)\n#check (add_left_neg : \u2200 a : R, -a + a = 0)\n#check (mul_assoc : \u2200 a b c : R, a * b * c = a * (b * c))\n#check (mul_one : \u2200 a : R, a * 1 = a)\n#check (one_mul : \u2200 a : R, 1 * a = a)\n#check (mul_add : \u2200 a b c : R, a * (b + c) = a * b + a * c)\n#check (add_mul : \u2200 a b c : R, (a + b) * c = a * c + b * c)\n</code></pre> <p>You will learn more about the square brackets in the first line later, but for the time being, suffice it to say that the declaration gives us a type, <code>R</code>, and a ring structure on <code>R</code>. Lean then allows us to use generic ring notation with elements of <code>R</code>, and to make use of a library of theorems about rings.</p> <p>The names of some of the theorems should look familiar: they are exactly the ones we used to calculate with the real numbers in the last section. Lean is good not only for proving things about concrete mathematical structures like the natural numbers and the integers, but also for proving things about abstract structures, characterized axiomatically, like rings. Moreover, Lean supports generic reasoning about both abstract and concrete structures, and can be trained to recognize appropriate instances. So any theorem about rings can be applied to concrete rings like the integers, <code>\u2124</code>, the rational numbers, <code>\u211a</code>, and the complex numbers <code>\u2102</code>. It can also be applied to any instance of an abstract structure that extends rings, such as any ordered ring or any field.</p> <p>Not all important properties of the real numbers hold in an arbitrary ring, however. For example, multiplication on the real numbers is commutative, but that does not hold in general. If you have taken a course in linear algebra, you will recognize that, for every \\(n\\), the \\(n\\) by \\(n\\) matrices of real numbers form a ring in which commutativity usually fails. If we declare <code>R</code> to be a commutative ring, in fact, all the theorems in the last section continue to hold when we replace <code>\u211d</code> by <code>R</code>.</p> <pre><code>variable (R : Type*) [CommRing R]\nvariable (a b c d : R)\n\nexample : c * b * a = b * (a * c) := by ring\n\nexample : (a + b) * (a + b) = a * a + 2 * (a * b) + b * b := by ring\n\nexample : (a + b) * (a - b) = a ^ 2 - b ^ 2 := by ring\n\nexample (hyp : c = d * a + b) (hyp' : b = a * d) : c = 2 * a * d := by\n  rw [hyp, hyp']\n  ring\n</code></pre> <p>We leave it to you to check that all the other proofs go through unchanged. Notice that when a proof is short, like <code>by ring</code> or <code>by linarith</code> or <code>by sorry</code>, it is common (and permissible) to put it on the same line as the <code>by</code>. Good proof-writing style should strike a balance between concision and readability.</p> <p>The goal of this section is to strengthen the skills you have developed in the last section and apply them to reasoning axiomatically about rings. We will start with the axioms listed above, and use them to derive other facts. Most of the facts we prove are already in Mathlib. We will give the versions we prove the same names to help you learn the contents of the library as well as the naming conventions.</p> <p>Lean provides an organizational mechanism similar to those used in programming languages: when a definition or theorem <code>foo</code> is introduced in a namespace <code>bar</code>, its full name is <code>bar.foo</code>. The command <code>open bar</code> later opens the namespace, which allows us to use the shorter name <code>foo</code>. To avoid errors due to name clashes, in the next example we put our versions of the library theorems in a new namespace called <code>MyRing.</code></p> <p>The next example shows that we do not need <code>add_zero</code> or <code>add_right_neg</code> as ring axioms, because they follow from the other axioms.</p> <pre><code>namespace MyRing\nvariable {R : Type*} [Ring R]\n\ntheorem add_zero (a : R) : a + 0 = a := by rw [add_comm, zero_add]\n\ntheorem add_right_neg (a : R) : a + -a = 0 := by rw [add_comm, add_left_neg]\n\n#check MyRing.add_zero\n#check add_zero\n\nend MyRing\n</code></pre> <p>The net effect is that we can temporarily reprove a theorem in the library, and then go on using the library version after that. But don't cheat! In the exercises that follow, take care to use only the general facts about rings that we have proved earlier in this section.</p> <p>(If you are paying careful attention, you may have noticed that we changed the round brackets in <code>(R : Type*)</code> for curly brackets in <code>{R : Type*}</code>. This declares <code>R</code> to be an implicit argument. We will explain what this means in a moment, but don't worry about it in the meanwhile.)</p> <p>Here is a useful theorem:</p> <pre><code>theorem neg_add_cancel_left (a b : R) : -a + (a + b) = b := by\n  rw [\u2190 add_assoc, add_left_neg, zero_add]\n</code></pre> <p>Prove the companion version:</p> <pre><code>theorem add_neg\\*cancel\\*right (a b : R) : a + b + -b = a := by\n  sorry\n</code></pre> <p>Use these to prove the following:</p> <pre><code>theorem add_left_cancel {a b c : R} (h : a + b = a + c) : b = c := by\n  sorry\n\ntheorem add_right_cancel {a b c : R} (h : a + b = c + b) : a = c := by\n  sorry\n</code></pre> <p>With enough planning, you can do each of them with three rewrites.</p> <p>We will now explain the use of the curly braces. Imagine you are in a situation where you have <code>a</code>, <code>b</code>, and <code>c</code> in your context, as well as a hypothesis <code>h : a + b = a + c</code>, and you would like to draw the conclusion <code>b = c</code>. In Lean, you can apply a theorem to hypotheses and facts just the same way that you can apply them to objects, so you might think that <code>add_left_cancel a b c h</code> is a proof of the fact <code>b = c</code>. But notice that explicitly writing <code>a</code>, <code>b</code>, and <code>c</code> is redundant, because the hypothesis <code>h</code> makes it clear that those are the objects we have in mind. In this case, typing a few extra characters is not onerous, but if we wanted to apply <code>add_left_cancel</code> to more complicated expressions, writing them would be tedious. In cases like these, Lean allows us to mark arguments as implicit, meaning that they are supposed to be left out and inferred by other means, such as later arguments and hypotheses. The curly brackets in<code>{a b c : R}</code>do exactly that. So, given the statement of the theorem above, the correct expression is simply<code>add_left_cancel h</code>.</p> <p>To illustrate, let us show that <code>a * 0 = 0</code> follows from the ring axioms.</p> <pre><code>theorem mul_zero (a : R) : a * 0 = 0 := by\nhave h : a * 0 + a * 0 = a * 0 + 0 := by\nrw [\u2190 mul_add, add_zero, add_zero]\nrw [add_left_cancel h]\n</code></pre> <p>We have used a new trick! If you step through the proof, you can see what is going on. The <code>have</code> tactic introduces a new goal, <code>a * 0 + a * 0 = a * 0 + 0</code>, with the same context as the original goal. The fact that the next line is indented indicates that Lean is expecting a block of tactics that serves to prove this new goal. The indentation therefore promotes a modular style of proof: the indented subproof establishes the goal that was introduced by the <code>have</code>. After that, we are back to proving the original goal, except a new hypothesis <code>h</code> has been added: having proved it, we are now free to use it. At this point, the goal is exactly the result of <code>add_left_cancel h</code>.</p> <p>We could equally well have closed the proof with <code>apply add_left_cancel h</code> or <code>exact add_left_cancel h</code>. The <code>exact</code> tactic takes as argument a proof term which completely proves the current goal, without creating any new goal. The <code>apply</code> tactic is a variant whose argument is not necessarily a complete proof. The missing pieces are either inferred automatically by Lean or become new goals to prove. While the <code>exact</code> tactic is technically redundant since it is strictly less powerful than <code>apply</code>, it makes proof scripts slightly clearer to human readers and easier to maintain when the library evolves.</p> <p>Remember that multiplication is not assumed to be commutative, so the following theorem also requires some work.</p> <pre><code>theorem zero_mul (a : R) : 0 * a = 0 := by\n  sorry\n</code></pre> <p>By now, you should also be able replace each <code>sorry</code> in the next exercise with a proof, still using only facts about rings that we have established in this section.</p> <pre><code>theorem neg_eq_of_add_eq_zero {a b : R} (h : a + b = 0) : -a = b := by\n  sorry\n\ntheorem eq_neg_of_add_eqzero {a b : R} (h : a + b = 0) : a = -b := by\n  sorry\n\ntheorem neg_zero : (-0 : R) = 0 := by\n  apply neg_eq_of_add_eq_zero\n  rw [add_zero]\n\ntheorem neg_neg (a : R) : - -a = a := by\n  sorry\n</code></pre> <p>We had to use the annotation <code>(-0 : R)</code> instead of <code>0</code> in the third theorem because without specifying <code>R</code> it is impossible for Lean to infer which <code>0</code> we have in mind, and by default it would be interpreted as a natural number.</p> <p>In Lean, subtraction in a ring is provably equal to addition of the additive inverse.</p> <pre><code>example (a b : R) : a - b = a + -b :=\n  sub_eq_add_neg a b\n</code></pre> <p>On the real numbers, it is defined that way:</p> <pre><code>example (a b : \u211d) : a - b = a + -b :=\n  rfl\n\nexample (a b : \u211d) : a - b = a + -b := by\n  rfl\n</code></pre> <p>The proof term <code>rfl</code> is short for \"reflexivity\". Presenting it as a proof of <code>a - b = a + -b</code> forces Lean to unfold the definition and recognize both sides as being the same. The <code>rfl</code> tactic does the same. This is an instance of what is known as a definitional equality in Lean's underlying logic. This means that not only can one rewrite with <code>sub_eq_add_neg</code> to replace <code>a - b = a + -b</code>, but in some contexts, when dealing with the real numbers, you can use the two sides of the equation interchangeably. For example, you now have enough information to prove the theorem <code>self_sub</code> from the last section:</p> <pre><code>theorem self_sub (a : R) : a - a = 0 := by\nsorry\n</code></pre> <p>Show that you can prove this using <code>rw</code>, but if you replace the arbitrary ring <code>R</code> by the real numbers, you can also prove it using either <code>apply</code> or <code>exact</code>.</p> <p>Lean knows that <code>1 + 1 = 2</code> holds in any ring. With a bit of effort, you can use that to prove the theorem <code>two_mul</code> from the last section:</p> <pre><code>theorem one_add_one_eq_two : 1 + 1 = (2 : R) := by\n  norm_num\n\ntheorem two_mul (a : R) : 2 * a = a + a := by\n  sorry\n</code></pre> <p>We close this section by noting that some of the facts about addition and negation that we established above do not need the full strength of the ring axioms, or even commutativity of addition. The weaker notion of a group can be axiomatized as follows:</p> <pre><code>variable (A : Type*) [AddGroup A]\n\n#check (add_assoc : \u2200 a b c : A, a + b + c = a + (b + c))\n#check (zero_add : \u2200 a : A, 0 + a = a)\n#check (add_left_neg : \u2200 a : A, -a + a = 0)\n</code></pre> <p>It is conventional to use additive notation when the group operation is commutative, and multiplicative notation otherwise. So Lean defines a multiplicative version as well as the additive version (and also their abelian variants, <code>AddCommGroup</code> and <code>CommGroup</code>).</p> <pre><code>variable {G : Type*} [Group G]\n\nrcheck (mul_assoc : \u2200 a b c : G, a * b * c = a * (b * c))\n#check (one_mul : \u2200 a : G, 1 * a = a)\n#check (mul_left_inv : \u2200 a : G, a\u207b\u00b9 * a = 1)\n</code></pre> <p>If you are feeling cocky, try proving the following facts about groups, using only these axioms. You will need to prove a number of helper lemmas along the way. The proofs we have carried out in this section provide some hints.</p> <pre><code>theorem mul_right_inv (a : G) : a \\* a\u207b\u00b9 = 1 := by\n  sorry\n\ntheorem mul_one (a : G) : a * 1 = a := by\n  sorry\n\ntheorem mul_inv_rev (a b : G) : (a * b)\u207b\u00b9 = b\u207b\u00b9 * a\u207b\u00b9 := by\n  sorry\n</code></pre> <p>Explicitly invoking those lemmas is tedious, so Mathlib provides tactics similar to <code>ring</code> in order to cover most uses: <code>group</code> is for non-commutative multiplicative groups, <code>abel</code> for abelian additive groups, and <code>noncomm_ring</code> for non-commutative rings. It may seem odd that the algebraic structures are called <code>Ring</code> and <code>CommRing</code> while the tactics are named <code>noncomm_ring</code> and <code>ring</code>. This is partly for historical reasons, but also for the convenience of using a shorter name for the tactic that deals with commutative rings, since it is used more often.</p>"},{"location":"en/C02_Basics/#using-theorems-and-lemmas","title":"Using Theorems and Lemmas","text":"<p>Rewriting is great for proving equations, but what about other sorts of theorems? For example, how can we prove an inequality, like the fact that :math:<code>a + e^b \\le a + e^c</code> holds whenever :math:<code>b \\le c</code>? We have already seen that theorems can be applied to arguments and hypotheses, and that the <code>apply</code> and <code>exact</code> tactics can be used to solve goals. In this section, we will make good use of these tools.</p> <p>Consider the library theorems <code>le_refl</code> and <code>le_trans</code>:</p> <pre><code>#check (le_refl : \u2200 a : \u211d, a \u2264 a)\n#check (le_trans : a \u2264 b \u2192 b \u2264 c \u2192 a \u2264 c)\n</code></pre> <p>As we explain in more detail in :numref:<code>implication_and_the_universal_quantifier</code>, the implicit parentheses in the statement of <code>le_trans</code> associate to the right, so it should be interpreted as <code>a \u2264 b \u2192 (b \u2264 c \u2192 a \u2264 c)</code>. The library designers have set the arguments <code>a</code>, <code>b</code> and <code>c</code> to <code>le_trans</code> implicit, so that Lean will not let you provide them explicitly (unless you really insist, as we will discuss later). Rather, it expects to infer them from the context in which they are used. For example, when hypotheses <code>h : a \u2264 b</code> and <code>h' : b \u2264 c</code> are in the context, all the following work:</p> <pre><code>variable (h : a \u2264 b) (h' : b \u2264 c)\n\n#check (le_refl : \u2200 a : Real, a \u2264 a)\n#check (le_refl a : a \u2264 a)\n#check (le_trans : a \u2264 b \u2192 b \u2264 c \u2192 a \u2264 c)\n#check (le_trans h : b \u2264 c \u2192 a \u2264 c)\n#check (le_trans h h' : a \u2264 c)\n</code></pre> <p>The <code>apply</code> tactic takes a proof of a general statement or implication, tries to match the conclusion with the current goal, and leaves the hypotheses, if any, as new goals. If the given proof matches the goal exactly (modulo definitional equality), you can use the <code>exact</code> tactic instead of <code>apply</code>. So, all of these work:</p> <pre><code>example (x y z : \u211d) (h\u2080 : x \u2264 y) (h\u2081 : y \u2264 z) : x \u2264 z := by\n  apply le_trans\n    apply h\u2080\n    apply h\u2081\n\nexample (x y z : \u211d) (h\u2080 : x \u2264 y) (h\u2081 : y \u2264 z) : x \u2264 z := by\n  apply le_trans h\u2080\n  apply h\u2081\n\nexample (x y z : \u211d) (h\u2080 : x \u2264 y) (h\u2081 : y \u2264 z) : x \u2264 z :=\n  le_trans h\u2080 h\u2081\n\nexample (x : \u211d) : x \u2264 x := by\n  apply le_refl\n\nexample (x : \u211d) : x \u2264 x :=\n  le_refl x\n</code></pre> <p>In the first example, applying <code>le_trans</code> creates two goals, and we use the dots to indicate where the proof of each begins. The dots are optional, but they serve to focus the goal: within the block introduced by the dot, only one goal is visible, and it must be completed before the end of the block. Here we end the first block by starting a new one with another dot. We could just as well have decreased the indentation. In the fourth example and in the last example, we avoid going into tactic mode entirely: <code>le_trans h\u2080 h\u2081</code> and <code>le_refl x</code> are the proof terms we need.</p> <p>Here are a few more library theorems:</p> <pre><code>#check (le_refl : \u2200 a, a \u2264 a)\n#check (le_trans : a \u2264 b \u2192 b \u2264 c \u2192 a \u2264 c)\n#check (lt_of_le_of_lt : a \u2264 b \u2192 b &lt; c \u2192 a &lt; c)\n#check (lt_of_lt_of_le : a &lt; b \u2192 b \u2264 c \u2192 a &lt; c)\n#check (lt_trans : a &lt; b \u2192 b &lt; c \u2192 a &lt; c)\n</code></pre> <p>Use them together with <code>apply</code> and <code>exact</code> to prove the following:</p> <pre><code>example (h\u2080 : a \u2264 b) (h\u2081 : b &lt; c) (h\u2082 : c \u2264 d) (h\u2083 : d &lt; e) : a &lt; e := by\n  sorry\n</code></pre> <p>In fact, Lean has a tactic that does this sort of thing automatically:</p> <pre><code>example (h\u2080 : a \u2264 b) (h\u2081 : b &lt; c) (h\u2082 : c \u2264 d) (h\u2083 : d &lt; e) : a &lt; e := by\n  linarith\n</code></pre> <p>The <code>linarith</code> tactic is designed to handle linear arithmetic.</p> <pre><code>example (h : 2 * a \u2264 3 * b) (h' : 1 \u2264 a) (h'' : d = 2) : d + a \u2264 5 * b := by\n  linarith\n</code></pre> <p>In addition to equations and inequalities in the context, <code>linarith</code> will use additional inequalities that you pass as arguments. In the next example, <code>exp_le_exp.mpr h'</code> is a proof of <code>exp b \u2264 exp c</code>, as we will explain in a moment. Notice that, in Lean, we write <code>f x</code> to denote the application of a function <code>f</code> to the argument <code>x</code>, exactly the same way we write <code>h x</code> to denote the result of applying a fact or theorem <code>h</code> to the argument <code>x</code>. Parentheses are only needed for compound arguments, as in <code>f (x + y)</code>. Without the parentheses, <code>f x + y</code> would be parsed as <code>(f x) + y</code>.</p> <pre><code>example (h : 1 \u2264 a) (h' : b \u2264 c) : 2 + a + exp b \u2264 3 * a + exp c := by\n  linarith [exp_le_exp.mpr h']\n</code></pre> <p>Here are some more theorems in the library that can be used to establish inequalities on the real numbers.</p> <pre><code>#check (exp_le_exp : exp a \u2264 exp b \u2194 a \u2264 b)\n#check (exp_lt_exp : exp a &lt; exp b \u2194 a &lt; b)\n#check (log_le_log : 0 &lt; a \u2192 a \u2264 b \u2192 log a \u2264 log b)\n#check (log_lt_log : 0 &lt; a \u2192 a &lt; b \u2192 log a &lt; log b)\n#check (add_le_add : a \u2264 b \u2192 c \u2264 d \u2192 a + c \u2264 b + d)\n#check (add_le_add_left : a \u2264 b \u2192 \u2200 c, c + a \u2264 c + b)\n#check (add_le_add_right : a \u2264 b \u2192 \u2200 c, a + c \u2264 b + c)\n#check (add_lt_add_of_le_of_lt : a \u2264 b \u2192 c &lt; d \u2192 a + c &lt; b + d)\n#check (add_lt_add_of_lt_of_le : a &lt; b \u2192 c \u2264 d \u2192 a + c &lt; b + d)\n#check (add_lt_add_left : a &lt; b \u2192 \u2200 c, c + a &lt; c + b)\n#check (add_lt_add_right : a &lt; b \u2192 \u2200 c, a + c &lt; b + c)\n#check (add_nonneg : 0 \u2264 a \u2192 0 \u2264 b \u2192 0 \u2264 a + b)\n#check (add_pos : 0 &lt; a \u2192 0 &lt; b \u2192 0 &lt; a + b)\n#check (add_pos_of_pos_of_nonneg : 0 &lt; a \u2192 0 \u2264 b \u2192 0 &lt; a + b)\n#check (exp_pos : \u2200 a, 0 &lt; exp a)\n#check add_le_add_left\n</code></pre> <p>Some of the theorems, <code>exp_le_exp</code>, <code>exp_lt_exp</code> use a bi-implication, which represents the phrase \"if and only if.\" (You can type it in VS Code with <code>\\lr</code> of <code>\\iff</code>). We will discuss this connective in greater detail in the next chapter. Such a theorem can be used with <code>rw</code> to rewrite a goal to an equivalent one:</p> <pre><code>example (h : a \u2264 b) : exp a \u2264 exp b := by\n  rw [exp_le_exp]\n  exact h\n</code></pre> <p>In this section, however, we will use the fact that if <code>h : A \u2194 B</code> is such an equivalence, then <code>h.mp</code> establishes the forward direction, <code>A \u2192 B</code>, and <code>h.mpr</code> establishes the reverse direction, <code>B \u2192 A</code>. Here, <code>mp</code> stands for \"modus ponens\" and <code>mpr</code> stands for \"modus ponens reverse.\" You can also use <code>h.1</code> and <code>h.2</code> for <code>h.mp</code> and <code>h.mpr</code>, respectively, if you prefer. Thus the following proof works:</p> <pre><code>example (h\u2080 : a \u2264 b) (h\u2081 : c &lt; d) : a + exp c + e &lt; b + exp d + e := by\n  apply add_lt_add_of_lt_of_le\n  \u00b7 apply add_lt_add_of_le_of_lt h\u2080\n    apply exp_lt_exp.mpr h\u2081\n  apply le_refl\n</code></pre> <p>The first line, <code>apply add_lt_add_of_lt_of_le</code>, creates two goals, and once again we use a dot to separate the proof of the first from the proof of the second.</p> <p>Try the following examples on your own. The example in the middle shows you that the <code>norm_num</code> tactic can be used to solve concrete numeric goals.</p> <pre><code>example (h\u2080 : d \u2264 e) : c + exp (a + d) \u2264 c + exp (a + e) := by sorry\n\nexample : (0 : \u211d) &lt; 1 := by norm_num\n\nexample (h : a \u2264 b) : log (1 + exp a) \u2264 log (1 + exp b) := by\n  have h\u2080 : 0 &lt; 1 + exp a := by sorry\n  apply log_le_log h\u2080\n  sorry\n</code></pre> <p>From these examples, it should be clear that being able to find the library theorems you need constitutes an important part of formalization. There are a number of strategies you can use:</p> <ul> <li> <p>You can browse Mathlib in its   GitHub repository.</p> </li> <li> <p>You can use the API documentation on the Mathlib   web pages.</p> </li> <li> <p>You can rely on Mathlib naming conventions and Ctrl-space completion in   the editor to guess a theorem name (or Cmd-space on a Mac keyboard).   In Lean, a theorem named <code>A_of_B_of_C</code> establishes   something of the form <code>A</code> from hypotheses of the form <code>B</code> and <code>C</code>,   where <code>A</code>, <code>B</code>, and <code>C</code>   approximate the way we might read the goals out loud.   So a theorem establishing something like <code>x + y \u2264 ...</code> will probably   start with <code>add_le</code>.   Typing <code>add_le</code> and hitting Ctrl-space will give you some helpful choices.   Note that hitting Ctrl-space twice displays more information about the available   completions.</p> </li> <li> <p>If you right-click on an existing theorem name in VS Code,   the editor will show a menu with the option to   jump to the file where the theorem is defined,   and you can find similar theorems nearby.</p> </li> <li> <p>You can use the <code>apply?</code> tactic,   which tries to find the relevant theorem in the library.</p> </li> </ul> <pre><code>example : 0 \u2264 a ^ 2 := by\n  -- apply?\n  exact sq_nonneg a\n</code></pre> <p>To try out <code>apply?</code> in this example, delete the <code>exact</code> command and uncomment the previous line. Using these tricks, see if you can find what you need to do the next example:</p> <pre><code>example (h : a \u2264 b) : c - exp b \u2264 c - exp a := by\n  sorry\n</code></pre> <p>Using the same tricks, confirm that <code>linarith</code> instead of <code>apply?</code> can also finish the job.</p> <p>Here is another example of an inequality:</p> <pre><code>example : 2 * a * b \u2264 a ^ 2 + b ^ 2 := by\n  have h : 0 \u2264 a ^ 2 - 2 * a * b + b ^ 2\n  calc\n    a ^ 2 - 2 * a * b + b ^ 2 = (a - b) ^ 2 := by ring\n    _ \u2265 0 := by apply pow_two_nonneg\n\n  calc\n    2 * a * b = 2 * a * b + 0 := by ring\n    _ \u2264 2 * a * b + (a ^ 2 - 2 * a * b + b ^ 2) := add_le_add (le_refl _) h\n    _ = a ^ 2 + b ^ 2 := by ring\n</code></pre> <p>Mathlib tends to put spaces around binary operations like <code>*</code> and <code>^</code>, but in this example, the more compressed format increases readability. There are a number of things worth noticing. First, an expression <code>s \u2265 t</code> is definitionally equivalent to <code>t \u2264 s</code>. In principle, this means one should be able to use them interchangeably. But some of Lean's automation does not recognize the equivalence, so Mathlib tends to favor <code>\u2264</code> over <code>\u2265</code>. Second, we have used the <code>ring</code> tactic extensively. It is a real timesaver! Finally, notice that in the second line of the second <code>calc</code> proof, instead of writing <code>by exact add_le_add (le_refl _) h</code>, we can simply write the proof term <code>add_le_add (le_refl _) h</code>.</p> <p>In fact, the only cleverness in the proof above is figuring out the hypothesis <code>h</code>. Once we have it, the second calculation involves only linear arithmetic, and <code>linarith</code> can handle it:</p> <pre><code>example : 2 * a * b \u2264 a ^ 2 + b ^ 2 := by\n  have h : 0 \u2264 a ^ 2 - 2 * a * b + b ^ 2\n  calc\n    a ^ 2 - 2 * a * b + b ^ 2 = (a - b) ^ 2 := by ring\n    _ \u2265 0 := by apply pow_two_nonneg\n  linarith\n</code></pre> <p>How nice! We challenge you to use these ideas to prove the following theorem. You can use the theorem <code>abs_le'.mpr</code>. You will also need the <code>constructor</code> tactic to split a conjunction to two goals; see :numref:<code>conjunction_and_biimplication</code>.</p> <pre><code>example : |a * b| \u2264 (a ^ 2 + b ^ 2) / 2 := by\n  sorry\n\n#check abs_le'.mpr\n</code></pre> <p>If you managed to solve this, congratulations! You are well on your way to becoming a master formalizer.</p>"},{"location":"en/C02_Basics/#more-examples-using-apply-and-rw","title":"More examples using apply and rw","text":"<p>The <code>min</code> function on the real numbers is uniquely characterized by the following three facts: section</p> <pre><code>#check (min_le_left a b : min a b \u2264 a)\n#check (min_le_right a b : min a b \u2264 b)\n#check (le_min : c \u2264 a \u2192 c \u2264 b \u2192 c \u2264 min a b)\n</code></pre> <p>Can you guess the names of the theorems that characterize <code>max</code> in a similar way?</p> <p>Notice that we have to apply <code>min</code> to a pair of arguments <code>a</code> and <code>b</code> by writing <code>min a b</code> rather than <code>min (a, b)</code>. Formally, <code>min</code> is a function of type <code>\u211d \u2192 \u211d \u2192 \u211d</code>. When we write a type like this with multiple arrows, the convention is that the implicit parentheses associate to the right, so the type is interpreted as <code>\u211d \u2192 (\u211d \u2192 \u211d)</code>. The net effect is that if <code>a</code> and <code>b</code> have type <code>\u211d</code> then <code>min a</code> has type <code>\u211d \u2192 \u211d</code> and <code>min a b</code> has type <code>\u211d</code>, so <code>min</code> acts like a function of two arguments, as we expect. Handling multiple arguments in this way is known as currying, after the logician Haskell Curry.</p> <p>The order of operations in Lean can also take some getting used to. Function application binds tighter than infix operations, so the expression <code>min a b + c</code> is interpreted as <code>(min a b) + c</code>. With time, these conventions will become second nature.</p> <p>Using the theorem <code>le_antisymm</code>, we can show that two real numbers are equal if each is less than or equal to the other. Using this and the facts above, we can show that <code>min</code> is commutative:</p> <pre><code>example : min a b = min b a := by\n  apply le_antisymm\n  \u00b7 show min a b \u2264 min b a\n    apply le_min\n    \u00b7 apply min_le_right\n    apply min_le_left\n  \u00b7 show min b a \u2264 min a b\n    apply le_min\n    \u00b7 apply min_le_right\n    apply min_le_left\n</code></pre> <p>Here we have used dots to separate proofs of different goals. Our usage is inconsistent: at the outer level, we use dots and indentation for both goals, whereas for the nested proofs, we use dots only until a single goal remains. Both conventions are reasonable and useful. We also use the <code>show</code> tactic to structure the proof and indicate what is being proved in each block. The proof still works without the <code>show</code> commands, but using them makes the proof easier to read and maintain.</p> <p>It may bother you that the proof is repetitive. To foreshadow skills you will learn later on, we note that one way to avoid the repetition is to state a local lemma and then use it:</p> <pre><code>example : min a b = min b a := by\n  have h : \u2200 x y : \u211d, min x y \u2264 min y x := by\n    intro x y\n    apply le_min\n    apply min_le_right\n    apply min_le_left\n  apply le_antisymm\n  apply h\n  apply h\n</code></pre> <p>We will say more about the universal quantifier in :numref:<code>implication_and_the_universal_quantifier</code>, but suffice it to say here that the hypothesis <code>h</code> says that the desired inequality holds for any <code>x</code> and <code>y</code>, and the <code>intro</code> tactic introduces an arbitrary <code>x</code> and <code>y</code> to establish the conclusion. The first <code>apply</code> after <code>le_antisymm</code> implicitly uses <code>h a b</code>, whereas the second one uses <code>h b a</code>.</p> <p>Another solution is to use the <code>repeat</code> tactic, which applies a tactic (or a block) as many times as it can.</p> <pre><code>example : min a b = min b a := by\n  apply le_antisymm\n  repeat\n    apply le_min\n    apply min_le_right\n    apply min_le_left\n</code></pre> <p>We encourage you to prove the following as exercises. You can use either of the tricks just described to shorten the first.</p> <pre><code>example : max a b = max b a := by\n  sorry\nexample : min (min a b) c = min a (min b c) := by\n  sorry\n</code></pre> <p>Of course, you are welcome to prove the associativity of <code>max</code> as well.</p> <p>It is an interesting fact that <code>min</code> distributes over <code>max</code> the way that multiplication distributes over addition, and vice-versa. In other words, on the real numbers, we have the identity <code>min a (max b c) = max (min a b) (min a c)</code> as well as the corresponding version with <code>max</code> and <code>min</code> switched. But in the next section we will see that this does not follow from the transitivity and reflexivity of <code>\u2264</code> and the characterizing properties of <code>min</code> and <code>max</code> enumerated above. We need to use the fact that <code>\u2264</code> on the real numbers is a total order, which is to say, it satisfies <code>\u2200 x y, x \u2264 y \u2228 y \u2264 x</code>. Here the disjunction symbol, <code>\u2228</code>, represents \"or\". In the first case, we have <code>min x y = x</code>, and in the second case, we have <code>min x y = y</code>. We will learn how to reason by cases in :numref:<code>disjunction</code>, but for now we will stick to examples that don't require the case split.</p> <p>Here is one such example:</p> <pre><code>theorem aux : min a b + c \u2264 min (a + c) (b + c) := by\n  sorry\nexample : min a b + c = min (a + c) (b + c) := by\n  sorry\n</code></pre> <p>It is clear that <code>aux</code> provides one of the two inequalities needed to prove the equality, but applying it to suitable values yields the other direction as well. As a hint, you can use the theorem <code>add_neg_cancel_right</code> and the <code>linarith</code> tactic.</p> <p>Lean's naming convention is made manifest in the library's name for the triangle inequality:</p> <pre><code>#check (abs_add : \u2200 a b : \u211d, |a + b| \u2264 |a| + |b|)\n</code></pre> <p>Use it to prove the following variant, using also <code>add_sub_cancel_right</code>:</p> <pre><code>example : |a| - |b| \u2264 |a - b| :=\n  sorry\nend\n</code></pre> <p>See if you can do this in three lines or less. You can use the theorem <code>sub_add_cancel</code>.</p> <p>Another important relation that we will make use of in the sections to come is the divisibility relation on the natural numbers, <code>x \u2223 y</code>. Be careful: the divisibility symbol is not the ordinary bar on your keyboard. Rather, it is a unicode character obtained by typing <code>\\|</code> in VS Code. By convention, Mathlib uses <code>dvd</code> to refer to it in theorem names.</p> <pre><code>example (h\u2080 : x \u2223 y) (h\u2081 : y \u2223 z) : x \u2223 z :=\n  dvd_trans h\u2080 h\u2081\n\nexample : x \u2223 y * x * z := by\n  apply dvd_mul_of_dvd_left\n  apply dvd_mul_left\n\nexample : x \u2223 x ^ 2 := by\n  apply dvd_mul_left\n</code></pre> <p>In the last example, the exponent is a natural number, and applying <code>dvd_mul_left</code> forces Lean to expand the definition of <code>x^2</code> to <code>x^1 * x</code>. See if you can guess the names of the theorems you need to prove the following:</p> <pre><code>example (h : x \u2223 w) : x \u2223 y * (x * z) + x ^ 2 + w ^ 2 := by\n  sorry\nend\n</code></pre> <p>With respect to divisibility, the greatest common divisor, <code>gcd</code>, and least common multiple, <code>lcm</code>, are analogous to <code>min</code> and <code>max</code>. Since every number divides <code>0</code>, <code>0</code> is really the greatest element with respect to divisibility:</p> <pre><code>variable (m n : \u2115)\n\n#check (Nat.gcd_zero_right n : Nat.gcd n 0 = n)\n#check (Nat.gcd_zero_left n : Nat.gcd 0 n = n)\n#check (Nat.lcm_zero_right n : Nat.lcm n 0 = 0)\n#check (Nat.lcm_zero_left n : Nat.lcm 0 n = 0)\n</code></pre> <p>See if you can guess the names of the theorems you will need to prove the following:</p> <pre><code>example : Nat.gcd m n = Nat.gcd n m := by\n  sorry\n</code></pre> <p>Hint: you can use <code>dvd_antisymm</code>, but if you do, Lean will complain that the expression is ambiguous between the generic theorem and the version <code>Nat.dvd_antisymm</code>, the one specifically for the natural numbers. You can use <code>_root_.dvd_antisymm</code> to specify the generic one; either one will work.</p>"},{"location":"en/C02_Basics/#proving-facts-about-algebraic-structures","title":"Proving Facts about Algebraic Structures","text":"<p>In :numref:<code>proving_identities_in_algebraic_structures</code>, we saw that many common identities governing the real numbers hold in more general classes of algebraic structures, such as commutative rings. We can use any axioms we want to describe an algebraic structure, not just equations. For example, a partial order consists of a set with a binary relation that is reflexive, transitive, and antisymmetric. like <code>\u2264</code> on the real numbers. Lean knows about partial orders: TEXT. -/ section</p> <pre><code>variable {\u03b1 : Type*} [PartialOrder \u03b1]\nvariable (x y z : \u03b1)\n\n#check x \u2264 y\n#check (le_refl x : x \u2264 x)\n#check (le_trans : x \u2264 y \u2192 y \u2264 z \u2192 x \u2264 z)\n#check (le_antisymm : x \u2264 y \u2192 y \u2264 x \u2192 x = y)\n</code></pre> <p>Here we are adopting the Mathlib convention of using letters like <code>\u03b1</code>, <code>\u03b2</code>, and <code>\u03b3</code> (entered as <code>\\a</code>, <code>\\b</code>, and <code>\\g</code>) for arbitrary types. The library often uses letters like <code>R</code> and <code>G</code> for the carriers of algebraic structures like rings and groups, respectively, but in general Greek letters are used for types, especially when there is little or no structure associated with them.</p> <p>Associated to any partial order, <code>\u2264</code>, there is also a strict partial order, <code>&lt;</code>, which acts somewhat like <code>&lt;</code> on the real numbers. Saying that <code>x</code> is less than <code>y</code> in this order is equivalent to saying that it is less-than-or-equal to <code>y</code> and not equal to <code>y</code>.</p> <pre><code>#check x &lt; y\n#check (lt_irrefl x : \u00ac (x &lt; x))\n#check (lt_trans : x &lt; y \u2192 y &lt; z \u2192 x &lt; z)\n#check (lt_of_le_of_lt : x \u2264 y \u2192 y &lt; z \u2192 x &lt; z)\n#check (lt_of_lt_of_le : x &lt; y \u2192 y \u2264 z \u2192 x &lt; z)\n\nexample : x &lt; y \u2194 x \u2264 y \u2227 x \u2260 y :=\n  lt_iff_le_and_ne\n</code></pre> <p>In this example, the symbol <code>\u2227</code> stands for \"and,\" the symbol <code>\u00ac</code> stands for \"not,\" and <code>x \u2260 y</code> abbreviates <code>\u00ac (x = y)</code>. In :numref:<code>Chapter %s &lt;logic&gt;</code>, you will learn how to use these logical connectives to prove that <code>&lt;</code> has the properties indicated.</p> <p>A lattice is a structure that extends a partial order with operations <code>\u2293</code> and <code>\u2294</code> that are analogous to <code>min</code> and <code>max</code> on the real numbers:</p> <pre><code>variable {\u03b1 : Type*} [Lattice \u03b1]\nvariable (x y z : \u03b1)\n\n#check x \u2293 y\n#check (inf_le_left : x \u2293 y \u2264 x)\n#check (inf_le_right : x \u2293 y \u2264 y)\n#check (le_inf : z \u2264 x \u2192 z \u2264 y \u2192 z \u2264 x \u2293 y)\n#check x \u2294 y\n#check (le_sup_left : x \u2264 x \u2294 y)\n#check (le_sup_right : y \u2264 x \u2294 y)\n#check (sup_le : x \u2264 z \u2192 y \u2264 z \u2192 x \u2294 y \u2264 z)\n</code></pre> <p>The characterizations of <code>\u2293</code> and <code>\u2294</code> justify calling them the greatest lower bound and least upper bound, respectively. You can type them in VS code using <code>\\glb</code> and <code>\\lub</code>. The symbols are also often called then infimum and the supremum, and Mathlib refers to them as <code>inf</code> and <code>sup</code> in theorem names. To further complicate matters, they are also often called meet and join. Therefore, if you work with lattices, you have to keep the following dictionary in mind:</p> <ul> <li> <p><code>\u2293</code> is the greatest lower bound, infimum, or meet.</p> </li> <li> <p><code>\u2294</code> is the least upper bound, supremum, or join.</p> </li> </ul> <p>Some instances of lattices include:</p> <ul> <li> <p><code>min</code> and <code>max</code> on any total order, such as the integers or real numbers with <code>\u2264</code></p> </li> <li> <p><code>\u2229</code> and <code>\u222a</code> on the collection of subsets of some domain, with the ordering <code>\u2286</code></p> </li> <li> <p><code>\u2227</code> and <code>\u2228</code> on boolean truth values, with ordering <code>x \u2264 y</code> if either <code>x</code> is false or <code>y</code> is true</p> </li> <li> <p><code>gcd</code> and <code>lcm</code> on the natural numbers (or positive natural numbers), with the divisibility ordering, <code>\u2223</code></p> </li> <li> <p>the collection of linear subspaces of a vector space,   where the greatest lower bound is given by the intersection,   the least upper bound is given by the sum of the two spaces,   and the ordering is inclusion</p> </li> <li> <p>the collection of topologies on a set (or, in Lean, a type),   where the greatest lower bound of two topologies consists of   the topology that is generated by their union,   the least upper bound is their intersection,   and the ordering is reverse inclusion</p> </li> </ul> <p>You can check that, as with <code>min</code> / <code>max</code> and <code>gcd</code> / <code>lcm</code>, you can prove the commutativity and associativity of the infimum and supremum using only their characterizing axioms, together with <code>le_refl</code> and <code>le_trans</code>.</p> <p>Using <code>apply le_trans</code> when seeing a goal <code>x \u2264 z</code> is not a great idea. Indeed Lean has no way to guess which intermediate element <code>y</code> we want to use. So <code>apply le_trans</code> produces three goals that look like<code>x \u2264 ?a</code>, <code>?a \u2264 z</code> and <code>\u03b1</code> where <code>?a</code> (probably with a more complicated auto-generated name) stands for the mysterious <code>y</code>. The last goal, with type <code>\u03b1</code>, is to provide the value of <code>y</code>. It comes lasts because Lean hopes to automatically infer it from the proof of the first goal <code>x \u2264 ?a</code>. In order to avoid this unappealing situation, you can use the <code>calc</code> tactic to explicitly provide <code>y</code>. Alternatively, you can use the <code>trans</code> tactic which takes <code>y</code> as an argument and produces the expected goals <code>x \u2264 y</code> and <code>y \u2264 z</code>. Of course you can also avoid this issue by providing directly a full proof such as <code>exact le_trans inf_le_left inf_le_right</code>, but this requires a lot more planning.</p> <pre><code>example : x \u2293 y = y \u2293 x := by\n  sorry\n\nexample : x \u2293 y \u2293 z = x \u2293 (y \u2293 z) := by\n  sorry\n\nexample : x \u2294 y = y \u2294 x := by\n  sorry\n\nexample : x \u2294 y \u2294 z = x \u2294 (y \u2294 z) := by\n  sorry\n</code></pre> <p>You can find these theorems in the Mathlib as <code>inf_comm</code>, <code>inf_assoc</code>, <code>sup_comm</code>, and <code>sup_assoc</code>, respectively.</p> <p>Another good exercise is to prove the absorption laws using only those axioms:</p> <pre><code>theorem absorb1 : x \u2293 (x \u2294 y) = x := by\n  sorry\n\ntheorem absorb2 : x \u2294 x \u2293 y = x := by\n  sorry\n</code></pre> <p>These can be found in Mathlib with the names <code>inf_sup_self</code> and <code>sup_inf_self</code>.</p> <p>A lattice that satisfies the additional identities <code>x \u2293 (y \u2294 z) = (x \u2293 y) \u2294 (x \u2293 z)</code> and <code>x \u2294 (y \u2293 z) = (x \u2294 y) \u2293 (x \u2294 z)</code> is called a distributive lattice. Lean knows about these too:</p> <pre><code>variable {\u03b1 : Type*} [DistribLattice \u03b1]\nvariable (x y z : \u03b1)\n\n#check (inf_sup_left x y z : x \u2293 (y \u2294 z) = x \u2293 y \u2294 x \u2293 z)\n#check (inf_sup_right x y z : (x \u2294 y) \u2293 z = x \u2293 z \u2294 y \u2293 z)\n#check (sup_inf_left x y z : x \u2294 y \u2293 z = (x \u2294 y) \u2293 (x \u2294 z))\n#check (sup_inf_right x y z : x \u2293 y \u2294 z = (x \u2294 z) \u2293 (y \u2294 z))\n</code></pre> <p>The left and right versions are easily shown to be equivalent, given the commutativity of <code>\u2293</code> and <code>\u2294</code>. It is a good exercise to show that not every lattice is distributive by providing an explicit description of a nondistributive lattice with finitely many elements. It is also a good exercise to show that in any lattice, either distributivity law implies the other:</p> <pre><code>variable {\u03b1 : Type*} [Lattice \u03b1]\nvariable (a b c : \u03b1)\n\nexample (h : \u2200 x y z : \u03b1, x \u2293 (y \u2294 z) = x \u2293 y \u2294 x \u2293 z) : a \u2294 b \u2293 c = (a \u2294 b) \u2293 (a \u2294 c) := by\n  sorry\n\nexample (h : \u2200 x y z : \u03b1, x \u2294 y \u2293 z = (x \u2294 y) \u2293 (x \u2294 z)) : a \u2293 (b \u2294 c) = a \u2293 b \u2294 a \u2293 c := by\n  sorry\n</code></pre> <p>It is possible to combine axiomatic structures into larger ones. For example, a strict ordered ring consists of a commutative ring together with a partial order on the carrier satisfying additional axioms that say that the ring operations are compatible with the order:</p> <pre><code>variable {R : Type*} [StrictOrderedRing R]\nvariable (a b c : R)\n\n#check (add_le_add_left : a \u2264 b \u2192 \u2200 c, c + a \u2264 c + b)\n#check (mul_pos : 0 &lt; a \u2192 0 &lt; b \u2192 0 &lt; a * b)\n</code></pre> <p>:numref:<code>Chapter %s &lt;logic&gt;</code> will provide the means to derive the following from <code>mul_pos</code> and the definition of <code>&lt;</code>:</p> <pre><code>#check (mul_nonneg : 0 \u2264 a \u2192 0 \u2264 b \u2192 0 \u2264 a * b)\n</code></pre> <p>It is then an extended exercise to show that many common facts used to reason about arithmetic and the ordering on the real numbers hold generically for any ordered ring. Here are a couple of examples you can try, using only properties of rings, partial orders, and the facts enumerated in the last two examples:</p> <pre><code>example (h : a \u2264 b) : 0 \u2264 b - a := by\n  sorry\n\nexample (h: 0 \u2264 b - a) : a \u2264 b := by\n  sorry\n\nexample (h : a \u2264 b) (h' : 0 \u2264 c) : a * c \u2264 b * c := by\n  sorry\n</code></pre> <p>Finally, here is one last example. A metric space consists of a set equipped with a notion of distance, <code>dist x y</code>, mapping any pair of elements to a real number. The distance function is assumed to satisfy the following axioms:</p> <pre><code>variable {X : Type*} [MetricSpace X]\nvariable (x y z : X)\n\n#check (dist_self x : dist x x = 0)\n#check (dist_comm x y : dist x y = dist y x)\n#check (dist_triangle x y z : dist x z \u2264 dist x y + dist y z)\n</code></pre> <p>Having mastered this section, you can show that it follows from these axioms that distances are always nonnegative:</p> <pre><code>example (x y : X) : 0 \u2264 dist x y := by\n  sorry\n</code></pre> <p>We recommend making use of the theorem <code>nonneg_of_mul_nonneg_left</code>. As you may have guessed, this theorem is called <code>dist_nonneg</code> in Mathlib.</p>"},{"location":"en/C03_Logic/","title":"Logic","text":"<p>In the last chapter, we dealt with equations, inequalities, and basic mathematical statements like \":math:<code>x</code> divides :math:<code>y</code>.\" Complex mathematical statements are built up from simple ones like these using logical terms like \"and,\" \"or,\" \"not,\" and \"if ... then,\" \"every,\" and \"some.\" In this chapter, we show you how to work with statements that are built up in this way.</p>"},{"location":"en/C03_Logic/#implication-and-the-universal-quantifier","title":"Implication and the Universal Quantifier","text":"<p>Consider the statement after the <code>#check</code>:</p> <pre><code>#check \u2200 x : \u211d, 0 \u2264 x \u2192 |x| = x\n</code></pre> <p>In words, we would say \"for every real number <code>x</code>, if <code>0 \u2264 x</code> then the absolute value of <code>x</code> equals <code>x</code>\". We can also have more complicated statements like:</p> <pre><code>#check \u2200 x y \u03b5 : \u211d, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5\n</code></pre> <p>In words, we would say \"for every <code>x</code>, <code>y</code>, and <code>\u03b5</code>, if <code>0 &lt; \u03b5 \u2264 1</code>, the absolute value of <code>x</code> is less than <code>\u03b5</code>, and the absolute value of <code>y</code> is less than <code>\u03b5</code>, then the absolute value of <code>x * y</code> is less than <code>\u03b5</code>.\" In Lean, in a sequence of implications there are implicit parentheses grouped to the right. So the expression above means \"if <code>0 &lt; \u03b5</code> then if <code>\u03b5 \u2264 1</code> then if <code>|x| &lt; \u03b5</code> ...\" As a result, the expression says that all the assumptions together imply the conclusion.</p> <p>You have already seen that even though the universal quantifier in this statement ranges over objects and the implication arrows introduce hypotheses, Lean treats the two in very similar ways. In particular, if you have proved a theorem of that form, you can apply it to objects and hypotheses in the same way. We will use as an example the following statement that we will help you to prove a bit later:</p> <pre><code>theorem my_lemma : \u2200 x y \u03b5 : \u211d, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 :=\n  sorry\n\nsection\nvariable (a b \u03b4 : \u211d)\nvariable (h\u2080 : 0 &lt; \u03b4) (h\u2081 : \u03b4 \u2264 1)\nvariable (ha : |a| &lt; \u03b4) (hb : |b| &lt; \u03b4)\n\n#check my_lemma a b \u03b4\n#check my_lemma a b \u03b4 h\u2080 h\u2081\n#check my_lemma a b \u03b4 h\u2080 h\u2081 ha hb\n\nend\n</code></pre> <p>You have also already seen that it is common in Lean to use curly brackets to make quantified variables implicit when they can be inferred from subsequent hypotheses. When we do that, we can just apply a lemma to the hypotheses without mentioning the objects.</p> <pre><code>theorem my_lemma2 : \u2200 {x y \u03b5 : \u211d}, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 :=\n  sorry\n\nsection\nvariable (a b \u03b4 : \u211d)\nvariable (h\u2080 : 0 &lt; \u03b4) (h\u2081 : \u03b4 \u2264 1)\nvariable (ha : |a| &lt; \u03b4) (hb : |b| &lt; \u03b4)\n\n#check my_lemma2 h\u2080 h\u2081 ha hb\n\nend\n</code></pre> <p>At this stage, you also know that if you use the <code>apply</code> tactic to apply <code>my_lemma</code> to a goal of the form <code>|a * b| &lt; \u03b4</code>, you are left with new goals that require you to prove each of the hypotheses.</p> <p>To prove a statement like this, use the <code>intro</code> tactic. Take a look at what it does in this example:</p> <pre><code>theorem my_lemma3 :\n    \u2200 {x y \u03b5 : \u211d}, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 := by\n  intro x y \u03b5 epos ele1 xlt ylt\n  sorry\n</code></pre> <p>We can use any names we want for the universally quantified variables; they do not have to be <code>x</code>, <code>y</code>, and <code>\u03b5</code>. Notice that we have to introduce the variables even though they are marked implicit: making them implicit means that we leave them out when we write an expression using <code>my_lemma</code>, but they are still an essential part of the statement that we are proving. After the <code>intro</code> command, the goal is what it would have been at the start if we listed all the variables and hypotheses before the colon, as we did in the last section. In a moment, we will see why it is sometimes necessary to introduce variables and hypotheses after the proof begins.</p> <p>To help you prove the lemma, we will start you off:</p> <pre><code>theorem my_lemma4 :\n    \u2200 {x y \u03b5 : \u211d}, 0 &lt; \u03b5 \u2192 \u03b5 \u2264 1 \u2192 |x| &lt; \u03b5 \u2192 |y| &lt; \u03b5 \u2192 |x * y| &lt; \u03b5 := by\n  intro x y \u03b5 epos ele1 xlt ylt\n  calc\n    |x * y| = |x| * |y| := sorry\n    _ \u2264 |x| * \u03b5 := sorry\n    _ &lt; 1 * \u03b5 := sorry\n    _ = \u03b5 := sorry\n</code></pre> <p>Finish the proof using the theorems <code>abs_mul</code>, <code>mul_le_mul</code>, <code>abs_nonneg</code>, <code>mul_lt_mul_right</code>, and <code>one_mul</code>. Remember that you can find theorems like these using Ctrl-space completion (or Cmd-space completion on a Mac). Remember also that you can use <code>.mp</code> and <code>.mpr</code> or <code>.1</code> and <code>.2</code> to extract the two directions of an if-and-only-if statement.</p> <p>Universal quantifiers are often hidden in definitions, and Lean will unfold definitions to expose them when necessary. For example, let's define two predicates, <code>FnUb f a</code> and <code>FnLb f a</code>, where <code>f</code> is a function from the real numbers to the real numbers and <code>a</code> is a real number. The first says that <code>a</code> is an upper bound on the values of <code>f</code>, and the second says that <code>a</code> is a lower bound on the values of <code>f</code>.</p> <pre><code>def FnUb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, f x \u2264 a\n\ndef FnLb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, a \u2264 f x\n</code></pre> <p>In the next example, <code>fun x \u21a6 f x + g x</code> is the function that maps <code>x</code> to <code>f x + g x</code>. Going from the expression <code>f x + g x</code> to this function is called a lambda abstraction in type theory.</p> <pre><code>example (hfa : FnUb f a) (hgb : FnUb g b) : FnUb (fun x \u21a6 f x + g x) (a + b) := by\n  intro x\n  dsimp\n  apply add_le_add\n  apply hfa\n  apply hgb\n</code></pre> <p>Applying <code>intro</code> to the goal <code>FnUb (fun x \u21a6 f x + g x) (a + b)</code> forces Lean to unfold the definition of <code>FnUb</code> and introduce <code>x</code> for the universal quantifier. The goal is then <code>(fun (x : \u211d) \u21a6 f x + g x) x \u2264 a + b</code>. But applying <code>(fun x \u21a6 f x + g x)</code> to <code>x</code> should result in <code>f x + g x</code>, and the <code>dsimp</code> command performs that simplification. (The \"d\" stands for \"definitional.\") You can delete that command and the proof still works; Lean would have to perform that contraction anyhow to make sense of the next <code>apply</code>. The <code>dsimp</code> command simply makes the goal more readable and helps us figure out what to do next. Another option is to use the <code>change</code> tactic by writing <code>change f x + g x \u2264 a + b</code>. This helps make the proof more readable, and gives you more control over how the goal is transformed.</p> <p>The rest of the proof is routine. The last two <code>apply</code> commands force Lean to unfold the definitions of <code>FnUb</code> in the hypotheses. Try carrying out similar proofs of these:</p> <pre><code>example (hfa : FnLb f a) (hgb : FnLb g b) : FnLb (fun x \u21a6 f x + g x) (a + b) :=\n  sorry\n\nexample (nnf : FnLb f 0) (nng : FnLb g 0) : FnLb (fun x \u21a6 f x * g x) 0 :=\n  sorry\n\nexample (hfa : FnUb f a) (hgb : FnUb g b) (nng : FnLb g 0) (nna : 0 \u2264 a) :\n    FnUb (fun x \u21a6 f x * g x) (a * b) :=\n  sorry\n</code></pre> <p>Even though we have defined <code>FnUb</code> and <code>FnLb</code> for functions from the reals to the reals, you should recognize that the definitions and proofs are much more general. The definitions make sense for functions between any two types for which there is a notion of order on the codomain. Checking the type of the theorem <code>add_le_add</code> shows that it holds of any structure that is an \"ordered additive commutative monoid\"; the details of what that means don't matter now, but it is worth knowing that the natural numbers, integers, rationals, and real numbers are all instances. So if we prove the theorem <code>fnUb_add</code> at that level of generality, it will apply in all these instances.</p> <pre><code>variable {\u03b1 : Type*} {R : Type*} [OrderedCancelAddCommMonoid R]\n\n#check add_le_add\n\ndef FnUb' (f : \u03b1 \u2192 R) (a : R) : Prop :=\n  \u2200 x, f x \u2264 a\n\ntheorem fnUb_add {f g : \u03b1 \u2192 R} {a b : R} (hfa : FnUb' f a) (hgb : FnUb' g b) :\n    FnUb' (fun x \u21a6 f x + g x) (a + b) := fun x \u21a6 add_le_add (hfa x) (hgb x)\n</code></pre> <p>You have already seen square brackets like these in Section :numref:<code>proving_identities_in_algebraic_structures</code>, though we still haven't explained what they mean. For concreteness, we will stick to the real numbers for most of our examples, but it is worth knowing that Mathlib contains definitions and theorems that work at a high level of generality.</p> <p>For another example of a hidden universal quantifier, Mathlib defines a predicate <code>Monotone</code>, which says that a function is nondecreasing in its arguments:</p> <pre><code>example (f : \u211d \u2192 \u211d) (h : Monotone f) : \u2200 {a b}, a \u2264 b \u2192 f a \u2264 f b :=\n  @h\n</code></pre> <p>The property <code>Monotone f</code> is defined to be exactly the expression after the colon. We need to put the <code>@</code> symbol before <code>h</code> because if we don't, Lean expands the implicit arguments to <code>h</code> and inserts placeholders.</p> <p>Proving statements about monotonicity involves using <code>intro</code> to introduce two variables, say, <code>a</code> and <code>b</code>, and the hypothesis <code>a \u2264 b</code>. To use a monotonicity hypothesis, you can apply it to suitable arguments and hypotheses, and then apply the resulting expression to the goal. Or you can apply it to the goal and let Lean help you work backwards by displaying the remaining hypotheses as new subgoals.</p> <pre><code>example (mf : Monotone f) (mg : Monotone g) : Monotone fun x \u21a6 f x + g x := by\n  intro a b aleb\n  apply add_le_add\n  apply mf aleb\n  apply mg aleb\n</code></pre> <p>When a proof is this short, it is often convenient to give a proof term instead. To describe a proof that temporarily introduces objects <code>a</code> and <code>b</code> and a hypothesis <code>aleb</code>, Lean uses the notation <code>fun a b aleb \u21a6 ...</code>. This is analogous to the way that an expression like <code>fun x \u21a6 x^2</code> describes a function by temporarily naming an object, <code>x</code>, and then using it to describe a value. So the <code>intro</code> command in the previous proof corresponds to the lambda abstraction in the next proof term. The <code>apply</code> commands then correspond to building the application of the theorem to its arguments.</p> <pre><code>example (mf : Monotone f) (mg : Monotone g) : Monotone fun x \u21a6 f x + g x :=\n  fun a b aleb \u21a6 add_le_add (mf aleb) (mg aleb)\n</code></pre> <p>Here is a useful trick: if you start writing the proof term <code>fun a b aleb \u21a6 _</code> using an underscore where the rest of the expression should go, Lean will flag an error, indicating that it can't guess the value of that expression. If you check the Lean Goal window in VS Code or hover over the squiggly error marker, Lean will show you the goal that the remaining expression has to solve.</p> <p>Try proving these, with either tactics or proof terms:</p> <pre><code>example {c : \u211d} (mf : Monotone f) (nnc : 0 \u2264 c) : Monotone fun x \u21a6 c * f x :=\n  sorry\n\nexample (mf : Monotone f) (mg : Monotone g) : Monotone fun x \u21a6 f (g x) :=\n  sorry\n</code></pre> <p>Here are some more examples. A function :math:<code>f</code> from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) is said to be even if \\(f(-x) = f(x)\\) for every \\(x\\), and odd if \\(f(-x) = -f(x)\\) for every \\(x\\). The following example defines these two notions formally and establishes one fact about them. You can complete the proofs of the others.</p> <pre><code>def FnEven (f : \u211d \u2192 \u211d) : Prop :=\n  \u2200 x, f x = f (-x)\n\ndef FnOdd (f : \u211d \u2192 \u211d) : Prop :=\n  \u2200 x, f x = -f (-x)\n\nexample (ef : FnEven f) (eg : FnEven g) : FnEven fun x \u21a6 f x + g x := by\n  intro x\n  calc\n    (fun x \u21a6 f x + g x) x = f x + g x := rfl\n    _ = f (-x) + g (-x) := by rw [ef, eg]\n\n\nexample (of : FnOdd f) (og : FnOdd g) : FnEven fun x \u21a6 f x * g x := by\n  sorry\n\nexample (ef : FnEven f) (og : FnOdd g) : FnOdd fun x \u21a6 f x * g x := by\n  sorry\n\nexample (ef : FnEven f) (og : FnOdd g) : FnEven fun x \u21a6 f (g x) := by\n  sorry\n</code></pre> <p>The first proof can be shortened using <code>dsimp</code> or <code>change</code> to get rid of the lambda abstraction. But you can check that the subsequent <code>rw</code> won't work unless we get rid of the lambda abstraction explicitly, because otherwise it cannot find the patterns <code>f x</code> and <code>g x</code> in the expression. Contrary to some other tactics, <code>rw</code> operates on the syntactic level, it won't unfold definitions or apply reductions for you (it has a variant called <code>erw</code> that tries a little harder in this direction, but not much harder).</p> <p>You can find implicit universal quantifiers all over the place, once you know how to spot them.</p> <p>Mathlib includes a good library for manipulating sets. Recall that Lean does not use foundations based on set theory, so here the word set has its mundane meaning of a collection of mathematical objects of some given type <code>\u03b1</code>. If <code>x</code> has type <code>\u03b1</code> and <code>s</code> has type <code>Set \u03b1</code>, then <code>x \u2208 s</code> is a proposition that asserts that <code>x</code> is an element of <code>s</code>. If <code>y</code> has some different type <code>\u03b2</code> then the expression <code>y \u2208 s</code> makes no sense. Here \"makes no sense\" means \"has no type hence Lean does not accept it as a well-formed statement\". This contrasts with Zermelo-Fraenkel set theory for instance where <code>a \u2208 b</code> is a well-formed statement for every mathematical objects <code>a</code> and <code>b</code>. For instance <code>sin \u2208 cos</code> is a well-formed statement in ZF. This defect of set theoretic foundations is an important motivation for not using it in a proof assistant which is meant to assist us by detecting meaningless expressions. In Lean <code>sin</code> has type <code>\u211d \u2192 \u211d</code> and <code>cos</code> has type <code>\u211d \u2192 \u211d</code> which is not equal to <code>Set (\u211d \u2192 \u211d)</code>, even after unfolding definitions, so the statement <code>sin \u2208 cos</code> makes no sense. One can also use Lean to work on set theory itself. For instance the independence of the continuum hypothesis from the axioms of Zermelo-Fraenkel has been formalized in Lean. But such a meta-theory of set theory is completely beyond the scope of this book.</p> <p>If <code>s</code> and <code>t</code> are of type <code>Set \u03b1</code>, then the subset relation <code>s \u2286 t</code> is defined to mean <code>\u2200 {x : \u03b1}, x \u2208 s \u2192 x \u2208 t</code>. The variable in the quantifier is marked implicit so that given <code>h : s \u2286 t</code> and <code>h' : x \u2208 s</code>, we can write <code>h h'</code> as justification for <code>x \u2208 t</code>. The following example provides a tactic proof and a proof term justifying the reflexivity of the subset relation, and asks you to do the same for transitivity.</p> <pre><code>variable {\u03b1 : Type*} (r s t : Set \u03b1)\n\nexample : s \u2286 s := by\n  intro x xs\n  exact xs\n\ntheorem Subset.refl : s \u2286 s := fun x xs \u21a6 xs\n\ntheorem Subset.trans : r \u2286 s \u2192 s \u2286 t \u2192 r \u2286 t := by\n  sorry\n</code></pre> <p>Just as we defined <code>FnUb</code> for functions, we can define <code>SetUb s a</code> to mean that <code>a</code> is an upper bound on the set <code>s</code>, assuming <code>s</code> is a set of elements of some type that has an order associated with it. In the next example, we ask you to prove that if <code>a</code> is a bound on <code>s</code> and <code>a \u2264 b</code>, then <code>b</code> is a bound on <code>s</code> as well.</p> <pre><code>variable {\u03b1 : Type*} [PartialOrder \u03b1]\nvariable (s : Set \u03b1) (a b : \u03b1)\n\ndef SetUb (s : Set \u03b1) (a : \u03b1) :=\n  \u2200 x, x \u2208 s \u2192 x \u2264 a\n\nexample (h : SetUb s a) (h' : a \u2264 b) : SetUb s b :=\n  sorry\n</code></pre> <p>We close this section with one last important example. A function :math:<code>f</code> is said to be injective if for every :math:<code>x_1</code> and :math:<code>x_2</code>, if :math:<code>f(x_1) = f(x_2)</code> then :math:<code>x_1 = x_2</code>. Mathlib defines <code>Function.Injective f</code> with <code>x\u2081</code> and <code>x\u2082</code> implicit. The next example shows that, on the real numbers, any function that adds a constant is injective. We then ask you to show that multiplication by a nonzero constant is also injective, using the lemma name in the example as a source of inspiration. Recall you should use Ctrl-space completion after guessing the beginning of a lemma name.</p> <pre><code>open Function\n\nexample (c : \u211d) : Injective fun x \u21a6 x + c := by\n  intro x\u2081 x\u2082 h'\n  exact (add_left_inj c).mp h'\n\nexample {c : \u211d} (h : c \u2260 0) : Injective fun x \u21a6 c * x := by\n  sorry\n</code></pre> <p>Finally, show that the composition of two injective functions is injective:</p> <pre><code>variable {\u03b1 : Type*} {\u03b2 : Type*} {\u03b3 : Type*}\nvariable {g : \u03b2 \u2192 \u03b3} {f : \u03b1 \u2192 \u03b2}\n\nexample (injg : Injective g) (injf : Injective f) : Injective fun x \u21a6 g (f x) := by\n  sorry\n</code></pre>"},{"location":"en/C03_Logic/#the-existential-quantifier","title":"The Existential Quantifier","text":"<p>The existential quantifier, which can be entered as <code>\\ex</code> in VS Code, is used to represent the phrase \"there exists.\" The formal expression <code>\u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3</code> in Lean says that there is a real number between 2 and 3. (We will discuss the conjunction symbol, <code>\u2227</code>, in :numref:<code>conjunction_and_biimplication</code>.) The canonical way to prove such a statement is to exhibit a real number and show that it has the stated property. The number 2.5, which we can enter as <code>5 / 2</code> or <code>(5 : \u211d) / 2</code> when Lean cannot infer from context that we have the real numbers in mind, has the required property, and the <code>norm_num</code> tactic can prove that it meets the description.</p> <p>There are a few ways we can put the information together. Given a goal that begins with an existential quantifier, the <code>use</code> tactic is used to provide the object, leaving the goal of proving the property.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 := by\n  use 5 / 2\n  norm_num\n</code></pre> <p>You can give the <code>use</code> tactic proofs as well as data:</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 := by\n  have h1 : 2 &lt; (5 : \u211d) / 2 := by norm_num\n  have h2 : (5 : \u211d) / 2 &lt; 3 := by norm_num\n  use 5 / 2, h1, h2\n</code></pre> <p>In fact, the <code>use</code> tactic automatically tries to use available assumptions as well.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 := by\n  have h : 2 &lt; (5 : \u211d) / 2 \u2227 (5 : \u211d) / 2 &lt; 3 := by norm_num\n  use 5 / 2\n</code></pre> <p>Alternatively, we can use Lean's anonymous constructor notation to construct a proof of an existential quantifier.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 :=\n  have h : 2 &lt; (5 : \u211d) / 2 \u2227 (5 : \u211d) / 2 &lt; 3 := by norm_num\n  \u27e85 / 2, h\u27e9\n</code></pre> <p>Notice that there is no <code>by</code>; here we are giving an explicit proof term. The left and right angle brackets, which can be entered as <code>\\&lt;</code> and <code>\\&gt;</code> respectively, tell Lean to put together the given data using whatever construction is appropriate for the current goal. We can use the notation without going first into tactic mode:</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 3 :=\n  \u27e85 / 2, by norm_num\u27e9\n</code></pre> <p>So now we know how to prove an exists statement. But how do we use one? If we know that there exists an object with a certain property, we should be able to give a name to an arbitrary one and reason about it. For example, remember the predicates <code>FnUb f a</code> and <code>FnLb f a</code> from the last section, which say that <code>a</code> is an upper bound or lower bound on <code>f</code>, respectively. We can use the existential quantifier to say that \"<code>f</code> is bounded\" without specifying the bound:</p> <pre><code>def FnUb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, f x \u2264 a\n\ndef FnLb (f : \u211d \u2192 \u211d) (a : \u211d) : Prop :=\n  \u2200 x, a \u2264 f x\n\ndef FnHasUb (f : \u211d \u2192 \u211d) :=\n  \u2203 a, FnUb f a\n\ndef FnHasLb (f : \u211d \u2192 \u211d) :=\n  \u2203 a, FnLb f a\n</code></pre> <p>We can use the theorem <code>FnUb_add</code> from the last section to prove that if <code>f</code> and <code>g</code> have upper bounds, then so does <code>fun x \u21a6 f x + g x</code>.</p> <pre><code>variable {f g : \u211d \u2192 \u211d}\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  rcases ubf with \u27e8a, ubfa\u27e9\n  rcases ubg with \u27e8b, ubgb\u27e9\n  use a + b\n  apply fnUb_add ubfa ubgb\n</code></pre> <p>The <code>rcases</code> tactic unpacks the information in the existential quantifier. The annotations like <code>\u27e8a, ubfa\u27e9</code>, written with the same angle brackets as the anonymous constructors, are known as patterns, and they describe the information that we expect to find when we unpack the main argument. Given the hypothesis <code>ubf</code> that there is an upper bound for <code>f</code>, <code>rcases ubf with \u27e8a, ubfa\u27e9</code> adds a new variable <code>a</code> for an upper bound to the context, together with the hypothesis <code>ubfa</code> that it has the given property. The goal is left unchanged; what has changed is that we can now use the new object and the new hypothesis to prove the goal. This is a common method of reasoning in mathematics: we unpack objects whose existence is asserted or implied by some hypothesis, and then use it to establish the existence of something else.</p> <p>Try using this method to establish the following. You might find it useful to turn some of the examples from the last section into named theorems, as we did with <code>fn_ub_add</code>, or you can insert the arguments directly into the proofs.</p> <pre><code>example (lbf : FnHasLb f) (lbg : FnHasLb g) : FnHasLb fun x \u21a6 f x + g x := by\n  sorry\n\nexample {c : \u211d} (ubf : FnHasUb f) (h : c \u2265 0) : FnHasUb fun x \u21a6 c * f x := by\n  sorry\n</code></pre> <p>The \"r\" in <code>rcases</code> stands for \"recursive,\" because it allows us to use arbitrarily complex patterns to unpack nested data. The <code>rintro</code> tactic is a combination of <code>intro</code> and <code>rcases</code>:</p> <pre><code>example : FnHasUb f \u2192 FnHasUb g \u2192 FnHasUb fun x \u21a6 f x + g x := by\n  rintro \u27e8a, ubfa\u27e9 \u27e8b, ubgb\u27e9\n  exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>In fact, Lean also supports a pattern-matching fun in expressions and proof terms:</p> <pre><code>example : FnHasUb f \u2192 FnHasUb g \u2192 FnHasUb fun x \u21a6 f x + g x :=\n  fun \u27e8a, ubfa\u27e9 \u27e8b, ubgb\u27e9 \u21a6 \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>The task of unpacking information in a hypothesis is so important that Lean and Mathlib provide a number of ways to do it. For example, the <code>obtain</code> tactic provides suggestive syntax:</p> <pre><code>example (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  obtain \u27e8a, ubfa\u27e9 := ubf\n  obtain \u27e8b, ubgb\u27e9 := ubg\n  exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>Think of the first <code>obtain</code> instruction as matching the \"contents\" of <code>ubf</code> with the given pattern and assigning the components to the named variables. <code>rcases</code> and <code>obtain</code> are said to <code>destruct</code> their arguments, though there is a small difference in that <code>rcases</code> clears <code>ubf</code> from the context when it is done, whereas it is still present after <code>obtain</code>.</p> <p>Lean also supports syntax that is similar to that used in other functional programming languages:</p> <pre><code>example (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  cases ubf\n  case intro a ubfa =&gt;\n    cases ubg\n    case intro b ubgb =&gt;\n      exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  cases ubf\n  next a ubfa =&gt;\n    cases ubg\n    next b ubgb =&gt;\n      exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x := by\n  match ubf, ubg with\n    | \u27e8a, ubfa\u27e9, \u27e8b, ubgb\u27e9 =&gt;\n      exact \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n\nexample (ubf : FnHasUb f) (ubg : FnHasUb g) : FnHasUb fun x \u21a6 f x + g x :=\n  match ubf, ubg with\n    | \u27e8a, ubfa\u27e9, \u27e8b, ubgb\u27e9 =&gt;\n      \u27e8a + b, fnUb_add ubfa ubgb\u27e9\n</code></pre> <p>In the first example, if you put your cursor after <code>cases ubf</code>, you will see that the tactic produces a single goal, which Lean has tagged <code>intro</code>. (The particular name chosen comes from the internal name for the axiomatic primitive that builds a proof of an existential statement.) The <code>case</code> tactic then names the components. The second example is similar, except using <code>next</code> instead of <code>case</code> means that you can avoid mentioning <code>intro</code>. The word <code>match</code> in the last two examples highlights that what we are doing here is what computer scientists call \"pattern matching.\" Notice that the third proof begins by <code>by</code>, after which the tactic version of <code>match</code> expects a tactic proof on the right side of the arrow. The last example is a proof term: there are no tactics in sight.</p> <p>For the rest of this book, we will stick to <code>rcases</code>, <code>rintro</code>, and <code>obtain</code>, as the preferred ways of using an existential quantifier. But it can't hurt to see the alternative syntax, especially if there is a chance you will find yourself in the company of computer scientists.</p> <p>To illustrate one way that <code>rcases</code> can be used, we prove an old mathematical chestnut: if two integers <code>x</code> and <code>y</code> can each be written as a sum of two squares, then so can their product, <code>x * y</code>. In fact, the statement is true for any commutative ring, not just the integers. In the next example, <code>rcases</code> unpacks two existential quantifiers at once. We then provide the magic values needed to express <code>x * y</code> as a sum of squares as a list to the <code>use</code> statement, and we use <code>ring</code> to verify that they work.</p> <pre><code>variable {\u03b1 : Type*} [CommRing \u03b1]\n\ndef SumOfSquares (x : \u03b1) :=\n  \u2203 a b, x = a ^ 2 + b ^ 2\n\ntheorem sumOfSquares_mul {x y : \u03b1} (sosx : SumOfSquares x) (sosy : SumOfSquares y) :\n    SumOfSquares (x * y) := by\n  rcases sosx with \u27e8a, b, xeq\u27e9\n  rcases sosy with \u27e8c, d, yeq\u27e9\n  rw [xeq, yeq]\n  use a * c - b * d, a * d + b * c\n  ring\n</code></pre> <p>This proof doesn't provide much insight, but here is one way to motivate it. A Gaussian integer is a number of the form \\(a + bi\\) where \\(a\\) and \\(b\\) are integers and \\(i = \\sqrt{-1}\\). The norm of the Gaussian integer \\(a + bi\\) is, by definition, \\(a^2 + b^2\\). So the norm of a Gaussian integer is a sum of squares, and any sum of squares can be expressed in this way. The theorem above reflects the fact that norm of a product of Gaussian integers is the product of their norms: if \\(x\\) is the norm of \\(a + bi\\) and \\(y\\) in the norm of \\(c + di\\), then \\(xy\\) is the norm of \\((a + bi) (c + di)\\). Our cryptic proof illustrates the fact that the proof that is easiest to formalize isn't always the most perspicuous one. In :numref:<code>section_building_the_gaussian_integers</code>, we will provide you with the means to define the Gaussian integers and use them to provide an alternative proof.</p> <p>The pattern of unpacking an equation inside an existential quantifier and then using it to rewrite an expression in the goal comes up often, so much so that the <code>rcases</code> tactic provides an abbreviation: if you use the keyword <code>rfl</code> in place of a new identifier, <code>rcases</code> does the rewriting automatically (this trick doesn't work with pattern-matching lambdas).</p> <pre><code>theorem sumOfSquares_mul' {x y : \u03b1} (sosx : SumOfSquares x) (sosy : SumOfSquares y) :\n    SumOfSquares (x * y) := by\n  rcases sosx with \u27e8a, b, rfl\u27e9\n  rcases sosy with \u27e8c, d, rfl\u27e9\n  use a * c - b * d, a * d + b * c\n  ring\n</code></pre> <p>As with the universal quantifier, you can find existential quantifiers hidden all over if you know how to spot them. For example, divisibility is implicitly an \"exists\" statement.</p> <pre><code>example (divab : a \u2223 b) (divbc : b \u2223 c) : a \u2223 c := by\n  rcases divab with \u27e8d, beq\u27e9\n  rcases divbc with \u27e8e, ceq\u27e9\n  rw [ceq, beq]\n  use d * e; ring\n</code></pre> <p>And once again, this provides a nice setting for using <code>rcases</code> with <code>rfl</code>. Try it out in the proof above. It feels pretty good!</p> <p>Then try proving the following:</p> <pre><code>example (divab : a \u2223 b) (divac : a \u2223 c) : a \u2223 b + c := by\n  sorry\n</code></pre> <p>For another important example, a function \\(f : \\alpha \\to \\beta\\) is said to be surjective if for every \\(y\\) in the codomain, \\(\\beta\\), there is an \\(x\\) in the domain, \\(\\alpha\\), such that $f(x) = $<code>. Notice that this statement includes both a universal and an existential quantifier, which explains why the next example makes use of both</code>intro<code>and</code>use`.</p> <pre><code>example {c : \u211d} : Surjective fun x \u21a6 x + c := by\n  intro x\n  use x - c\n  dsimp; ring\n</code></pre> <p>Try this example yourself using the theorem <code>mul_div_cancel\u2080</code>.:</p> <pre><code>example {c : \u211d} (h : c \u2260 0) : Surjective fun x \u21a6 c * x := by\n  sorry\n</code></pre> <p>At this point, it is worth mentioning that there is a tactic, <code>field_simp</code>, that will often clear denominators in a useful way. It can be used in conjunction with the <code>ring</code> tactic.</p> <pre><code>example (x y : \u211d) (h : x - y \u2260 0) : (x ^ 2 - y ^ 2) / (x - y) = x + y := by\n  field_simp [h]\n  ring\n</code></pre> <p>The next example uses a surjectivity hypothesis by applying it to a suitable value. Note that you can use <code>rcases</code> with any expression, not just a hypothesis.</p> <pre><code>example {f : \u211d \u2192 \u211d} (h : Surjective f) : \u2203 x, f x ^ 2 = 4 := by\n  rcases h 2 with \u27e8x, hx\u27e9\n  use x\n  rw [hx]\n  norm_num\n</code></pre> <p>See if you can use these methods to show that the composition of surjective functions is surjective.</p> <pre><code>variable {\u03b1 : Type*} {\u03b2 : Type*} {\u03b3 : Type*}\nvariable {g : \u03b2 \u2192 \u03b3} {f : \u03b1 \u2192 \u03b2}\n\nexample (surjg : Surjective g) (surjf : Surjective f) : Surjective fun x \u21a6 g (f x) := by\n  sorry\n</code></pre>"},{"location":"en/C03_Logic/#negation","title":"Negation","text":"<p>The symbol <code>\u00ac</code> is meant to express negation, so <code>\u00ac x &lt; y</code> says that <code>x</code> is not less than <code>y</code>, <code>\u00ac x = y</code> (or, equivalently, <code>x \u2260 y</code>) says that <code>x</code> is not equal to <code>y</code>, and <code>\u00ac \u2203 z, x &lt; z \u2227 z &lt; y</code> says that there does not exist a <code>z</code> strictly between <code>x</code> and <code>y</code>. In Lean, the notation <code>\u00ac A</code> abbreviates <code>A \u2192 False</code>, which you can think of as saying that <code>A</code> implies a contradiction. Practically speaking, this means that you already know something about how to work with negations: you can prove <code>\u00ac A</code> by introducing a hypothesis <code>h : A</code> and proving <code>False</code>, and if you have <code>h : \u00ac A</code> and <code>h' : A</code>, then applying <code>h</code> to <code>h'</code> yields <code>False</code>.</p> <p>To illustrate, consider the irreflexivity principle <code>lt_irrefl</code> for a strict order, which says that we have <code>\u00ac a &lt; a</code> for every <code>a</code>. The asymmetry principle <code>lt_asymm</code> says that we have <code>a &lt; b \u2192 \u00ac b &lt; a</code>. Let's show that <code>lt_asymm</code> follows from <code>lt_irrefl</code>. section</p> <pre><code>example (h : a &lt; b) : \u00acb &lt; a := by\n  intro h'\n  have : a &lt; a := lt_trans h h'\n  apply lt_irrefl a this\n</code></pre> <p>This example introduces a couple of new tricks. First, when you use <code>have</code> without providing a label, Lean uses the name <code>this</code>, providing a convenient way to refer back to it. Because the proof is so short, we provide an explicit proof term. But what you should really be paying attention to in this proof is the result of the <code>intro</code> tactic, which leaves a goal of <code>False</code>, and the fact that we eventually prove <code>False</code> by applying <code>lt_irrefl</code> to a proof of <code>a &lt; a</code>.</p> <p>Here is another example, which uses the predicate <code>FnHasUb</code> defined in the last section, which says that a function has an upper bound.</p> <pre><code>example (h : \u2200 a, \u2203 x, f x &gt; a) : \u00acFnHasUb f := by\n  intro fnub\n  rcases fnub with \u27e8a, fnuba\u27e9\n  rcases h a with \u27e8x, hx\u27e9\n  have : f x \u2264 a := fnuba x\n  linarith\n</code></pre> <p>Remember that it is often convenient to use <code>linarith</code> when a goal follows from linear equations and inequalities that are in the context.</p> <p>See if you can prove these in a similar way:</p> <pre><code>example (h : \u2200 a, \u2203 x, f x &lt; a) : \u00acFnHasLb f :=\n  sorry\n\nexample : \u00acFnHasUb fun x \u21a6 x :=\n  sorry\n</code></pre> <p>Mathlib offers a number of useful theorems for relating orders and negations:</p> <pre><code>#check (not_le_of_gt : a &gt; b \u2192 \u00aca \u2264 b)\n#check (not_lt_of_ge : a \u2265 b \u2192 \u00aca &lt; b)\n#check (lt_of_not_ge : \u00aca \u2265 b \u2192 a &lt; b)\n#check (le_of_not_gt : \u00aca &gt; b \u2192 a \u2264 b)\n</code></pre> <p>Recall the predicate <code>Monotone f</code>, which says that <code>f</code> is nondecreasing. Use some of the theorems just enumerated to prove the following:</p> <pre><code>example (h : Monotone f) (h' : f a &lt; f b) : a &lt; b := by\n  sorry\n\nexample (h : a \u2264 b) (h' : f b &lt; f a) : \u00acMonotone f := by\n  sorry\n</code></pre> <p>We can show that the first example in the last snippet cannot be proved if we replace <code>&lt;</code> by <code>\u2264</code>. Notice that we can prove the negation of a universally quantified statement by giving a counterexample. Complete the proof.</p> <pre><code>example : \u00ac\u2200 {f : \u211d \u2192 \u211d}, Monotone f \u2192 \u2200 {a b}, f a \u2264 f b \u2192 a \u2264 b := by\n  intro h\n  let f := fun x : \u211d \u21a6 (0 : \u211d)\n  have monof : Monotone f := by sorry\n  have h' : f 1 \u2264 f 0 := le_refl _\n  sorry\n</code></pre> <p>This example introduces the <code>let</code> tactic, which adds a local definition to the context. If you put the cursor after the <code>let</code> command, in the goal window you will see that the definition <code>f : \u211d \u2192 \u211d := fun x \u21a6 0</code> has been added to the context. Lean will unfold the definition of <code>f</code> when it has to. In particular, when we prove <code>f 1 \u2264 f 0</code> with <code>le_refl</code>, Lean reduces <code>f 1</code> and <code>f 0</code> to <code>0</code>.</p> <p>Use <code>le_of_not_gt</code> to prove the following:</p> <pre><code>example (x : \u211d) (h : \u2200 \u03b5 &gt; 0, x &lt; \u03b5) : x \u2264 0 := by\n  sorry\n</code></pre> <p>Implicit in many of the proofs we have just done is the fact that if <code>P</code> is any property, saying that there is nothing with property <code>P</code> is the same as saying that everything fails to have property <code>P</code>, and saying that not everything has property <code>P</code> is equivalent to saying that something fails to have property <code>P</code>. In other words, all four of the following implications are valid (but one of them cannot be proved with what we explained so far):</p> <pre><code>variable {\u03b1 : Type*} (P : \u03b1 \u2192 Prop) (Q : Prop)\n\nexample (h : \u00ac\u2203 x, P x) : \u2200 x, \u00acP x := by\n  sorry\n\nexample (h : \u2200 x, \u00acP x) : \u00ac\u2203 x, P x := by\n  sorry\n\nexample (h : \u00ac\u2200 x, P x) : \u2203 x, \u00acP x := by\n  sorry\n\nexample (h : \u2203 x, \u00acP x) : \u00ac\u2200 x, P x := by\n  sorry\n</code></pre> <p>The first, second, and fourth are straightforward to prove using the methods you have already seen. We encourage you to try it. The third is more difficult, however, because it concludes that an object exists from the fact that its nonexistence is contradictory. This is an instance of classical mathematical reasoning. We can use proof by contradiction to prove the third implication as follows.</p> <pre><code>example (h : \u00ac\u2200 x, P x) : \u2203 x, \u00acP x := by\n  by_contra h'\n  apply h\n  intro x\n  show P x\n  by_contra h''\n  exact h' \u27e8x, h''\u27e9\n</code></pre> <p>Make sure you understand how this works. The <code>by_contra</code> tactic allows us to prove a goal <code>Q</code> by assuming <code>\u00ac Q</code> and deriving a contradiction. In fact, it is equivalent to using the equivalence <code>not_not : \u00ac \u00ac Q \u2194 Q</code>. Confirm that you can prove the forward direction of this equivalence using <code>by_contra</code>, while the reverse direction follows from the ordinary rules for negation.</p> <pre><code>example (h : \u00ac\u00acQ) : Q := by\n  sorry\n\nexample (h : Q) : \u00ac\u00acQ := by\n  sorry\n</code></pre> <p>Use proof by contradiction to establish the following, which is the converse of one of the implications we proved above. (Hint: use <code>intro</code> first.)</p> <pre><code>example (h : \u00acFnHasUb f) : \u2200 a, \u2203 x, f x &gt; a := by\n  sorry\n</code></pre> <p>It is often tedious to work with compound statements with a negation in front, and it is a common mathematical pattern to replace such statements with equivalent forms in which the negation has been pushed inward. To facilitate this, Mathlib offers a <code>push_neg</code> tactic, which restates the goal in this way. The command <code>push_neg at h</code> restates the hypothesis <code>h</code>.</p> <pre><code>example (h : \u00ac\u2200 a, \u2203 x, f x &gt; a) : FnHasUb f := by\n  push_neg at h\n  exact h\n\nexample (h : \u00acFnHasUb f) : \u2200 a, \u2203 x, f x &gt; a := by\n  dsimp only [FnHasUb, FnUb] at h\n  push_neg at h\n  exact h\n</code></pre> <p>In the second example, we use dsimp to expand the definitions of <code>FnHasUb</code> and <code>FnUb</code>. (We need to use <code>dsimp</code> rather than <code>rw</code> to expand <code>FnUb</code>, because it appears in the scope of a quantifier.) You can verify that in the examples above with <code>\u00ac\u2203 x, P x</code> and <code>\u00ac\u2200 x, P x</code>, the <code>push_neg</code> tactic does the expected thing. Without even knowing how to use the conjunction symbol, you should be able to use <code>push_neg</code> to prove the following:</p> <pre><code>example (h : \u00acMonotone f) : \u2203 x y, x \u2264 y \u2227 f y &lt; f x := by\n  sorry\n</code></pre> <p>Mathlib also has a tactic, <code>contrapose</code>, which transforms a goal <code>A \u2192 B</code> to <code>\u00acB \u2192 \u00acA</code>. Similarly, given a goal of proving <code>B</code> from hypothesis <code>h : A</code>, <code>contrapose h</code> leaves you with a goal of proving <code>\u00acA</code> from hypothesis <code>\u00acB</code>. Using <code>contrapose!</code> instead of <code>contrapose</code> applies <code>push_neg</code> to the goal and the relevant hypothesis as well.</p> <pre><code>example (h : \u00acFnHasUb f) : \u2200 a, \u2203 x, f x &gt; a := by\n  contrapose! h\n  exact h\n\nexample (x : \u211d) (h : \u2200 \u03b5 &gt; 0, x \u2264 \u03b5) : x \u2264 0 := by\n  contrapose! h\n  use x / 2\n  constructor &lt;;&gt; linarith\n</code></pre> <p>We have not yet explained the <code>constructor</code> command or the use of the semicolon after it, but we will do that in the next section.</p> <p>We close this section with the principle of ex falso, which says that anything follows from a contradiction. In Lean, this is represented by <code>False.elim</code>, which establishes <code>False \u2192 P</code> for any proposition <code>P</code>. This may seem like a strange principle, but it comes up fairly often. We often prove a theorem by splitting on cases, and sometimes we can show that one of the cases is contradictory. In that case, we need to assert that the contradiction establishes the goal so we can move on to the next one. (We will see instances of reasoning by cases in :numref:<code>disjunction</code>.)</p> <p>Lean provides a number of ways of closing a goal once a contradiction has been reached.</p> <pre><code>example (h : 0 &lt; 0) : a &gt; 37 := by\n  exfalso\n  apply lt_irrefl 0 h\n\nexample (h : 0 &lt; 0) : a &gt; 37 :=\n  absurd h (lt_irrefl 0)\n\nexample (h : 0 &lt; 0) : a &gt; 37 := by\n  have h' : \u00ac0 &lt; 0 := lt_irrefl 0\n  contradiction\n</code></pre> <p>The <code>exfalso</code> tactic replaces the current goal with the goal of proving <code>False</code>. Given <code>h : P</code> and <code>h' : \u00ac P</code>, the term <code>absurd h h'</code> establishes any proposition. Finally, the <code>contradiction</code> tactic tries to close a goal by finding a contradiction in the hypotheses, such as a pair of the form <code>h : P</code> and <code>h' : \u00ac P</code>. Of course, in this example, <code>linarith</code> also works.</p>"},{"location":"en/C03_Logic/#conjunction-and-iff","title":"Conjunction and Iff","text":"<p>You have already seen that the conjunction symbol, <code>\u2227</code>, is used to express \"and.\" The <code>constructor</code> tactic allows you to prove a statement of the form <code>A \u2227 B</code> by proving <code>A</code> and then proving <code>B</code>.</p> <pre><code>example {x y : \u211d} (h\u2080 : x \u2264 y) (h\u2081 : \u00acy \u2264 x) : x \u2264 y \u2227 x \u2260 y := by\n  constructor\n  \u00b7 assumption\n  intro h\n  apply h\u2081\n  rw [h]\n</code></pre> <p>In this example, the <code>assumption</code> tactic tells Lean to find an assumption that will solve the goal. Notice that the final <code>rw</code> finishes the goal by applying the reflexivity of <code>\u2264</code>. The following are alternative ways of carrying out the previous examples using the anonymous constructor angle brackets. The first is a slick proof-term version of the previous proof, which drops into tactic mode at the keyword <code>by</code>.</p> <pre><code>example {x y : \u211d} (h\u2080 : x \u2264 y) (h\u2081 : \u00acy \u2264 x) : x \u2264 y \u2227 x \u2260 y :=\n  \u27e8h\u2080, fun h \u21a6 h\u2081 (by rw [h])\u27e9\n\nexample {x y : \u211d} (h\u2080 : x \u2264 y) (h\u2081 : \u00acy \u2264 x) : x \u2264 y \u2227 x \u2260 y :=\n  have h : x \u2260 y := by\n    contrapose! h\u2081\n    rw [h\u2081]\n  \u27e8h\u2080, h\u27e9\n</code></pre> <p>Using a conjunction instead of proving one involves unpacking the proofs of the two parts. You can use the <code>rcases</code> tactic for that, as well as <code>rintro</code> or a pattern-matching <code>fun</code>, all in a manner similar to the way they are used with the existential quantifier.</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  rcases h with \u27e8h\u2080, h\u2081\u27e9\n  contrapose! h\u2081\n  exact le_antisymm h\u2080 h\u2081\n\nexample {x y : \u211d} : x \u2264 y \u2227 x \u2260 y \u2192 \u00acy \u2264 x := by\n  rintro \u27e8h\u2080, h\u2081\u27e9 h'\n  exact h\u2081 (le_antisymm h\u2080 h')\n\nexample {x y : \u211d} : x \u2264 y \u2227 x \u2260 y \u2192 \u00acy \u2264 x :=\n  fun \u27e8h\u2080, h\u2081\u27e9 h' \u21a6 h\u2081 (le_antisymm h\u2080 h')\n</code></pre> <p>In analogy to the <code>obtain</code> tactic, there is also a pattern-matching <code>have</code>:</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  have \u27e8h\u2080, h\u2081\u27e9 := h\n  contrapose! h\u2081\n  exact le_antisymm h\u2080 h\u2081\n</code></pre> <p>In contrast to <code>rcases</code>, here the <code>have</code> tactic leaves <code>h</code> in the context. And even though we won't use them, once again we have the computer scientists' pattern-matching syntax:</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  cases h\n  case intro h\u2080 h\u2081 =&gt;\n    contrapose! h\u2081\n    exact le_antisymm h\u2080 h\u2081\n\nexample {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  cases h\n  next h\u2080 h\u2081 =&gt;\n    contrapose! h\u2081\n    exact le_antisymm h\u2080 h\u2081\n\nexample {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  match h with\n    | \u27e8h\u2080, h\u2081\u27e9 =&gt;\n        contrapose! h\u2081\n        exact le_antisymm h\u2080 h\u2081\n</code></pre> <p>In contrast to using an existential quantifier, you can also extract proofs of the two components of a hypothesis <code>h : A \u2227 B</code> by writing <code>h.left</code> and <code>h.right</code>, or, equivalently, <code>h.1</code> and <code>h.2</code>.</p> <pre><code>example {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x := by\n  intro h'\n  apply h.right\n  exact le_antisymm h.left h'\n\nexample {x y : \u211d} (h : x \u2264 y \u2227 x \u2260 y) : \u00acy \u2264 x :=\n  fun h' \u21a6 h.right (le_antisymm h.left h')\n</code></pre> <p>Try using these techniques to come up with various ways of proving of the following:</p> <pre><code>example {m n : \u2115} (h : m \u2223 n \u2227 m \u2260 n) : m \u2223 n \u2227 \u00acn \u2223 m :=\n  sorry\n</code></pre> <p>You can nest uses of <code>\u2203</code> and <code>\u2227</code> with anonymous constructors, <code>rintro</code>, and <code>rcases</code>.</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 4 :=\n  \u27e85 / 2, by norm_num, by norm_num\u27e9\n\nexample (x y : \u211d) : (\u2203 z : \u211d, x &lt; z \u2227 z &lt; y) \u2192 x &lt; y := by\n  rintro \u27e8z, xltz, zlty\u27e9\n  exact lt_trans xltz zlty\n\nexample (x y : \u211d) : (\u2203 z : \u211d, x &lt; z \u2227 z &lt; y) \u2192 x &lt; y :=\n  fun \u27e8z, xltz, zlty\u27e9 \u21a6 lt_trans xltz zlty\n</code></pre> <p>You can also use the <code>use</code> tactic:</p> <pre><code>example : \u2203 x : \u211d, 2 &lt; x \u2227 x &lt; 4 := by\n  use 5 / 2\n  constructor &lt;;&gt; norm_num\n\nexample : \u2203 m n : \u2115, 4 &lt; m \u2227 m &lt; n \u2227 n &lt; 10 \u2227 Nat.Prime m \u2227 Nat.Prime n := by\n  use 5\n  use 7\n  norm_num\n\nexample {x y : \u211d} : x \u2264 y \u2227 x \u2260 y \u2192 x \u2264 y \u2227 \u00acy \u2264 x := by\n  rintro \u27e8h\u2080, h\u2081\u27e9\n  use h\u2080\n  exact fun h' \u21a6 h\u2081 (le_antisymm h\u2080 h')\n</code></pre> <p>In the first example, the semicolon after the <code>constructor</code> command tells Lean to use the <code>norm_num</code> tactic on both of the goals that result.</p> <p>In Lean, <code>A \u2194 B</code> is not defined to be <code>(A \u2192 B) \u2227 (B \u2192 A)</code>, but it could have been, and it behaves roughly the same way. You have already seen that you can write <code>h.mp</code> and <code>h.mpr</code> or <code>h.1</code> and <code>h.2</code> for the two directions of <code>h : A \u2194 B</code>. You can also use <code>cases</code> and friends. To prove an if-and-only-if statement, you can use <code>constructor</code> or angle brackets, just as you would if you were proving a conjunction.</p> <pre><code>example {x y : \u211d} (h : x \u2264 y) : \u00acy \u2264 x \u2194 x \u2260 y := by\n  constructor\n  \u00b7 contrapose!\n    rintro rfl\n    rfl\n  contrapose!\n  exact le_antisymm h\n\nexample {x y : \u211d} (h : x \u2264 y) : \u00acy \u2264 x \u2194 x \u2260 y :=\n  \u27e8fun h\u2080 h\u2081 \u21a6 h\u2080 (by rw [h\u2081]), fun h\u2080 h\u2081 \u21a6 h\u2080 (le_antisymm h h\u2081)\u27e9\n</code></pre> <p>The last proof term is inscrutable. Remember that you can use underscores while writing an expression like that to see what Lean expects.</p> <p>Try out the various techniques and gadgets you have just seen in order to prove the following:</p> <pre><code>example {x y : \u211d} : x \u2264 y \u2227 \u00acy \u2264 x \u2194 x \u2264 y \u2227 x \u2260 y :=\n  sorry\n</code></pre> <p>For a more interesting exercise, show that for any two real numbers <code>x</code> and <code>y</code>, <code>x^2 + y^2 = 0</code> if and only if <code>x = 0</code> and <code>y = 0</code>. We suggest proving an auxiliary lemma using <code>linarith</code>, <code>pow_two_nonneg</code>, and <code>pow_eq_zero</code>.</p> <pre><code>theorem aux {x y : \u211d} (h : x ^ 2 + y ^ 2 = 0) : x = 0 :=\n  have h' : x ^ 2 = 0 := by sorry\n  pow_eq_zero h'\n\nexample (x y : \u211d) : x ^ 2 + y ^ 2 = 0 \u2194 x = 0 \u2227 y = 0 :=\n  sorry\n</code></pre> <p>In Lean, bi-implication leads a double-life. You can treat it like a conjunction and use its two parts separately. But Lean also knows that it is a reflexive, symmetric, and transitive relation between propositions, and you can also use it with <code>calc</code> and <code>rw</code>. It is often convenient to rewrite a statement to an equivalent one. In the next example, we use <code>abs_lt</code> to replace an expression of the form <code>|x| &lt; y</code> by the equivalent expression <code>- y &lt; x \u2227 x &lt; y</code>, and in the one after that we use <code>Nat.dvd_gcd_iff</code> to replace an expression of the form <code>m \u2223 Nat.gcd n k</code> by the equivalent expression <code>m \u2223 n \u2227 m \u2223 k</code>.</p> <pre><code>example (x : \u211d) : |x + 3| &lt; 5 \u2192 -8 &lt; x \u2227 x &lt; 2 := by\n  rw [abs_lt]\n  intro h\n  constructor &lt;;&gt; linarith\n\nexample : 3 \u2223 Nat.gcd 6 15 := by\n  rw [Nat.dvd_gcd_iff]\n  constructor &lt;;&gt; norm_num\n</code></pre> <p>See if you can use <code>rw</code> with the theorem below to provide a short proof that negation is not a nondecreasing function. (Note that <code>push_neg</code> won't unfold definitions for you, so the <code>rw [Monotone]</code> in the proof of the theorem is needed.)</p> <pre><code>theorem not_monotone_iff {f : \u211d \u2192 \u211d} : \u00acMonotone f \u2194 \u2203 x y, x \u2264 y \u2227 f x &gt; f y := by\n  rw [Monotone]\n  push_neg\n  rfl\n\nexample : \u00acMonotone fun x : \u211d \u21a6 -x := by\n  sorry\n</code></pre> <p>The remaining exercises in this section are designed to give you some more practice with conjunction and bi-implication. Remember that a partial order is a binary relation that is transitive, reflexive, and antisymmetric. An even weaker notion sometimes arises: a preorder is just a reflexive, transitive relation. For any pre-order <code>\u2264</code>, Lean axiomatizes the associated strict pre-order by <code>a &lt; b \u2194 a \u2264 b \u2227 \u00ac b \u2264 a</code>. Show that if <code>\u2264</code> is a partial order, then <code>a &lt; b</code> is equivalent to <code>a \u2264 b \u2227 a \u2260 b</code>:</p> <pre><code>variable {\u03b1 : Type*} [PartialOrder \u03b1]\nvariable (a b : \u03b1)\n\nexample : a &lt; b \u2194 a \u2264 b \u2227 a \u2260 b := by\n  rw [lt_iff_le_not_le]\n  sorry\n</code></pre> <p>Beyond logical operations, you do not need anything more than <code>le_refl</code> and <code>le_trans</code>. Show that even in the case where <code>\u2264</code> is only assumed to be a preorder, we can prove that the strict order is irreflexive and transitive. In the second example, for convenience, we use the simplifier rather than <code>rw</code> to express <code>&lt;</code> in terms of <code>\u2264</code> and <code>\u00ac</code>. We will come back to the simplifier later, but here we are only relying on the fact that it will use the indicated lemma repeatedly, even if it needs to be instantiated to different values.</p> <pre><code>variable {\u03b1 : Type*} [Preorder \u03b1]\nvariable (a b c : \u03b1)\n\n-- EXAMPLES:\nexample : \u00aca &lt; a := by\n  rw [lt_iff_le_not_le]\n  sorry\n\nexample : a &lt; b \u2192 b &lt; c \u2192 a &lt; c := by\n  simp only [lt_iff_le_not_le]\n  sorry\n</code></pre>"},{"location":"en/C03_Logic/#disjunction","title":"Disjunction","text":"<p>The canonical way to prove a disjunction <code>A \u2228 B</code> is to prove <code>A</code> or to prove <code>B</code>. The <code>left</code> tactic chooses <code>A</code>, and the <code>right</code> tactic chooses <code>B</code>.</p> <pre><code>variable {x y : \u211d}\n\nexample (h : y &gt; x ^ 2) : y &gt; 0 \u2228 y &lt; -1 := by\n  left\n  linarith [pow_two_nonneg x]\n\nexample (h : -y &gt; x ^ 2 + 1) : y &gt; 0 \u2228 y &lt; -1 := by\n  right\n  linarith [pow_two_nonneg x]\n</code></pre> <p>We cannot use an anonymous constructor to construct a proof of an \"or\" because Lean would have to guess which disjunct we are trying to prove. When we write proof terms we can use <code>Or.inl</code> and <code>Or.inr</code> instead to make the choice explicitly. Here, <code>inl</code> is short for \"introduction left\" and <code>inr</code> is short for \"introduction right.\"</p> <pre><code>example (h : y &gt; 0) : y &gt; 0 \u2228 y &lt; -1 :=\n  Or.inl h\n\nexample (h : y &lt; -1) : y &gt; 0 \u2228 y &lt; -1 :=\n  Or.inr h\n</code></pre> <p>It may seem strange to prove a disjunction by proving one side or the other. In practice, which case holds usually depends on a case distinction that is implicit or explicit in the assumptions and the data. The <code>rcases</code> tactic allows us to make use of a hypothesis of the form <code>A \u2228 B</code>. In contrast to the use of <code>rcases</code> with conjunction or an existential quantifier, here the <code>rcases</code> tactic produces two goals. Both have the same conclusion, but in the first case, <code>A</code> is assumed to be true, and in the second case, <code>B</code> is assumed to be true. In other words, as the name suggests, the <code>rcases</code> tactic carries out a proof by cases. As usual, we can tell Lean what names to use for the hypotheses. In the next example, we tell Lean to use the name <code>h</code> on each branch.</p> <pre><code>example : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  rcases le_or_gt 0 y with h | h\n  \u00b7 rw [abs_of_nonneg h]\n    intro h; left; exact h\n  . rw [abs_of_neg h]\n    intro h; right; exact h\n</code></pre> <p>Notice that the pattern changes from <code>\u27e8h\u2080, h\u2081\u27e9</code> in the case of a conjunction to <code>h\u2080 | h\u2081</code> in the case of a disjunction. Think of the first pattern as matching against data the contains both an <code>h\u2080</code> and a <code>h\u2081</code>, whereas second pattern, with the bar, matches against data that contains either an <code>h\u2080</code> or <code>h\u2081</code>. In this case, because the two goals are separate, we have chosen to use the same name, <code>h</code>, in each case.</p> <p>The absolute value function is defined in such a way that we can immediately prove that <code>x \u2265 0</code> implies <code>|x| = x</code> (this is the theorem <code>abs_of_nonneg</code>) and <code>x &lt; 0</code> implies <code>|x| = -x</code> (this is <code>abs_of_neg</code>). The expression <code>le_or_gt 0 x</code> establishes <code>0 \u2264 x \u2228 x &lt; 0</code>, allowing us to split on those two cases.</p> <p>Lean also supports the computer scientists' pattern-matching syntax for disjunction. Now the <code>cases</code> tactic is more attractive, because it allows us to name each <code>case</code>, and name the hypothesis that is introduced closer to where it is used.</p> <pre><code>example : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  cases le_or_gt 0 y\n  case inl h =&gt;\n    rw [abs_of_nonneg h]\n    intro h; left; exact h\n  case inr h =&gt;\n    rw [abs_of_neg h]\n    intro h; right; exact h\n</code></pre> <p>The names <code>inl</code> and <code>inr</code> are short for \"intro left\" and \"intro right,\" respectively. Using <code>case</code> has the advantage that you can prove the cases in either order; Lean uses the tag to find the relevant goal. If you don't care about that, you can use <code>next</code>, or <code>match</code>, or even a pattern-matching <code>have</code>.</p> <pre><code>example : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  cases le_or_gt 0 y\n  next h =&gt;\n    rw [abs_of_nonneg h]\n    intro h; left; exact h\n  next h =&gt;\n    rw [abs_of_neg h]\n    intro h; right; exact h\n\nexample : x &lt; |y| \u2192 x &lt; y \u2228 x &lt; -y := by\n  match le_or_gt 0 y with\n    | Or.inl h =&gt;\n      rw [abs_of_nonneg h]\n      intro h; left; exact h\n    | Or.inr h =&gt;\n      rw [abs_of_neg h]\n      intro h; right; exact h\n</code></pre> <p>In the case of <code>match</code>, we need to use the full names <code>Or.inl</code> and <code>Or.inr</code> of the canonical ways to prove a disjunction. In this textbook, we will generally use <code>rcases</code> to split on the cases of a disjunction.</p> <p>Try proving the triangle inequality using the first two theorems in the next snippet. They are given the same names they have in Mathlib.</p> <pre><code>namespace MyAbs\n\ntheorem le_abs_self (x : \u211d) : x \u2264 |x| := by\n  sorry\n\ntheorem neg_le_abs_self (x : \u211d) : -x \u2264 |x| := by\n  sorry\n\ntheorem abs_add (x y : \u211d) : |x + y| \u2264 |x| + |y| := by\n  sorry\n</code></pre> <p>In case you enjoyed these (pun intended) and you want more practice with disjunction, try these.</p> <pre><code>theorem lt_abs : x &lt; |y| \u2194 x &lt; y \u2228 x &lt; -y := by\n  sorry\n\ntheorem abs_lt : |x| &lt; y \u2194 -y &lt; x \u2227 x &lt; y := by\n  sorry\n</code></pre> <p>You can also use <code>rcases</code> and <code>rintro</code> with nested disjunctions. When these result in a genuine case split with multiple goals, the patterns for each new goal are separated by a vertical bar.</p> <pre><code>example {x : \u211d} (h : x \u2260 0) : x &lt; 0 \u2228 x &gt; 0 := by\n  rcases lt_trichotomy x 0 with xlt | xeq | xgt\n  \u00b7 left\n    exact xlt\n  \u00b7 contradiction\n  . right; exact xgt\n</code></pre> <p>You can still nest patterns and use the <code>rfl</code> keyword to substitute equations:</p> <pre><code>example {m n k : \u2115} (h : m \u2223 n \u2228 m \u2223 k) : m \u2223 n * k := by\n  rcases h with \u27e8a, rfl\u27e9 | \u27e8b, rfl\u27e9\n  \u00b7 rw [mul_assoc]\n    apply dvd_mul_right\n  . rw [mul_comm, mul_assoc]\n    apply dvd_mul_right\n</code></pre> <p>See if you can prove the following with a single (long) line. Use <code>rcases</code> to unpack the hypotheses and split on cases, and use a semicolon and <code>linarith</code> to solve each branch.</p> <pre><code>example {z : \u211d} (h : \u2203 x y, z = x ^ 2 + y ^ 2 \u2228 z = x ^ 2 + y ^ 2 + 1) : z \u2265 0 := by\n  sorry\n</code></pre> <p>On the real numbers, an equation <code>x * y = 0</code> tells us that <code>x = 0</code> or <code>y = 0</code>. In Mathlib, this fact is known as <code>eq_zero_or_eq_zero_of_mul_eq_zero</code>, and it is another nice example of how a disjunction can arise. See if you can use it to prove the following:</p> <pre><code>example {x : \u211d} (h : x ^ 2 = 1) : x = 1 \u2228 x = -1 := by\n  sorry\n\nexample {x y : \u211d} (h : x ^ 2 = y ^ 2) : x = y \u2228 x = -y := by\n  sorry\n</code></pre> <p>Remember that you can use the <code>ring</code> tactic to help with calculations.</p> <p>In an arbitrary ring :math:<code>R</code>, an element :math:<code>x</code> such that :math:<code>x y = 0</code> for some nonzero :math:<code>y</code> is called a left zero divisor, an element :math:<code>x</code> such that :math:<code>y x = 0</code> for some nonzero :math:<code>y</code> is called a right zero divisor, and an element that is either a left or right zero divisor is called simply a zero divisor. The theorem <code>eq_zero_or_eq_zero_of_mul_eq_zero</code> says that the real numbers have no nontrivial zero divisors. A commutative ring with this property is called an integral domain. Your proofs of the two theorems above should work equally well in any integral domain:</p> <pre><code>variable {R : Type*} [CommRing R] [IsDomain R]\nvariable (x y : R)\n\nexample (h : x ^ 2 = 1) : x = 1 \u2228 x = -1 := by\n  sorry\n\nexample (h : x ^ 2 = y ^ 2) : x = y \u2228 x = -y := by\n  sorry\n</code></pre> <p>In fact, if you are careful, you can prove the first theorem without using commutativity of multiplication. In that case, it suffices to assume that <code>R</code> is a <code>Ring</code> instead of an <code>CommRing</code>.</p> <p>.. index:: excluded middle</p> <p>Sometimes in a proof we want to split on cases depending on whether some statement is true or not. For any proposition <code>P</code>, we can use <code>em P : P \u2228 \u00ac P</code>. The name <code>em</code> is short for \"excluded middle.\"</p> <pre><code>example (P : Prop) : \u00ac\u00acP \u2192 P := by\n  intro h\n  cases em P\n  \u00b7 assumption\n  . contradiction\n</code></pre> <p>Alternatively, you can use the <code>by_cases</code> tactic.</p> <pre><code>-- EXAMPLES:\nexample (P : Prop) : \u00ac\u00acP \u2192 P := by\n  intro h\n  by_cases h' : P\n  \u00b7 assumption\n  contradiction\n</code></pre> <p>Notice that the <code>by_cases</code> tactic lets you specify a label for the hypothesis that is introduced in each branch, in this case, <code>h' : P</code> in one and <code>h' : \u00ac P</code> in the other. If you leave out the label, Lean uses <code>h</code> by default. Try proving the following equivalence, using <code>by_cases</code> to establish one direction.</p> <pre><code>example (P Q : Prop) : P \u2192 Q \u2194 \u00acP \u2228 Q := by\n  sorry\n</code></pre>"},{"location":"en/C03_Logic/#sequences-and-convergence","title":"Sequences and Convergence","text":"<p>We now have enough skills at our disposal to do some real mathematics. In Lean, we can represent a sequence \\(s_0, s_1, s_2, \\ldots\\) of real numbers as a function <code>s : \u2115 \u2192 \u211d</code>. Such a sequence is said to converge to a number :math:<code>a</code> if for every \\(\\varepsilon &gt; 0\\) there is a point beyond which the sequence remains within \\(\\varepsilon\\) of \\(a\\), that is, there is a number \\(N\\) such that for every \\(n \\ge N\\), \\(| s_n - a | &lt; \\varepsilon\\). In Lean, we can render this as follows:</p> <pre><code>def ConvergesTo (s : \u2115 \u2192 \u211d) (a : \u211d) :=\n  \u2200 \u03b5 &gt; 0, \u2203 N, \u2200 n \u2265 N, |s n - a| &lt; \u03b5\n</code></pre> <p>The notation <code>\u2200 \u03b5 &gt; 0, ...</code> is a convenient abbreviation for <code>\u2200 \u03b5, \u03b5 &gt; 0 \u2192 ...</code>, and, similarly, <code>\u2200 n \u2265 N, ...</code> abbreviates <code>\u2200 n, n \u2265 N \u2192  ...</code>. And remember that <code>\u03b5 &gt; 0</code>, in turn, is defined as <code>0 &lt; \u03b5</code>, and <code>n \u2265 N</code> is defined as <code>N \u2264 n</code>.</p> <p>.. index:: extensionality, ext, tactics ; ext</p> <p>In this section, we'll establish some properties of convergence. But first, we will discuss three tactics for working with equality that will prove useful. The first, the <code>ext</code> tactic, gives us a way of proving that two functions are equal. Let \\(f(x) = x + 1\\) and \\(g(x) = 1 + x\\) be functions from reals to reals. Then, of course, \\(f = g\\), because they return the same value for every \\(x\\). The <code>ext</code> tactic enables us to prove an equation between functions by proving that their values are the same at all the values of their arguments.</p> <pre><code>example : (fun x y : \u211d \u21a6 (x + y) ^ 2) = fun x y : \u211d \u21a6 x ^ 2 + 2 * x * y + y ^ 2 := by\n  ext\n  ring\n</code></pre> <p>.. index:: congr, tactics ; congr</p> <p>We'll see later that <code>ext</code> is actually more general, and also one can specify the name of the variables that appear. For instance you can try to replace <code>ext</code> with <code>ext u v</code> in the above proof. The second tactic, the <code>congr</code> tactic, allows us to prove an equation between two expressions by reconciling the parts that are different:</p> <pre><code>example (a b : \u211d) : |a| = |a - b + b| := by\n  congr\n  ring\n</code></pre> <p>Here the <code>congr</code> tactic peels off the <code>abs</code> on each side, leaving us to prove <code>a = a - b + b</code>.</p> <p>.. index:: convert, tactics ; convert</p> <p>Finally, the <code>convert</code> tactic is used to apply a theorem to a goal when the conclusion of the theorem doesn't quite match. For example, suppose we want to prove <code>a &lt; a * a</code> from <code>1 &lt; a</code>. A theorem in the library, <code>mul_lt_mul_right</code>, will let us prove <code>1 * a &lt; a * a</code>. One possibility is to work backwards and rewrite the goal so that it has that form. Instead, the <code>convert</code> tactic lets us apply the theorem as it is, and leaves us with the task of proving the equations that are needed to make the goal match.</p> <pre><code>example {a : \u211d} (h : 1 &lt; a) : a &lt; a * a := by\n  convert (mul_lt_mul_right _).2 h\n  \u00b7 rw [one_mul]\n  exact lt_trans zero_lt_one h\n</code></pre> <p>This example illustrates another useful trick: when we apply an expression with an underscore and Lean can't fill it in for us automatically, it simply leaves it for us as another goal.</p> <p>The following shows that any constant sequence \\(a, a, a, \\ldots\\) converges.</p> <pre><code>theorem convergesTo_const (a : \u211d) : ConvergesTo (fun x : \u2115 \u21a6 a) a := by\n  intro \u03b5 \u03b5pos\n  use 0\n  intro n nge\n  rw [sub_self, abs_zero]\n  apply \u03b5pos\n</code></pre> <p>Lean has a tactic, <code>simp</code>, which can often save you the trouble of carrying out steps like <code>rw [sub_self, abs_zero]</code> by hand. We will tell you more about it soon.</p> <p>For a more interesting theorem, let's show that if <code>s</code> converges to <code>a</code> and <code>t</code> converges to <code>b</code>, then <code>fun n \u21a6 s n + t n</code> converges to <code>a + b</code>. It is helpful to have a clear pen-and-paper proof in mind before you start writing a formal one. Given <code>\u03b5</code> greater than <code>0</code>, the idea is to use the hypotheses to obtain an <code>Ns</code> such that beyond that point, <code>s</code> is within <code>\u03b5 / 2</code> of <code>a</code>, and an <code>Nt</code> such that beyond that point, <code>t</code> is within <code>\u03b5 / 2</code> of <code>b</code>. Then, whenever <code>n</code> is greater than or equal to the maximum of <code>Ns</code> and <code>Nt</code>, the sequence <code>fun n \u21a6 s n + t n</code> should be within <code>\u03b5</code> of <code>a + b</code>. The following example begins to implement this strategy. See if you can finish it off.</p> <pre><code>theorem convergesTo_add {s t : \u2115 \u2192 \u211d} {a b : \u211d}\n      (cs : ConvergesTo s a) (ct : ConvergesTo t b) :\n    ConvergesTo (fun n \u21a6 s n + t n) (a + b) := by\n  intro \u03b5 \u03b5pos\n  dsimp -- this line is not needed but cleans up the goal a bit.\n  have \u03b52pos : 0 &lt; \u03b5 / 2 := by linarith\n  rcases cs (\u03b5 / 2) \u03b52pos with \u27e8Ns, hs\u27e9\n  rcases ct (\u03b5 / 2) \u03b52pos with \u27e8Nt, ht\u27e9\n  use max Ns Nt\n  sorry\n</code></pre> <p>As hints, you can use <code>le_of_max_le_left</code> and <code>le_of_max_le_right</code>, and <code>norm_num</code> can prove <code>\u03b5 / 2 + \u03b5 / 2 = \u03b5</code>. Also, it is helpful to use the <code>congr</code> tactic to show that <code>|s n + t n - (a + b)|</code> is equal to <code>|(s n - a) + (t n - b)|,</code> since then you can use the triangle inequality. Notice that we marked all the variables <code>s</code>, <code>t</code>, <code>a</code>, and <code>b</code> implicit because they can be inferred from the hypotheses.</p> <p>Proving the same theorem with multiplication in place of addition is tricky. We will get there by proving some auxiliary statements first. See if you can also finish off the next proof, which shows that if <code>s</code> converges to <code>a</code>, then <code>fun n \u21a6 c * s n</code> converges to <code>c * a</code>. It is helpful to split into cases depending on whether <code>c</code> is equal to zero or not. We have taken care of the zero case, and we have left you to prove the result with the extra assumption that <code>c</code> is nonzero.</p> <pre><code>theorem convergesTo_mul_const {s : \u2115 \u2192 \u211d} {a : \u211d} (c : \u211d) (cs : ConvergesTo s a) :\n    ConvergesTo (fun n \u21a6 c * s n) (c * a) := by\n  by_cases h : c = 0\n  \u00b7 convert convergesTo_const 0\n    \u00b7 rw [h]\n      ring\n    rw [h]\n    ring\n  have acpos : 0 &lt; |c| := abs_pos.mpr h\n  sorry\n</code></pre> <p>The next theorem is also independently interesting: it shows that a convergent sequence is eventually bounded in absolute value. We have started you off; see if you can finish it.</p> <pre><code>theorem exists_abs_le_of_convergesTo {s : \u2115 \u2192 \u211d} {a : \u211d} (cs : ConvergesTo s a) :\n    \u2203 N b, \u2200 n, N \u2264 n \u2192 |s n| &lt; b := by\n  rcases cs 1 zero_lt_one with \u27e8N, h\u27e9\n  use N, |a| + 1\n  sorry\n</code></pre> <p>In fact, the theorem could be strengthened to assert that there is a bound <code>b</code> that holds for all values of <code>n</code>. But this version is strong enough for our purposes, and we will see at the end of this section that it holds more generally.</p> <p>The next lemma is auxiliary: we prove that if <code>s</code> converges to <code>a</code> and <code>t</code> converges to <code>0</code>, then <code>fun n \u21a6 s n * t n</code> converges to <code>0</code>. To do so, we use the previous theorem to find a <code>B</code> that bounds <code>s</code> beyond some point <code>N\u2080</code>. See if you can understand the strategy we have outlined and finish the proof.</p> <pre><code>theorem aux {s t : \u2115 \u2192 \u211d} {a : \u211d} (cs : ConvergesTo s a) (ct : ConvergesTo t 0) :\n    ConvergesTo (fun n \u21a6 s n * t n) 0 := by\n  intro \u03b5 \u03b5pos\n  dsimp\n  rcases exists_abs_le_of_convergesTo cs with \u27e8N\u2080, B, h\u2080\u27e9\n  have Bpos : 0 &lt; B := lt_of_le_of_lt (abs_nonneg _) (h\u2080 N\u2080 (le_refl _))\n  have pos\u2080 : \u03b5 / B &gt; 0 := div_pos \u03b5pos Bpos\n  rcases ct _ pos\u2080 with \u27e8N\u2081, h\u2081\u27e9\n  sorry\n</code></pre> <p>If you have made it this far, congratulations! We are now within striking distance of our theorem. The following proof finishes it off.</p> <pre><code>-- BOTH:\ntheorem convergesTo_mul {s t : \u2115 \u2192 \u211d} {a b : \u211d}\n      (cs : ConvergesTo s a) (ct : ConvergesTo t b) :\n    ConvergesTo (fun n \u21a6 s n * t n) (a * b) := by\n  have h\u2081 : ConvergesTo (fun n \u21a6 s n * (t n + -b)) 0 := by\n    apply aux cs\n    convert convergesTo_add ct (convergesTo_const (-b))\n    ring\n  have := convergesTo_add h\u2081 (convergesTo_mul_const b cs)\n  convert convergesTo_add h\u2081 (convergesTo_mul_const b cs) using 1\n  \u00b7 ext; ring\n  ring\n</code></pre> <p>For another challenging exercise, try filling out the following sketch of a proof that limits are unique. (If you are feeling bold, you can delete the proof sketch and try proving it from scratch.)</p> <pre><code>theorem convergesTo_unique {s : \u2115 \u2192 \u211d} {a b : \u211d}\n      (sa : ConvergesTo s a) (sb : ConvergesTo s b) :\n    a = b := by\n  by_contra abne\n  have : |a - b| &gt; 0 := by sorry\n  let \u03b5 := |a - b| / 2\n  have \u03b5pos : \u03b5 &gt; 0 := by\n    change |a - b| / 2 &gt; 0\n    linarith\n  rcases sa \u03b5 \u03b5pos with \u27e8Na, hNa\u27e9\n  rcases sb \u03b5 \u03b5pos with \u27e8Nb, hNb\u27e9\n  let N := max Na Nb\n  have absa : |s N - a| &lt; \u03b5 := by sorry\n  have absb : |s N - b| &lt; \u03b5 := by sorry\n  have : |a - b| &lt; |a - b| := by sorry\n  exact lt_irrefl _ this\n</code></pre> <p>We close the section with the observation that our proofs can be generalized. For example, the only properties that we have used of the natural numbers is that their structure carries a partial order with <code>min</code> and <code>max</code>. You can check that everything still works if you replace <code>\u2115</code> everywhere by any linear order <code>\u03b1</code>:</p> <pre><code>variable {\u03b1 : Type*} [LinearOrder \u03b1]\n\ndef ConvergesTo' (s : \u03b1 \u2192 \u211d) (a : \u211d) :=\n  \u2200 \u03b5 &gt; 0, \u2203 N, \u2200 n \u2265 N, |s n - a| &lt; \u03b5\n</code></pre> <p>In :numref:<code>filters</code>, we will see that Mathlib has mechanisms for dealing with convergence in vastly more general terms, not only abstracting away particular features of the domain and codomain, but also abstracting over different types of convergence.</p>"},{"location":"en/C04_Sets_and_Functions/","title":"Sets and Functions","text":"<p>The vocabulary of sets, relations, and functions provides a uniform language for carrying out constructions in all the branches of mathematics. Since functions and relations can be defined in terms of sets, axiomatic set theory can be used as a foundation for mathematics.</p> <p>Lean's foundation is based instead on the primitive notion of a type, and it includes ways of defining functions between types. Every expression in Lean has a type: there are natural numbers, real numbers, functions from reals to reals, groups, vector spaces, and so on. Some expressions are types, which is to say, their type is <code>Type</code>. Lean and Mathlib provide ways of defining new types, and ways of defining objects of those types.</p> <p>Conceptually, you can think of a type as just a set of objects. Requiring every object to have a type has some advantages. For example, it makes it possible to overload notation like <code>+</code>, and it sometimes makes input less verbose because Lean can infer a lot of information from an object's type. The type system also enables Lean to flag errors when you apply a function to the wrong number of arguments, or apply a function to arguments of the wrong type.</p> <p>Lean's library does define elementary set-theoretic notions. In contrast to set theory, in Lean a set is always a set of objects of some type, such as a set of natural numbers or a set of functions from real numbers to real numbers. The distinction between types and sets takes some getting used to, but this chapter will take you through the essentials.</p>"},{"location":"en/C04_Sets_and_Functions/#sets","title":"Sets","text":"<p>.. index:: set operations</p> <p>If <code>\u03b1</code> is any type, the type <code>Set \u03b1</code> consists of sets of elements of <code>\u03b1</code>. This type supports the usual set-theoretic operations and relations. For example, <code>s \u2286 t</code> says that <code>s</code> is a subset of <code>t</code>, <code>s \u2229 t</code> denotes the intersection of <code>s</code> and <code>t</code>, and <code>s \u222a t</code> denotes their union. The subset relation can be typed with <code>\\ss</code> or <code>\\sub</code>, intersection can be typed with <code>\\i</code> or <code>\\cap</code>, and union can be typed with <code>\\un</code> or <code>\\cup</code>. The library also defines the set <code>univ</code>, which consists of all the elements of type <code>\u03b1</code>, and the empty set, <code>\u2205</code>, which can be typed as <code>\\empty</code>. Given <code>x : \u03b1</code> and <code>s : Set \u03b1</code>, the expression <code>x \u2208 s</code> says that <code>x</code> is a member of <code>s</code>. Theorems that mention set membership often include <code>mem</code> in their name. The expression <code>x \u2209 s</code> abbreviates <code>\u00ac x \u2208 s</code>. You can type <code>\u2208</code> as <code>\\in</code> or <code>\\mem</code> and <code>\u2209</code> as <code>\\notin</code>.</p> <p>.. index:: simp, tactics ; simp</p> <p>One way to prove things about sets is to use <code>rw</code> or the simplifier to expand the definitions. In the second example below, we use <code>simp only</code> to tell the simplifier to use only the list of identities we give it, and not its full database of identities. Unlike <code>rw</code>, <code>simp</code> can perform simplifications inside a universal or existential quantifier. If you step through the proof, you can see the effects of these commands.</p> <pre><code>variable {\u03b1 : Type*}\nvariable (s t u : Set \u03b1)\nopen Set\n\n-- EXAMPLES:\nexample (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u := by\n  rw [subset_def, inter_def, inter_def]\n  rw [subset_def] at h\n  simp only [mem_setOf]\n  rintro x \u27e8xs, xu\u27e9\n  exact \u27e8h _ xs, xu\u27e9\n\nexample (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u := by\n  simp only [subset_def, mem_inter_iff] at *\n  rintro x \u27e8xs, xu\u27e9\n  exact \u27e8h _ xs, xu\u27e9\n</code></pre> <p>In this example, we open the <code>set</code> namespace to have access to the shorter names for the theorems. But, in fact, we can delete the calls to <code>rw</code> and <code>simp</code> entirely:</p> <pre><code>example (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u := by\n  intro x xsu\n  exact \u27e8h xsu.1, xsu.2\u27e9\n</code></pre> <p>What is going on here is known as definitional reduction: to make sense of the <code>intro</code> command and the anonymous constructors Lean is forced to expand the definitions. The following example also illustrate the phenomenon:</p> <pre><code>example (h : s \u2286 t) : s \u2229 u \u2286 t \u2229 u :=\n  fun x \u27e8xs, xu\u27e9 \u21a6 \u27e8h xs, xu\u27e9\n</code></pre> <p>To deal with unions, we can use <code>Set.union_def</code> and <code>Set.mem_union</code>. Since <code>x \u2208 s \u222a t</code> unfolds to <code>x \u2208 s \u2228 x \u2208 t</code>, we can also use the <code>cases</code> tactic to force a definitional reduction.</p> <pre><code>example : s \u2229 (t \u222a u) \u2286 s \u2229 t \u222a s \u2229 u := by\n  intro x hx\n  have xs : x \u2208 s := hx.1\n  have xtu : x \u2208 t \u222a u := hx.2\n  rcases xtu with xt | xu\n  \u00b7 left\n    show x \u2208 s \u2229 t\n    exact \u27e8xs, xt\u27e9\n  . right\n    show x \u2208 s \u2229 u\n    exact \u27e8xs, xu\u27e9\n</code></pre> <p>Since intersection binds tighter than union, the use of parentheses in the expression <code>(s \u2229 t) \u222a (s \u2229 u)</code> is unnecessary, but they make the meaning of the expression clearer. The following is a shorter proof of the same fact:</p> <pre><code>example : s \u2229 (t \u222a u) \u2286 s \u2229 t \u222a s \u2229 u := by\n  rintro x \u27e8xs, xt | xu\u27e9\n  \u00b7 left; exact \u27e8xs, xt\u27e9\n  . right; exact \u27e8xs, xu\u27e9\n</code></pre> <p>As an exercise, try proving the other inclusion:</p> <pre><code>example : s \u2229 t \u222a s \u2229 u \u2286 s \u2229 (t \u222a u) := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  rintro x (\u27e8xs, xt\u27e9 | \u27e8xs, xu\u27e9)\n  \u00b7 use xs; left; exact xt\n  . use xs; right; exact xu\n</code></pre> <p>It might help to know that when using <code>rintro</code>, sometimes we need to use parentheses around a disjunctive pattern <code>h1 | h2</code> to get Lean to parse it correctly.</p> <p>The library also defines set difference, <code>s \\ t</code>, where the backslash is a special unicode character entered as <code>\\\\</code>. The expression <code>x \u2208 s \\ t</code> expands to <code>x \u2208 s \u2227 x \u2209 t</code>. (The <code>\u2209</code> can be entered as <code>\\notin</code>.) It can be rewritten manually using <code>Set.diff_eq</code> and <code>dsimp</code> or <code>Set.mem_diff</code>, but the following two proofs of the same inclusion show how to avoid using them.</p> <pre><code>example : (s \\ t) \\ u \u2286 s \\ (t \u222a u) := by\n  intro x xstu\n  have xs : x \u2208 s := xstu.1.1\n  have xnt : x \u2209 t := xstu.1.2\n  have xnu : x \u2209 u := xstu.2\n  constructor\n  \u00b7 exact xs\n  intro xtu\n  -- x \u2208 t \u2228 x \u2208 u\n  rcases xtu with xt | xu\n  \u00b7 show False; exact xnt xt\n  . show False; exact xnu xu\n\nexample : (s \\ t) \\ u \u2286 s \\ (t \u222a u) := by\n  rintro x \u27e8\u27e8xs, xnt\u27e9, xnu\u27e9\n  use xs\n  rintro (xt | xu) &lt;;&gt; contradiction\n</code></pre> <p>As an exercise, prove the reverse inclusion:</p> <pre><code>example : s \\ (t \u222a u) \u2286 (s \\ t) \\ u := by\n/- EXAMPLES:\n  sorry\nSOLUTIONS: -/\n  rintro x \u27e8xs, xntu\u27e9\n  constructor\n  use xs\n  \u00b7 intro xt\n    exact xntu (Or.inl xt)\n  intro xu\n  apply xntu (Or.inr xu)\n</code></pre> <p>To prove that two sets are equal, it suffices to show that every element of one is an element of the other. This principle is known as \"extensionality,\" and, unsurprisingly, the <code>ext</code> tactic is equipped to handle it.</p> <pre><code>example : s \u2229 t = t \u2229 s := by\n  ext x\n  simp only [mem_inter_iff]\n  constructor\n  \u00b7 rintro \u27e8xs, xt\u27e9; exact \u27e8xt, xs\u27e9\n  . rintro \u27e8xt, xs\u27e9; exact \u27e8xs, xt\u27e9\n</code></pre> <p>Once again, deleting the line <code>simp only [mem_inter_iff]</code> does not harm the proof. In fact, if you like inscrutable proof terms, the following one-line proof is for you:</p> <pre><code>example : s \u2229 t = t \u2229 s :=\n  Set.ext fun x \u21a6 \u27e8fun \u27e8xs, xt\u27e9 \u21a6 \u27e8xt, xs\u27e9, fun \u27e8xt, xs\u27e9 \u21a6 \u27e8xs, xt\u27e9\u27e9\n</code></pre> <p>Here is an even shorter proof, using the simplifier:</p> <pre><code>example : s \u2229 t = t \u2229 s := by ext x; simp [and_comm]\n</code></pre> <p>An alternative to using <code>ext</code> is to use the theorem <code>Subset.antisymm</code> which allows us to prove an equation <code>s = t</code> between sets by proving <code>s \u2286 t</code> and <code>t \u2286 s</code>.</p> <pre><code>example : s \u2229 t = t \u2229 s := by\n  apply Subset.antisymm\n  \u00b7 rintro x \u27e8xs, xt\u27e9; exact \u27e8xt, xs\u27e9\n  . rintro x \u27e8xt, xs\u27e9; exact \u27e8xs, xt\u27e9\n</code></pre> <p>Try finishing this proof term:</p> <pre><code>example : s \u2229 t = t \u2229 s :=\n    Subset.antisymm sorry sorry\n    Subset.antisymm\n    (fun x \u27e8xs, xt\u27e9 \u21a6 \u27e8xt, xs\u27e9) fun x \u27e8xt, xs\u27e9 \u21a6 \u27e8xs, xt\u27e9\n</code></pre> <p>Remember that you can replace <code>sorry</code> by an underscore, and when you hover over it, Lean will show you what it expects at that point.</p> <p>Here are some set-theoretic identities you might enjoy proving:</p> <pre><code>example : s \u2229 (s \u222a t) = s := by\n  sorry\n\nexample : s \u222a s \u2229 t = s := by\n  sorry\n\nexample : s \\ t \u222a t = s \u222a t := by\n  sorry\n\nexample : s \\ t \u222a t \\ s = (s \u222a t) \\ (s \u2229 t) := by\n  sorry\n</code></pre> <p>When it comes to representing sets, here is what is going on underneath the hood. In type theory, a property or predicate on a type <code>\u03b1</code> is just a function <code>P : \u03b1 \u2192 Prop</code>. This makes sense: given <code>a : \u03b1</code>, <code>P a</code> is just the proposition that <code>P</code> holds of <code>a</code>. In the library, <code>Set \u03b1</code> is defined to be <code>\u03b1 \u2192 Prop</code> and <code>x \u2208 s</code> is defined to be <code>s x</code>. In other words, sets are really properties, treated as objects.</p> <p>The library also defines set-builder notation. The expression <code>{ y | P y }</code> unfolds to <code>(fun y \u21a6 P y)</code>, so <code>x \u2208 { y | P y }</code> reduces to <code>P x</code>. So we can turn the property of being even into the set of even numbers:</p> <pre><code>def evens : Set \u2115 :=\n  { n | Even n }\n\ndef odds : Set \u2115 :=\n  { n | \u00acEven n }\n\nexample : evens \u222a odds = univ := by\n  rw [evens, odds]\n  ext n\n  simp\n  apply Classical.em\n</code></pre> <p>You should step through this proof and make sure you understand what is going on. Try deleting the line <code>rw [evens, odds]</code> and confirm that the proof still works.</p> <p>In fact, set-builder notation is used to define</p> <ul> <li><code>s \u2229 t</code> as <code>{x | x \u2208 s \u2227 x \u2208 t}</code>,</li> <li><code>s \u222a t</code> as <code>{x | x \u2208 s \u2228 x \u2208 t}</code>,</li> <li><code>\u2205</code> as <code>{x | False}</code>, and</li> <li><code>univ</code> as <code>{x | True}</code>.</li> </ul> <p>We often need to indicate the type of <code>\u2205</code> and <code>univ</code> explicitly, because Lean has trouble guessing which ones we mean. The following examples show how Lean unfolds the last two definitions when needed. In the second one, <code>trivial</code> is the canonical proof of <code>True</code> in the library.</p> <pre><code>example (x : \u2115) (h : x \u2208 (\u2205 : Set \u2115)) : False :=\n  h\n\nexample (x : \u2115) : x \u2208 (univ : Set \u2115) :=\n  trivial\n</code></pre> <p>As an exercise, prove the following inclusion. Use <code>intro n</code> to unfold the definition of subset, and use the simplifier to reduce the set-theoretic constructions to logic. We also recommend using the theorems <code>Nat.Prime.eq_two_or_odd</code> and <code>Nat.even_iff</code>.</p> <pre><code>example : { n | Nat.Prime n } \u2229 { n | n &gt; 2 } \u2286 { n | \u00acEven n } := by\n  sorry\n</code></pre> <p>Be careful: it is somewhat confusing that the library has multiple versions of the predicate <code>Prime</code>. The most general one makes sense in any commutative monoid with a zero element. The predicate <code>Nat.Prime</code> is specific to the natural numbers. Fortunately, there is a theorem that says that in the specific case, the two notions agree, so you can always rewrite one to the other.</p> <pre><code>#print Prime\n\n#print Nat.Prime\n\nexample (n : \u2115) : Prime n \u2194 Nat.Prime n :=\n  Nat.prime_iff.symm\n\nexample (n : \u2115) (h : Prime n) : Nat.Prime n := by\n  rw [Nat.prime_iff]\n  exact h\n</code></pre> <p>.. index:: rwa, tactics ; rwa</p> <p>The <code>rwa</code> tactic follows a rewrite with the assumption tactic.</p> <pre><code>example (n : \u2115) (h : Prime n) : Nat.Prime n := by\n  rwa [Nat.prime_iff]\n</code></pre> <p>-- BOTH: end</p> <p>.. index:: bounded quantifiers</p> <p>Lean introduces the notation <code>\u2200 x \u2208 s, ...</code>, \"for every <code>x</code> in <code>s</code> .,\" as an abbreviation for <code>\u2200 x, x \u2208 s \u2192 ...</code>. It also introduces the notation <code>\u2203 x \u2208 s, ...,</code> \"there exists an <code>x</code> in <code>s</code> such that ..\" These are sometimes known as bounded quantifiers, because the construction serves to restrict their significance to the set <code>s</code>. As a result, theorems in the library that make use of them often contain <code>ball</code> or <code>bex</code> in the name. The theorem <code>bex_def</code> asserts that <code>\u2203 x \u2208 s, ...</code> is equivalent to <code>\u2203 x, x \u2208 s \u2227 ...,</code> but when they are used with <code>rintro</code>, <code>use</code>, and anonymous constructors, these two expressions behave roughly the same. As a result, we usually don't need to use <code>bex_def</code> to transform them explicitly. Here is are some examples of how they are used:</p> <pre><code>variable (s t : Set \u2115)\n\nexample (h\u2080 : \u2200 x \u2208 s, \u00acEven x) (h\u2081 : \u2200 x \u2208 s, Prime x) : \u2200 x \u2208 s, \u00acEven x \u2227 Prime x := by\n  intro x xs\n  constructor\n  \u00b7 apply h\u2080 x xs\n  apply h\u2081 x xs\n\nexample (h : \u2203 x \u2208 s, \u00acEven x \u2227 Prime x) : \u2203 x \u2208 s, Prime x := by\n  rcases h with \u27e8x, xs, _, prime_x\u27e9\n  use x, xs\n</code></pre> <p>See if you can prove these slight variations:</p> <pre><code>section\nvariable (ssubt : s \u2286 t)\n\nexample (h\u2080 : \u2200 x \u2208 t, \u00acEven x) (h\u2081 : \u2200 x \u2208 t, Prime x) : \u2200 x \u2208 s, \u00acEven x \u2227 Prime x := by\n  sorry\n\nexample (h : \u2203 x \u2208 s, \u00acEven x \u2227 Prime x) : \u2203 x \u2208 t, Prime x := by\n  sorry\n\nend\n</code></pre> <p>Indexed unions and intersections are another important set-theoretic construction. We can model a sequence \\(A_0, A_1, A_2, \\ldots\\) of sets of elements of <code>\u03b1</code> as a function <code>A : \u2115 \u2192 Set \u03b1</code>, in which case <code>\u22c3 i, A i</code> denotes their union, and <code>\u22c2 i, A i</code> denotes their intersection. There is nothing special about the natural numbers here, so <code>\u2115</code> can be replaced by any type <code>I</code> used to index the sets. The following illustrates their use.</p> <pre><code>variable {\u03b1 I : Type*}\nvariable (A B : I \u2192 Set \u03b1)\nvariable (s : Set \u03b1)\n\nopen Set\n\nexample : (s \u2229 \u22c3 i, A i) = \u22c3 i, A i \u2229 s := by\n  ext x\n  simp only [mem_inter_iff, mem_iUnion]\n  constructor\n  \u00b7 rintro \u27e8xs, \u27e8i, xAi\u27e9\u27e9\n    exact \u27e8i, xAi, xs\u27e9\n  rintro \u27e8i, xAi, xs\u27e9\n  exact \u27e8xs, \u27e8i, xAi\u27e9\u27e9\n\nexample : (\u22c2 i, A i \u2229 B i) = (\u22c2 i, A i) \u2229 \u22c2 i, B i := by\n  ext x\n  simp only [mem_inter_iff, mem_iInter]\n  constructor\n  \u00b7 intro h\n    constructor\n    \u00b7 intro i\n      exact (h i).1\n    intro i\n    exact (h i).2\n  rintro \u27e8h1, h2\u27e9 i\n  constructor\n  \u00b7 exact h1 i\n  exact h2 i\n</code></pre> <p>Parentheses are often needed with an indexed union or intersection because, as with the quantifiers, the scope of the bound variable extends as far as it can.</p> <p>Try proving the following identity. One direction requires classical logic! We recommend using <code>by_cases xs : x \u2208 s</code> at an appropriate point in the proof.</p> <pre><code>example : (s \u222a \u22c2 i, A i) = \u22c2 i, A i \u222a s := by\n  sorry\n</code></pre> <p>Mathlib also has bounded unions and intersections, which are analogous to the bounded quantifiers. You can unpack their meaning with <code>mem_iUnion\u2082</code> and <code>mem_iInter\u2082</code>. As the following examples show, Lean's simplifier carries out these replacements as well.</p> <pre><code>def primes : Set \u2115 :=\n  { x | Nat.Prime x }\n\nexample : (\u22c3 p \u2208 primes, { x | p ^ 2 \u2223 x }) = { x | \u2203 p \u2208 primes, p ^ 2 \u2223 x } :=by\n  ext\n  rw [mem_iUnion\u2082]\n  simp\n\nexample : (\u22c3 p \u2208 primes, { x | p ^ 2 \u2223 x }) = { x | \u2203 p \u2208 primes, p ^ 2 \u2223 x } := by\n  ext\n  simp\n\nexample : (\u22c2 p \u2208 primes, { x | \u00acp \u2223 x }) \u2286 { x | x = 1 } := by\n  intro x\n  contrapose!\n  simp\n  apply Nat.exists_prime_and_dvd\n</code></pre> <p>Try solving the following example, which is similar. If you start typing <code>eq_univ</code>, tab completion will tell you that <code>apply eq_univ_of_forall</code> is a good way to start the proof. We also recommend using the theorem <code>Nat.exists_infinite_primes</code>.</p> <pre><code>example : (\u22c3 p \u2208 primes, { x | x \u2264 p }) = univ := by\n  sorry\n</code></pre> <p>Give a collection of sets, <code>s : Set (Set \u03b1)</code>, their union, <code>\u22c3\u2080 s</code>, has type <code>Set \u03b1</code> and is defined as <code>{x | \u2203 t \u2208 s, x \u2208 t}</code>. Similarly, their intersection, <code>\u22c2\u2080 s</code>, is defined as <code>{x | \u2200 t \u2208 s, x \u2208 t}</code>. These operations are called <code>sUnion</code> and <code>sInter</code>, respectively. The following examples show their relationship to bounded union and intersection.</p> <pre><code>variable {\u03b1 : Type*} (s : Set (Set \u03b1))\n\nexample : \u22c3\u2080 s = \u22c3 t \u2208 s, t := by\n  ext x\n  rw [mem_iUnion\u2082]\n  simp\n\nexample : \u22c2\u2080 s = \u22c2 t \u2208 s, t := by\n  ext x\n  rw [mem_iInter\u2082]\n  rfl\n</code></pre> <p>In the library, these identities are called <code>sUnion_eq_biUnion</code> and <code>sInter_eq_biInter</code>.</p>"},{"location":"en/C04_Sets_and_Functions/#functions","title":"Functions","text":"<p>If <code>f : \u03b1 \u2192 \u03b2</code> is a function and <code>p</code> is a set of elements of type <code>\u03b2</code>, the library defines <code>preimage f p</code>, written <code>f \u207b\u00b9' p</code>, to be <code>{x | f x \u2208 p}</code>. The expression <code>x \u2208 f \u207b\u00b9' p</code> reduces to <code>f x \u2208 p</code>. This is often convenient, as in the following example:</p> <pre><code>variable {\u03b1 \u03b2 : Type*}\nvariable (f : \u03b1 \u2192 \u03b2)\nvariable (s t : Set \u03b1)\nvariable (u v : Set \u03b2)\n\nopen Function\nopen Set\n\nexample : f \u207b\u00b9' (u \u2229 v) = f \u207b\u00b9' u \u2229 f \u207b\u00b9' v := by\n  ext\n  rfl\n</code></pre> <p>If <code>s</code> is a set of elements of type <code>\u03b1</code>, the library also defines <code>image f s</code>, written <code>f '' s</code>, to be <code>{y | \u2203 x, x \u2208 s \u2227 f x = y}</code>. So a hypothesis <code>y \u2208 f '' s</code> decomposes to a triple <code>\u27e8x, xs, xeq\u27e9</code> with <code>x : \u03b1</code> satisfying the hypotheses <code>xs : x \u2208 s</code> and <code>xeq : f x = y</code>. The <code>rfl</code> tag in the <code>rintro</code> tactic (see :numref:<code>the_existential_quantifier</code>) was made precisely for this sort of situation.</p> <pre><code>example : f '' (s \u222a t) = f '' s \u222a f '' t := by\n  ext y; constructor\n  \u00b7 rintro \u27e8x, xs | xt, rfl\u27e9\n    \u00b7 left\n      use x, xs\n    right\n    use x, xt\n  rintro (\u27e8x, xs, rfl\u27e9 | \u27e8x, xt, rfl\u27e9)\n  \u00b7 use x, Or.inl xs\n  use x, Or.inr xt\n</code></pre> <p>Notice also that the <code>use</code> tactic applies <code>rfl</code> to close goals when it can.</p> <p>Here is another example:</p> <pre><code>example : s \u2286 f \u207b\u00b9' (f '' s) := by\n  intro x xs\n  show f x \u2208 f '' s\n  use x, xs\n</code></pre> <p>We can replace the line <code>use x, xs</code> by <code>apply mem_image_of_mem f xs</code> if we want to use a theorem specifically designed for that purpose. But knowing that the image is defined in terms of an existential quantifier is often convenient.</p> <p>The following equivalence is a good exercise:</p> <pre><code>example : f '' s \u2286 v \u2194 s \u2286 f \u207b\u00b9' v := by\n  sorry\n</code></pre> <p>It shows that <code>image f</code> and <code>preimage f</code> are an instance of what is known as a Galois connection between <code>Set \u03b1</code> and <code>Set \u03b2</code>, each partially ordered by the subset relation. In the library, this equivalence is named <code>image_subset_iff</code>. In practice, the right-hand side is often the more useful representation, because <code>y \u2208 f \u207b\u00b9' t</code> unfolds to <code>f y \u2208 t</code> whereas working with <code>x \u2208 f '' s</code> requires decomposing an existential quantifier.</p> <p>Here is a long list of set-theoretic identities for you to enjoy. You don't have to do all of them at once; do a few of them, and set the rest aside for a rainy day.</p> <pre><code>example (h : Injective f) : f \u207b\u00b9' (f '' s) \u2286 s := by\n  sorry\n\nexample : f '' (f \u207b\u00b9' u) \u2286 u := by\n  sorry\n\nexample (h : Surjective f) : u \u2286 f '' (f \u207b\u00b9' u) := by\n  sorry\n\nexample (h : s \u2286 t) : f '' s \u2286 f '' t := by\n  sorry\n\nexample (h : u \u2286 v) : f \u207b\u00b9' u \u2286 f \u207b\u00b9' v := by\n  sorry\n\nexample : f \u207b\u00b9' (u \u222a v) = f \u207b\u00b9' u \u222a f \u207b\u00b9' v := by\n  sorry\n\nexample : f '' (s \u2229 t) \u2286 f '' s \u2229 f '' t := by\n  sorry\n\nexample (h : Injective f) : f '' s \u2229 f '' t \u2286 f '' (s \u2229 t) := by\n  sorry\n\nexample : f '' s \\ f '' t \u2286 f '' (s \\ t) := by\n  sorry\n\nexample : f \u207b\u00b9' u \\ f \u207b\u00b9' v \u2286 f \u207b\u00b9' (u \\ v) := by\n  sorry\n\nexample : f '' s \u2229 v = f '' (s \u2229 f \u207b\u00b9' v) := by\n  sorry\n\nexample : f '' (s \u2229 f \u207b\u00b9' u) \u2286 f '' s \u2229 u := by\n  sorry\n\nexample : s \u2229 f \u207b\u00b9' u \u2286 f \u207b\u00b9' (f '' s \u2229 u) := by\n  sorry\n\nexample : s \u222a f \u207b\u00b9' u \u2286 f \u207b\u00b9' (f '' s \u222a u) := by\n  sorry\n</code></pre> <p>You can also try your hand at the next group of exercises, which characterize the behavior of images and preimages with respect to indexed unions and intersections. In the third exercise, the argument <code>i : I</code> is needed to guarantee that the index set is nonempty. To prove any of these, we recommend using <code>ext</code> or <code>intro</code> to unfold the meaning of an equation or inclusion between sets, and then calling <code>simp</code> to unpack the conditions for membership.</p> <pre><code>variable {I : Type*} (A : I \u2192 Set \u03b1) (B : I \u2192 Set \u03b2)\n\nexample : (f '' \u22c3 i, A i) = \u22c3 i, f '' A i := by\n  sorry\n\nexample : (f '' \u22c2 i, A i) \u2286 \u22c2 i, f '' A i := by\n  sorry\n\nexample (i : I) (injf : Injective f) : (\u22c2 i, f '' A i) \u2286 f '' \u22c2 i, A i := by\n  sorry\n\nexample : (f \u207b\u00b9' \u22c3 i, B i) = \u22c3 i, f \u207b\u00b9' B i := by\n  sorry\n\nexample : (f \u207b\u00b9' \u22c2 i, B i) = \u22c2 i, f \u207b\u00b9' B i := by\n  sorry\n</code></pre> <p>The library defines a predicate <code>InjOn f s</code> to say that <code>f</code> is injective on <code>s</code>. It is defined as follows:</p> <pre><code>example : InjOn f s \u2194 \u2200 x\u2081 \u2208 s, \u2200 x\u2082 \u2208 s, f x\u2081 = f x\u2082 \u2192 x\u2081 = x\u2082 :=\n  Iff.refl _\n</code></pre> <p>The statement <code>Injective f</code> is provably equivalent to <code>InjOn f univ</code>. Similarly, the library defines <code>range f</code> to be <code>{x | \u2203y, f y = x}</code>, so <code>range f</code> is provably equal to <code>f '' univ</code>. This is a common theme in Mathlib: although many properties of functions are defined relative to their full domain, there are often relativized versions that restrict the statements to a subset of the domain type.</p> <p>Here is are some examples of <code>InjOn</code> and <code>range</code> in use:</p> <pre><code>open Set Real\n\nexample : InjOn log { x | x &gt; 0 } := by\n  intro x xpos y ypos\n  intro e\n  calc\n    x = exp (log x) := by rw [exp_log xpos]\n    _ = exp (log y) := by rw [e]\n    _ = y := by rw [exp_log ypos]\n\n\nexample : range exp = { y | y &gt; 0 } := by\n  ext y; constructor\n  \u00b7 rintro \u27e8x, rfl\u27e9\n    apply exp_pos\n  intro ypos\n  use log y\n  rw [exp_log ypos]\n</code></pre> <p>Try proving these:</p> <pre><code>example : InjOn sqrt { x | x \u2265 0 } := by\n  sorry\n\nexample : InjOn (fun x \u21a6 x ^ 2) { x : \u211d | x \u2265 0 } := by\n  sorry\n\nexample : sqrt '' { x | x \u2265 0 } = { y | y \u2265 0 } := by\n  sorry\n\nexample : (range fun x \u21a6 x ^ 2) = { y : \u211d | y \u2265 0 } := by\n  sorry\n</code></pre> <p>To define the inverse of a function <code>f : \u03b1 \u2192 \u03b2</code>, we will use two new ingredients. First, we need to deal with the fact that an arbitrary type in Lean may be empty. To define the inverse to <code>f</code> at <code>y</code> when there is no <code>x</code> satisfying <code>f x = y</code>, we want to assign a default value in <code>\u03b1</code>. Adding the annotation <code>[Inhabited \u03b1]</code> as a variable is tantamount to assuming that <code>\u03b1</code> has a preferred element, which is denoted <code>default</code>. Second, in the case where there is more than one <code>x</code> such that <code>f x = y</code>, the inverse function needs to choose one of them. This requires an appeal to the axiom of choice. Lean allows various ways of accessing it; one convenient method is to use the classical <code>choose</code> operator, illustrated below.</p> <pre><code>variable {\u03b1 \u03b2 : Type*} [Inhabited \u03b1]\n\n-- EXAMPLES:\n#check (default : \u03b1)\n\nvariable (P : \u03b1 \u2192 Prop) (h : \u2203 x, P x)\n\n#check Classical.choose h\n\nexample : P (Classical.choose h) :=\n  Classical.choose_spec h\n</code></pre> <p>Given <code>h : \u2203 x, P x</code>, the value of <code>Classical.choose h</code> is some <code>x</code> satisfying <code>P x</code>. The theorem <code>Classical.choose_spec h</code> says that <code>Classical.choose h</code> meets this specification.</p> <p>With these in hand, we can define the inverse function as follows:</p> <pre><code>noncomputable section\n\nopen Classical\n\ndef inverse (f : \u03b1 \u2192 \u03b2) : \u03b2 \u2192 \u03b1 := fun y : \u03b2 \u21a6\n  if h : \u2203 x, f x = y then Classical.choose h else default\n\ntheorem inverse_spec {f : \u03b1 \u2192 \u03b2} (y : \u03b2) (h : \u2203 x, f x = y) : f (inverse f y) = y := by\n  rw [inverse, dif_pos h]\n  exact Classical.choose_spec h\n</code></pre> <p>The lines <code>noncomputable section</code> and <code>open Classical</code> are needed because we are using classical logic in an essential way. On input <code>y</code>, the function <code>inverse f</code> returns some value of <code>x</code> satisfying <code>f x = y</code> if there is one, and a default element of <code>\u03b1</code> otherwise. This is an instance of a dependent if construction, since in the positive case, the value returned, <code>Classical.choose h</code>, depends on the assumption <code>h</code>. The identity <code>dif_pos h</code> rewrites <code>if h : e then a else b</code> to <code>a</code> given <code>h : e</code>, and, similarly, <code>dif_neg h</code> rewrites it to <code>b</code> given <code>h : \u00ac e</code>. There are also versions <code>if_pos</code> and <code>if_neg</code> that works for non-dependent if constructions and will be used in the next section. The theorem <code>inverse_spec</code> says that <code>inverse f</code> meets the first part of this specification.</p> <p>Don't worry if you do not fully understand how these work. The theorem <code>inverse_spec</code> alone should be enough to show that <code>inverse f</code> is a left inverse if and only if <code>f</code> is injective and a right inverse if and only if <code>f</code> is surjective. Look up the definition of <code>LeftInverse</code> and <code>RightInverse</code> by double-clicking or right-clicking on them in VS Code, or using the commands <code>#print LeftInverse</code> and <code>#print RightInverse</code>. Then try to prove the two theorems. They are tricky! It helps to do the proofs on paper before you start hacking through the details. You should be able to prove each of them with about a half-dozen short lines. If you are looking for an extra challenge, try to condense each proof to a single-line proof term.</p> <pre><code>variable (f : \u03b1 \u2192 \u03b2)\n\nopen Function\n\nexample : Injective f \u2194 LeftInverse (inverse f) f :=\n  sorry\n\nexample : Surjective f \u2194 RightInverse (inverse f) f :=\n  sorry\n</code></pre> <p>We close this section with a type-theoretic statement of Cantor's famous theorem that there is no surjective function from a set to its power set. See if you can understand the proof, and then fill in the two lines that are missing.</p> <pre><code>theorem Cantor : \u2200 f : \u03b1 \u2192 Set \u03b1, \u00acSurjective f := by\n  intro f surjf\n  let S := { i | i \u2209 f i }\n  rcases surjf S with \u27e8j, h\u27e9\n  have h\u2081 : j \u2209 f j := by\n    intro h'\n    have : j \u2209 f j := by rwa [h] at h'\n    contradiction\n  have h\u2082 : j \u2208 S\n  sorry\n  have h\u2083 : j \u2209 S\n  sorry\n  contradiction\n</code></pre>"},{"location":"en/C04_Sets_and_Functions/#the-schroder-bernstein-theorem","title":"The Schr\u00f6der-Bernstein Theorem","text":"<p>We close this chapter with an elementary but nontrivial theorem of set theory. Let \\(\\alpha\\) and \\(\\beta\\) be sets. (In our formalization, they will actually be types.) Suppose \\(f : \\alpha \u2192 \\beta\\) and \\(g : \\beta \u2192 \\alpha\\) are both injective. Intuitively, this means that \\(\\alpha\\) is no bigger than \\(\\beta\\) and vice-versa. If \\(\\alpha\\) and \\(\\beta\\) are finite, this implies that they have the same cardinality, which is equivalent to saying that there is a bijection between them. In the nineteenth century, Cantor stated that same result holds even in the case where \\(\\alpha\\) and \\(\\beta\\) are infinite. This was eventually established by Dedekind, Schr\u00f6der, and Bernstein independently.</p> <p>Our formalization will introduce some new methods that we will explain in greater detail in chapters to come. Don't worry if they go by too quickly here. Our goal is to show you that you already have the skills to contribute to the formal proof of a real mathematical result.</p> <p>To understand the idea behind the proof, consider the image of the map \\(g\\) in \\(\\alpha\\). On that image, the inverse of \\(g\\) is defined and is a bijection with \\(\\beta\\).</p> <p>The problem is that the bijection does not include the shaded region in the diagram, which is nonempty if \\(g\\) is not surjective. Alternatively, we can use \\(f\\) to map all of \\(\\alpha\\) to \\(\\beta\\), but in that case the problem is that if \\(f\\) is not surjective, it will miss some elements of \\(\\beta\\).</p> <p>But now consider the composition \\(g \\circ f\\) from \\(\\alpha\\) to itself. Because the composition is injective, it forms a bijection between \\(\\alpha\\) and its image, yielding a scaled-down copy of \\(alpha\\) inside itself.</p> <p>This composition maps the inner shaded ring to yet another such set, which we can think of as an even smaller concentric shaded ring, and so on. This yields a concentric sequence of shaded rings, each of which is in bijective correspondence with the next. If we map each ring to the next and leave the unshaded parts of \\(\\alpha\\) alone, we have a bijection of \\(\\alpha\\) with the image of \\(g\\). Composing with \\(g^{-1}\\), this yields the desired bijection between \\(\\alpha\\) and \\(\\beta\\).</p> <p>We can describe this bijection more simply. Let \\(A\\) be the union of the sequence of shaded regions, and define \\(h : \\alpha \\to \\beta\\) as follows:</p> \\[ h(x) = \\begin{cases} f(x) &amp; \\text{if $x \\in A$} \\\\ g^{-1}(x) &amp; \\text{otherwise.} \\end{cases} \\] <p>In other words, we use \\(f\\) on the shaded parts, and we use the inverse of \\(g\\) everywhere else. The resulting map \\(h\\) is injective because each component is injective and the images of the two components are disjoint. To see that it is surjective, suppose we are given a \\(y\\) in \\(\\beta\\), and consider \\(g(y)\\). If \\(g(y)\\) is in one of the shaded regions, it cannot be in the first ring, so we have \\(g(y) = g(f(x))\\) for some \\(x\\) is in the previous ring. By the injectivity of \\(g\\), we have \\(h(x) = f(x) = y\\). If \\(g(y)\\) is not in the shaded region, then by the definition of \\(h\\), we have \\(h(g(y))= y\\). Either way, \\(y\\) is in the image of \\(h\\).</p> <p>This argument should sound plausible, but the details are delicate. Formalizing the proof will not only improve our confidence in the result, but also help us understand it better. Because the proof uses classical logic, we tell Lean that our definitions will generally not be computable.</p> <pre><code>noncomputable section\nopen Classical\nvariable {\u03b1 \u03b2 : Type*} [Nonempty \u03b2]\n</code></pre> <p>The annotation <code>[Nonempty \u03b2]</code> specifies that <code>\u03b2</code> is nonempty. We use it because the Mathlib primitive that we will use to construct \\(g^{-1}\\) requires it. The case of the theorem where \\(\\beta\\) is empty is trivial, and even though it would not be hard to generalize the formalization to cover that case as well, we will not bother. Specifically, we need the hypothesis <code>[Nonempty \u03b2]</code> for the operation <code>invFun</code> that is defined in Mathlib. Given <code>x : \u03b1</code>, <code>invFun g x</code> chooses a preimage of <code>x</code> in <code>\u03b2</code> if there is one, and returns an arbitrary element of <code>\u03b2</code> otherwise. The function <code>invFun g</code> is always a left inverse if <code>g</code> is injective and a right inverse if <code>g</code> is surjective.</p> <p>We define the set corresponding to the union of the shaded regions as follows.</p> <pre><code>variable (f : \u03b1 \u2192 \u03b2) (g : \u03b2 \u2192 \u03b1)\n\ndef sbAux : \u2115 \u2192 Set \u03b1\n  | 0 =&gt; univ \\ g '' univ\n  | n + 1 =&gt; g '' (f '' sbAux n)\n\ndef sbSet :=\n  \u22c3 n, sbAux f g n\n</code></pre> <p>The definition <code>sbAux</code> is an example of a recursive definition, which we will explain in the next chapter. It defines a sequence of sets</p> \\[ S*0 &amp;= \\alpha \u2216 g(\\beta) \\\\ S*{n+1} &amp;= g(f(S_n)). \\] <p>The definition <code>sbSet</code> corresponds to the set \\(A = \\bigcup_{n \\in \\mathbb{N}} S_n\\) in our proof sketch. The function \\(h\\) described above is now defined as follows:</p> <pre><code>def sbFun (x : \u03b1) : \u03b2 :=\n  if x \u2208 sbSet f g then f x else invFun g x\n</code></pre> <p>We will need the fact that our definition of \\(g^{-1}\\) is a right inverse on the complement of \\(A\\), which is to say, on the non-shaded regions of \\(\\alpha\\). This is so because the outermost ring, \\(S_0\\), is equal to \\(\\alpha \\setminus g(\\beta)\\), so the complement of \\(A\\) is contained in \\(g(\\beta)\\). As a result, for every \\(x\\) in the complement of \\(A\\), there is a \\(y\\) such that \\(g(y) = x\\). (By the injectivity of \\(g\\), this \\(y\\) is unique, but next theorem says only that <code>invFun g x</code> returns some <code>y</code> such that <code>g y = x</code>.)</p> <p>Step through the proof below, make sure you understand what is going on, and fill in the remaining parts. You will need to use <code>invFun_eq</code> at the end. Notice that rewriting with <code>sbAux</code> here replaces <code>sbAux f g 0</code> with the right-hand side of the corresponding defining equation.</p> <pre><code>theorem sb*right_inv {x : \u03b1} (hx : x \u2209 sbSet f g) : g (invFun g x) = x := by\n  have : x \u2208 g '' univ := by\n    contrapose! hx\n    rw [sbSet, mem_iUnion]\n    use 0\n    rw [sbAux, mem_diff]\n    sorry\n  exact \u27e8mem_univ *, hx\u27e9\n  have : \u2203 y, g y = x := by\n    sorry\n  sorry\n</code></pre> <p>We now turn to the proof that \\(h\\) is injective. Informally, the proof goes as follows. First, suppose \\(h(x_1) = h(x_2)\\). If \\(x_1\\) is in \\(A\\), then \\(h(x_1) = f(x_1)\\), and we can show that \\(x_2\\) is in \\(A\\) as follows. If it isn't, then we have \\(h(x_2) = g^{-1}(x_2)\\). From \\(f(x_1) = h(x_1) = h(x_2)\\) we have \\(g(f(x_1)) = x_2\\). From the definition of \\(A\\), since \\(x_1\\) is in \\(A\\), \\(x_2\\) is in \\(A\\) as well, a contradiction. Hence, if $x_1` is in \\(A\\), so is \\(x_2\\), in which case we have \\(f(x_1) = h(x_1) = h(x_2) = f(x_2)\\). The injectivity of \\(f\\) then implies \\(x_1 = x_2\\). The symmetric argument shows that if \\(x_2\\) is in \\(A\\), then so is \\(x_1\\), which again implies \\(x_1 = x_2\\).</p> <p>The only remaining possibility is that neither \\(x_1\\) nor \\(x_2\\) is in \\(A\\). In that case, we have \\(g^{-1}(x_1) = h(x_1) = h(x_2) = g^{-1}(x_2)\\). Applying \\(g\\) to both sides yields \\(x_1 = x_2\\).</p> <p>Once again, we encourage you to step through the following proof to see how the argument plays out in Lean. See if you can finish off the proof using <code>sb_right_inv</code>.</p> <pre><code>theorem sb*injective (hf : Injective f) : Injective (sbFun f g) := by\n  set A := sbSet f g with A_def\n  set h := sbFun f g with h_def\n  intro x\u2081 x\u2082\n  intro (hxeq : h x\u2081 = h x\u2082)\n  show x\u2081 = x\u2082\n  simp only [h_def, sbFun, \u2190 A_def] at hxeq\n  by_cases xA : x\u2081 \u2208 A \u2228 x\u2082 \u2208 A\n  \u00b7 wlog x\u2081A : x\u2081 \u2208 A generalizing x\u2081 x\u2082 hxeq xA\n    \u00b7 symm\n      apply this hxeq.symm xA.symm (xA.resolve_left x\u2081A)\n    have x\u2082A : x\u2082 \u2208 A := by\n      apply \\_root*.not_imp_self.mp\n      intro (x\u2082nA : x\u2082 \u2209 A)\n      rw [if_pos x\u2081A, if_neg x\u2082nA] at hxeq\n      rw [A_def, sbSet, mem_iUnion] at x\u2081A\n      have x\u2082eq : x\u2082 = g (f x\u2081) := by\n        sorry\n      rcases x\u2081A with \u27e8n, hn\u27e9\n      rw [A_def, sbSet, mem_iUnion]\n      use n + 1\n      simp [sbAux]\n      exact \u27e8x\u2081, hn, x\u2082eq.symm\u27e9\n    sorry\n  push_neg at xA\n  sorry\n</code></pre> <p>The proof introduces some new tactics. To start with, notice the <code>set</code> tactic, which introduces abbreviations <code>A</code> and <code>h</code> for <code>sbSet f g</code> and <code>sb_fun f g</code> respectively. We name the corresponding defining equations <code>A_def</code> and <code>h_def</code>. The abbreviations are definitional, which is to say, Lean will sometimes unfold them automatically when needed. But not always; for example, when using <code>rw</code>, we generally need to use <code>A_def</code> and <code>h_def</code> explicitly. So the definitions bring a tradeoff: they can make expressions shorter and more readable, but they sometimes require us to do more work.</p> <p>A more interesting tactic is the <code>wlog</code> tactic, which encapsulates the symmetry argument in the informal proof above. We will not dwell on it now, but notice that it does exactly what we want. If you hover over the tactic you can take a look at its documentation.</p> <p>The argument for surjectivity is even easier. Given \\(y\\) in \\(\\beta\\), we consider two cases, depending on whether \\(g(y)\\) is in \\(A\\). If it is, it can't be in \\(S_0\\), the outermost ring, because by definition that is disjoint from the image of \\(g\\). Thus it is an element of \\(S_{n+1}\\) for some \\(n\\). This means that it is of the form \\(g(f(x))\\) for some \\(x\\) in \\(S_n\\). By the injectivity of \\(g\\), we have \\(f(x) = y\\). In the case where \\(g(y)\\) is in the complement of \\(A\\), we immediately have \\(h(g(y))= y\\), and we are done.</p> <p>Once again, we encourage you to step through the proof and fill in the missing parts. The tactic <code>rcases n with _ | n</code> splits on the cases <code>g y \u2208 sbAux f g 0</code> and <code>g y \u2208 sbAux f g (n + 1)</code>. In both cases, calling the simplifier with <code>simp [sbAux]</code> applies the corresponding defining equation of <code>sbAux</code>.</p> <pre><code>theorem sb_surjective (hg : Injective g) : Surjective (sbFun f g) := by\n  set A := sbSet f g with A_def\n  set h := sbFun f g with h_def\n  intro y\n  by_cases gyA : g y \u2208 A\n  \u00b7 rw [A_def, sbSet, mem_iUnion] at gyA\n    rcases gyA with \u27e8n, hn\u27e9\n    rcases n with * | n\n    \u00b7 simp [sbAux] at hn\n    simp [sbAux] at hn\n    rcases hn with \u27e8x, xmem, hx\u27e9\n    use x\n    have : x \u2208 A := by\n      rw [A_def, sbSet, mem_iUnion]\n      exact \u27e8n, xmem\u27e9\n    simp only [h_def, sbFun, if_pos this]\n    exact hg hx\n  sorry\n</code></pre> <p>We can now put it all together. The final statement is short and sweet, and the proof uses the fact that <code>Bijective h</code> unfolds to <code>Injective h \u2227 Surjective h</code>.</p> <pre><code>theorem schroeder_bernstein {f : \u03b1 \u2192 \u03b2} {g : \u03b2 \u2192 \u03b1} (hf : Injective f) (hg : Injective g) :\n  \u2203 h : \u03b1 \u2192 \u03b2, Bijective h :=\n  \u27e8sbFun f g, sb_injective f g hf, sb_surjective f g hg\u27e9\n</code></pre>"},{"location":"en/C05_Elementary_Number_Theory/","title":"Elementary Number Theory","text":"<p>In this chapter, we show you how to formalize some elementary results in number theory. As we deal with more substantive mathematical content, the proofs will get longer and more involved, building on the skills you have already mastered.</p>"},{"location":"en/C05_Elementary_Number_Theory/#irrational-roots","title":"Irrational Roots","text":"<p>Let's start with a fact known to the ancient Greeks, namely, that the square root of 2 is irrational. If we suppose otherwise, we can write \\(\\sqrt{2} = a / b\\) as a fraction in lowest terms. Squaring both sides yields \\(a^2 = 2 b^2\\), which implies that \\(a\\) is even. If we write \\(a = 2c\\), then we get \\(4c^2 = 2 b^2\\) and hence \\(b^2 = 2 c^2\\). This implies that \\(b\\) is also even, contradicting the fact that we have assumed that \\(a / b\\) has been reduced to lowest terms.</p> <p>Saying that \\(a / b\\) is a fraction in lowest terms means that \\(a\\) and \\(b\\) do not have any factors in common, which is to say, they are coprime. Mathlib defines the predicate <code>Nat.Coprime m n</code> to be <code>Nat.gcd m n = 1</code>. Using Lean's anonymous projection notation, if <code>s</code> and <code>t</code> are expressions of type <code>Nat</code>, we can write <code>s.Coprime t</code> instead of <code>Nat.Coprime s t</code>, and similarly for <code>Nat.gcd</code>. As usual, Lean will often unfold the definition of <code>Nat.Coprime</code> automatically when necessary, but we can also do it manually by rewriting or simplifying with the identifier <code>Nat.Coprime</code>. The <code>norm_num</code> tactic is smart enough to compute concrete values.</p> <pre><code>#print Nat.Coprime\n\nexample (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 :=\n  h\n\nexample (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 := by\n  rw [Nat.Coprime] at h\n  exact h\n\nexample : Nat.Coprime 12 7 := by norm_num\n\nexample : Nat.gcd 12 8 = 4 := by norm_num\n</code></pre> <p>We have already encountered the <code>gcd</code> function in :numref:<code>more_on_order_and_divisibility</code>. There is also a version of <code>gcd</code> for the integers; we will return to a discussion of the relationship between different number systems below. There are even a generic <code>gcd</code> function and generic notions of <code>Prime</code> and <code>Coprime</code> that make sense in general classes of algebraic structures. We will come to understand how Lean manages this generality in the next chapter. In the meanwhile, in this section, we will restrict attention to the natural numbers.</p> <p>We also need the notion of a prime number, <code>Nat.Prime</code>. The theorem <code>Nat.prime_def_lt</code> provides one familiar characterization, and <code>Nat.Prime.eq_one_or_self_of_dvd</code> provides another.</p> <pre><code>#check Nat.prime_def_lt\n\nexample (p : \u2115) (prime_p : Nat.Prime p) : 2 \u2264 p \u2227 \u2200 m : \u2115, m &lt; p \u2192 m \u2223 p \u2192 m = 1 := by\n  rwa [Nat.prime_def_lt] at prime_p\n\n#check Nat.Prime.eq_one_or_self_of_dvd\n\nexample (p : \u2115) (prime_p : Nat.Prime p) : \u2200 m : \u2115, m \u2223 p \u2192 m = 1 \u2228 m = p :=\n  prime_p.eq_one_or_self_of_dvd\n\nexample : Nat.Prime 17 := by norm_num\n\n-- commonly used\nexample : Nat.Prime 2 :=\n  Nat.prime_two\n\nexample : Nat.Prime 3 :=\n  Nat.prime_three\n</code></pre> <p>In the natural numbers, a prime number has the property that it cannot be written as a product of nontrivial factors. In a broader mathematical context, an element of a ring that has this property is said to be irreducible. An element of a ring is said to be prime if whenever it divides a product, it divides one of the factors. It is an important property of the natural numbers that in that setting the two notions coincide, giving rise to the theorem <code>Nat.Prime.dvd_mul</code>.</p> <p>We can use this fact to establish a key property in the argument above: if the square of a number is even, then that number is even as well. Mathlib defines the predicate <code>Even</code> in <code>Algebra.Group.Even</code>, but for reasons that will become clear below, we will simply use <code>2 \u2223 m</code> to express that <code>m</code> is even.</p> <pre><code>#check Nat.Prime.dvd_mul\n#check Nat.Prime.dvd_mul Nat.prime_two\n#check Nat.prime_two.dvd_mul\n\ntheorem even_of_even_sqr {m : \u2115} (h : 2 \u2223 m ^ 2) : 2 \u2223 m := by\n  rw [pow_two, Nat.prime_two.dvd_mul] at h\n  cases h &lt;;&gt; assumption\n\nexample {m : \u2115} (h : 2 \u2223 m ^ 2) : 2 \u2223 m :=\n  Nat.Prime.dvd_of_dvd_pow Nat.prime_two h\n</code></pre> <p>As we proceed, you will need to become proficient at finding the facts you need. Remember that if you can guess the prefix of the name and you have imported the relevant library, you can use tab completion (sometimes with <code>ctrl-tab</code>) to find what you are looking for. You can use <code>ctrl-click</code> on any identifier to jump to the file where it is defined, which enables you to browse definitions and theorems nearby. You can also use the search engine on the Lean community web pages, and if all else fails, don't hesitate to ask on Zulip.</p> <pre><code>example (a b c : Nat) (h : a * b = a * c) (h' : a \u2260 0) : b = c :=\n  -- apply? suggests the following:\n  (mul_right_inj' h').mp h\n</code></pre> <p>The heart of our proof of the irrationality of the square root of two is contained in the following theorem. See if you can fill out the proof sketch, using <code>even_of_even_sqr</code> and the theorem <code>Nat.dvd_gcd</code>.</p> <pre><code>example {m n : \u2115} (coprime_mn : m.Coprime n) : m ^ 2 \u2260 2 * n ^ 2 := by\n  intro sqr_eq\n  have : 2 \u2223 m := by\n    sorry\n  obtain \u27e8k, meq\u27e9 := dvd_iff_exists_eq_mul_left.mp this\n  have : 2 * (2 * k ^ 2) = 2 * n ^ 2 := by\n    rw [\u2190 sqr_eq, meq]\n    ring\n  have : 2 * k ^ 2 = n ^ 2 :=\n    sorry\n  have : 2 \u2223 n := by\n    sorry\n  have : 2 \u2223 m.gcd n := by\n    sorry\n  have : 2 \u2223 1 := by\n    sorry\n    convert this\n  norm_num at this\n</code></pre> <p>In fact, with very few changes, we can replace <code>2</code> by an arbitrary prime. Give it a try in the next example. At the end of the proof, you'll need to derive a contradiction from <code>p \u2223 1</code>. You can use <code>Nat.Prime.two_le</code>, which says that any prime number is greater than or equal to two, and <code>Nat.le_of_dvd</code>.</p> <pre><code>example {m n p : \u2115} (coprime_mn : m.Coprime n) (prime_p : p.Prime) : m ^ 2 \u2260 p * n ^ 2 := by\n  sorry\n</code></pre> <p>Let us consider another approach. Here is a quick proof that if \\(p\\) is prime, then \\(m^2 \\ne p n^2\\): if we assume \\(m^2 = p n^2\\) and consider the factorization of \\(m<code>and :math:</code>n\\) into primes, then \\(p\\) occurs an even number of times on the left side of the equation and an odd number of times on the right, a contradiction. Note that this argument requires that \\(n\\) and hence \\(m\\) are not equal to zero. The formalization below confirms that this assumption is sufficient.</p> <p>The unique factorization theorem says that any natural number other than zero can be written as the product of primes in a unique way. Mathlib contains a formal version of this, expressed in terms of a function <code>Nat.factors</code>, which returns the list of prime factors of a number in nondecreasing order. The library proves that all the elements of <code>Nat.factors n</code> are prime, that any <code>n</code> greater than zero is equal to the product of its factors, and that if <code>n</code> is equal to the product of another list of prime numbers, then that list is a permutation of <code>Nat.factors n</code>.</p> <pre><code>#check Nat.factors\n#check Nat.prime_of_mem_factors\n#check Nat.prod_factors\n#check Nat.factors_unique\n</code></pre> <p>You can browse these theorems and others nearby, even though we have not talked about list membership, products, or permutations yet. We won't need any of that for the task at hand. We will instead use the fact that Mathlib has a function <code>Nat.factorization</code>, that represents the same data as a function. Specifically, <code>Nat.factorization n p</code>, which we can also write <code>n.factorization p</code>, returns the multiplicity of <code>p</code> in the prime factorization of <code>n</code>. We will use the following three facts.</p> <pre><code>theorem factorization_mul' {m n : \u2115} (mnez : m \u2260 0) (nnez : n \u2260 0) (p : \u2115) :\n    (m * n).factorization p = m.factorization p + n.factorization p := by\n  rw [Nat.factorization_mul mnez nnez]\n  rfl\n\ntheorem factorization_pow' (n k p : \u2115) :\n    (n ^ k).factorization p = k * n.factorization p := by\n  rw [Nat.factorization_pow]\n  rfl\n\ntheorem Nat.Prime.factorization' {p : \u2115} (prime_p : p.Prime) :\n    p.factorization p = 1 := by\n  rw [prime_p.factorization]\n  simp\n</code></pre> <p>In fact, <code>n.factorization</code> is defined in Lean as a function of finite support, which explains the strange notation you will see as you step through the proofs above. Don't worry about this now. For our purposes here, we can use the three theorems above as a black box.</p> <p>The next example shows that the simplifier is smart enough to replace <code>n^2 \u2260 0</code> by <code>n \u2260 0</code>. The tactic <code>simpa</code> just calls <code>simp</code> followed by <code>assumption</code>.</p> <p>See if you can use the identities above to fill in the missing parts of the proof.</p> <pre><code>example {m n p : \u2115} (nnz : n \u2260 0) (prime_p : p.Prime) : m ^ 2 \u2260 p * n ^ 2 := by\n  intro sqr_eq\n  have nsqr_nez : n ^ 2 \u2260 0 := by simpa\n  have eq1 : Nat.factorization (m ^ 2) p = 2 * m.factorization p := by\n    sorry\n  have eq2 : (p * n ^ 2).factorization p = 2 * n.factorization p + 1 := by\n    sorry\n  have : 2 * m.factorization p % 2 = (2 * n.factorization p + 1) % 2 := by\n    rw [\u2190 eq1, sqr_eq, eq2]\n  rw [add_comm, Nat.add_mul_mod_self_left, Nat.mul_mod_right] at this\n  norm_num at this\n</code></pre> <p>A nice thing about this proof is that it also generalizes. There is nothing special about <code>2</code>; with small changes, the proof shows that whenever we write <code>m^k = r * n^k</code>, the multiplicity of any prime <code>p</code> in <code>r</code> has to be a multiple of <code>k</code>.</p> <p>To use <code>Nat.count_factors_mul_of_pos</code> with <code>r * n^k</code>, we need to know that <code>r</code> is positive. But when <code>r</code> is zero, the theorem below is trivial, and easily proved by the simplifier. So the proof is carried out in cases. The line <code>rcases r with _ | r</code> replaces the goal with two versions: one in which <code>r</code> is replaced by <code>0</code>, and the other in which <code>r</code> is replaces by <code>r + 1</code>. In the second case, we can use the theorem <code>r.succ_ne_zero</code>, which establishes <code>r + 1 \u2260 0</code> (<code>succ</code> stands for successor).</p> <p>Notice also that the line that begins <code>have : npow_nz</code> provides a short proof-term proof of <code>n^k \u2260 0</code>. To understand how it works, try replacing it with a tactic proof, and then think about how the tactics describe the proof term.</p> <p>See if you can fill in the missing parts of the proof below. At the very end, you can use <code>Nat.dvd_sub'</code> and <code>Nat.dvd_mul_right</code> to finish it off.</p> <p>Note that this example does not assume that <code>p</code> is prime, but the conclusion is trivial when <code>p</code> is not prime since <code>r.factorization p</code> is then zero by definition, and the proof works in all cases anyway.</p> <pre><code>example {m n k r : \u2115} (nnz : n \u2260 0) (pow_eq : m ^ k = r * n ^ k) {p : \u2115} :\n    k \u2223 r.factorization p := by\n  rcases r with _ | r\n  \u00b7 simp\n  have npow_nz : n ^ k \u2260 0 := fun npowz \u21a6 nnz (pow_eq_zero npowz)\n  have eq1 : (m ^ k).factorization p = k * m.factorization p := by\n    sorry\n  have eq2 : ((r + 1) * n ^ k).factorization p =\n      k * n.factorization p + (r + 1).factorization p := by\n    sorry\n  have : r.succ.factorization p = k * m.factorization p - k * n.factorization p := by\n    rw [\u2190 eq1, pow_eq, eq2, add_comm, Nat.add_sub_cancel]\n  rw [this]\n  sorry\n</code></pre> <p>There are a number of ways in which we might want to improve on these results. To start with, a proof that the square root of two is irrational should say something about the square root of two, which can be understood as an element of the real or complex numbers. And stating that it is irrational should say something about the rational numbers, namely, that no rational number is equal to it. Moreover, we should extend the theorems in this section to the integers. Although it is mathematically obvious that if we could write the square root of two as a quotient of two integers then we could write it as a quotient of two natural numbers, proving this formally requires some effort.</p> <p>In Mathlib, the natural numbers, the integers, the rationals, the reals, and the complex numbers are represented by separate data types. Restricting attention to the separate domains is often helpful: we will see that it is easy to do induction on the natural numbers, and it is easiest to reason about divisibility of integers when the real numbers are not part of the picture. But having to mediate between the different domains is a headache, one we will have to contend with. We will return to this issue later in this chapter.</p> <p>We should also expect to be able to strengthen the conclusion of the last theorem to say that the number <code>r</code> is a <code>k</code>-th power, since its <code>k</code>-th root is just the product of each prime dividing <code>r</code> raised to its multiplicity in <code>r</code> divided by <code>k</code>. To be able to do that we will need better means for reasoning about products and sums over a finite set, which is also a topic we will return to.</p> <p>In fact, the results in this section are all established in much greater generality in Mathlib, in <code>Data.Real.Irrational</code>. The notion of <code>multiplicity</code> is defined for an arbitrary commutative monoid, and that it takes values in the extended natural numbers <code>enat</code>, which adds the value infinity to the natural numbers. In the next chapter, we will begin to develop the means to appreciate the way that Lean supports this sort of generality.</p>"}]}